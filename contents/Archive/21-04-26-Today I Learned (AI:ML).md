---
title: "Today I Learned (AI/ML)"
date: "2021-04-26"
template: "post"
draft: false
path: "/cheatsheet/21-04-26/"
description: "ìƒˆë¡­ê²Œ ì•Œê²Œ ëœ ì§€ì‹ ì¤‘ì—ì„œ í•˜ë‚˜ì˜ í¬ìŠ¤íŒ…ìœ¼ë¡œ ë§Œë“¤ê¸°ì—ëŠ” ë¶€ë‹´ìŠ¤ëŸ¬ìš´ ë‚´ìš©ë“¤ì„ ì´ê³³ì— ëª¨ì•„ë‘¡ë‹ˆë‹¤. ë§¤ì¼ ê³µë¶€í•œ ë‚´ìš©ì„ ê¸°ë¡í•˜ê¸°ë³´ë‹¤ëŠ” ì•„ë¬´ë•Œë‚˜ ë¹„ì •ê¸°ì ìœ¼ë¡œ ë‚´ìš©ì„ ì—…ë°ì´íŠ¸ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ í¬ìŠ¤íŒ…ì—ì„œëŠ” AI/MLê³¼ ê´€ë ¨ëœ ê¸°ìˆ ìŠ¤íƒ ë‚´ìš©ì„ ìŒ“ê³  ìˆìŠµë‹ˆë‹¤. ìµœê·¼ì— ì‘ì„±í•œ ë‚´ìš©ë“¤ì´ í•˜ë‹¨ì— ìœ„ì¹˜í•˜ë„ë¡ ë°°ì—´í•˜ì˜€ìŠµë‹ˆë‹¤."
category: "Cheat Sheet"
---

ìƒˆë¡­ê²Œ ì•Œê²Œ ëœ ì§€ì‹ ì¤‘ì—ì„œ í•˜ë‚˜ì˜ í¬ìŠ¤íŒ…ìœ¼ë¡œ ë§Œë“¤ê¸°ì—ëŠ” ë¶€ë‹´ìŠ¤ëŸ¬ìš´ ë‚´ìš©ë“¤ì„ ì´ê³³ì— ëª¨ì•„ë‘¡ë‹ˆë‹¤. ë§¤ì¼ ê³µë¶€í•œ ë‚´ìš©ì„ ê¸°ë¡í•˜ê¸°ë³´ë‹¤ëŠ” ì•„ë¬´ë•Œë‚˜ ë¹„ì •ê¸°ì ìœ¼ë¡œ ë‚´ìš©ì„ ì—…ë°ì´íŠ¸ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ í¬ìŠ¤íŒ…ì—ì„œëŠ” AI/MLê³¼ ê´€ë ¨ëœ ê¸°ìˆ ìŠ¤íƒ ë‚´ìš©ì„ ìŒ“ê³  ìˆìŠµë‹ˆë‹¤. ìµœê·¼ì— ì‘ì„±í•œ ë‚´ìš©ë“¤ì´ í•˜ë‹¨ì— ìœ„ì¹˜í•˜ë„ë¡ ë°°ì—´í•˜ì˜€ìŠµë‹ˆë‹¤.

##### ğŸ§© ML library

*2021.04.25* 

[í…ì„œí”Œë¡œìš° ê³µì‹ë¬¸ì„œ](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/map_fn)ì˜ `tf.map_fn` í•¨ìˆ˜ì— ëŒ€í•œ ì„¤ëª…ì„ ì½ì—ˆìŠµë‹ˆë‹¤. dimension 0ì—ì„œ unpackëœ elemsì´ë¼ëŠ” tensor listì˜ ìš”ì†Œë“¤ì„ fnì— mapí•©ë‹ˆë‹¤. 

```python
tf.map_fn(fn, elems, dtype=None, parallel_iterations=None, back_prop=True,
    	  swap_memory=False, infer_shape=True, name=None)
```

MAMLì„ êµ¬í˜„ í•  ë•Œ meta-batchì— ëŒ€í•œ cross entropyë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œ ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ xsì˜ shapeì€ [meta-batch size, nway\*kshot, 84\*84\*3] ì…ë‹ˆë‹¤.

```python
cent, acc = tf.map_fn(lambda inputs: self.get_loss_single(inputs, weights),
					 elems=(xs, ys, xq, yq),
				 	 dtype=(tf.float32, tf.float32),
				 	 parallel_iterations=self.metabatch)
```

##### ğŸ§©  ML library

*2021.04.27*

ëª¨ë¸ ê·¸ë˜í”„ë¥¼ ë¹Œë“œí•˜ëŠ” í•¨ìˆ˜ì—ì„œ for loopë¥¼ ë§ì´ ì‚¬ìš©í•˜ë©´ ì´ê²Œ ê·¸ëŒ€ë¡œ ëª¨ë¸ training ë‹¨ê³„ì—ì„œë„ ë§¤ë²ˆ for loopê°€ ì ìš©ë˜ì–´ ëª¨ë¸ì˜ í•™ìŠµì´ ëŠë ¤ì§€ê² êµ¬ë‚˜ë¼ê³  ìƒê°í–ˆì—ˆëŠ”ë° ê³°ê³°íˆ ìƒê°í•´ë³´ë‹ˆê¹Œ ì•„ë‹ˆë”ë¼êµ¬ìš”. 

ë¹Œë“œí•˜ëŠ” ë‹¨ê³„ì—ì„œëŠ” for loopê°€ ì—¬ëŸ¬ ë²ˆ ëŒë”ë¼ë„, ê·¸ë˜í”„ì˜ ê° ë…¸ë“œë“¤ì´ ì—°ê²°ë˜ê³  ë‚œ ë’¤ì—ëŠ” ë¹Œë“œ ëœ ê·¸ë˜í”„ êµ¬ì¡° ìì²´ê°€ ì¤‘ìš”í•˜ì§€, ë¹Œë“œ ë‹¨ê³„ì—ì„œì˜ for loopëŠ” ê´€ë ¨ì´ ì—†ê²Œ ë©ë‹ˆë‹¤. ê½¤ë‚˜ ì˜¤ë«ë™ì•ˆ ì•„ë¬´ë ‡ì§€ ì•Šê²Œ ì°©ê°í•˜ê³  ìˆì—ˆì–´ì„œ ì´ ê³³ì— ê¸°ë¡í•©ë‹ˆë‹¤. ê·¸ëŸ¼ map\_fnì€ íŠ¹íˆ ì–´ë–¤ ê²½ìš°ì— ë©”ë¦¬íŠ¸ë¥¼ ê°€ì§ˆê¹Œ ê¶ê¸ˆí•˜ê¸´ í•˜ë„¤ìš” ğŸ§

##### ğŸ§©  ML library

*2021.05.02*

TensorFlow 1.15ë¡œ ì½”ë“œë¥¼ ì§œë‹¤ê°€ `softmax_cross_entropy_with_logits`ëŠ” lossì— ëŒ€í•œ 2nd-order ê³„ì‚°ì„ ì§€ì›í•˜ì§€ë§Œ `sparse_softmax_cross_entropy_with_logits`ëŠ” lossì— ëŒ€í•œ 2nd-order ê³„ì‚°ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ”ê±¸ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë‘˜ì˜ ì°¨ì´ëŠ” labelì´ one-hot í˜•íƒœë¡œ ì£¼ì–´ì§€ëƒ ì•„ë‹ˆëƒì˜ ì°¨ì´ë°–ì— ì—†ëŠ”ë° ì´ëŸ° ê²°ê³¼ë¥¼ ë‚˜íƒ€ëƒˆë‹¤ëŠ”ê²Œ ì´ìƒí•´ì„œ ì°¾ì•„ë³´ë‹¤ê°€ tensorflow repositoryì— [ê´€ë ¨ ì´ìŠˆ](https://github.com/tensorflow/tensorflow/issues/5876)ê°€ ì˜¬ë¼ì™”ë˜ ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

ìš”ì•½í•˜ìë©´ ì¼ë¶€ indexing ì‘ì—…ì— ëŒ€í•œ ë„í•¨ìˆ˜ ê³„ì‚°ì´ ì•„ì§ ì œëŒ€ë¡œ êµ¬í˜„ë˜ì§€ ì•Šì•˜ê±°ë‚˜, ëª‡ ê°€ì§€ operationì— ëŒ€í•´ì„œ 2ì°¨ ë¯¸ë¶„ ê³„ì‚°ì´ ê°œë°œìë“¤ë„ ì•„ì§ í•´ê²°í•˜ì§€ ëª»í•œ ì˜¤ë¥˜ë¥¼ ê°€ì§„ë‹¤ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤(êµ¬ì²´ì ì¸ ì›ì¸ì€ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤). 0.2 ë²„ì „ì—ì„œ 1.15 ê¹Œì§€ ê°œë°œì´ ì§„í–‰ë˜ë©´ì„œë„ TensorFlow íŒ€ì´ ì§€ì†ì ìœ¼ë¡œ í•´ê²°í•˜ì§€ ëª»í•˜ê³  ìˆëŠ” ë¬¸ì œì ì´ ìˆë‹¤ëŠ” ê²ƒì´ ì‹ ê¸°í–ˆìŠµë‹ˆë‹¤.

##### ğŸ¤– ML & DL

*2021.05.10*

[PR-317: MLP-Mixer: An all-MLP Architecture for Vision](https://www.youtube.com/watch?v=KQmZlxdnnuY) ì˜ìƒì„ í†µí•´ CNNê³¼ MLPê°€ ë³„ë¡œ ë‹¤ë¥´ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ì•Œì•˜ìŠµë‹ˆë‹¤. ì˜ìƒì—ì„œ ì´ì§„ì›ë‹˜ì€ CNN weightì´ Fully-Conneted weightê³¼ ë‹¤ë¥¸ ì  ë‘ ê°€ì§€ê°€ weight sharingê³¼ locally connectedë¼ê³  ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì‹œê°í™”ëœ ìë£Œë§Œ ë´ë„ ì´ë ‡ê²Œ ê°„ë‹¨í•˜ê²Œ ì´í•´ë˜ëŠ” ë‚´ìš©ì¸ë° ì™œ ì§€ê¸ˆê¹Œì§€ ê¹¨ë‹«ì§€ ëª»í–ˆì„ê¹Œë¼ëŠ” ìƒê°ì´ ë“¤ì—ˆê³ , CNNì— ëª‡ ê°œì˜(ì‚¬ì‹¤ì€ ì—„ì²­ ë§ì€ ì–‘ì´ì§€ë§Œ) weightì„ ì¶”ê°€í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ Fully-Connectedì™€ ì™„ì „íˆ ë™ì¼í•œ êµ¬ì¡°ë¡œ ë§Œë“¤ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì´í•´í–ˆìŠµë‹ˆë‹¤.

##### ğŸ§©  ML library

*2021.05.11*

`tf.contrib.layers.batch_norm` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ë•Œ `is_traning` ì•„ê·œë¨¼íŠ¸ ì„¤ì •ì— ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤. Batch normalizationì„ ì‚¬ìš©í•  ë•Œ í•™ìŠµ ìƒí™©ì¸ì§€ í…ŒìŠ¤íŠ¸ ìƒí™©ì¸ì§€ì— ë”°ë¼ì„œ meanê³¼ varianceë¡œ ì‚¬ìš©í•˜ëŠ” statisticsì˜ ì¶œì²˜ê°€ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— `is_traning`ë¥¼ ì˜ëª» ì„¤ì •í•œë‹¤ë©´ ì •í™•ë„ëŠ” ë†’ê²Œ ë‚˜ì˜¤ë”ë¼ë„ ê·¸ ì‹¤í—˜ì´ ì˜ëª»ëœ ê²°ê³¼ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

`is_training`ì´ Trueì¸ ê²½ìš°ì—ëŠ” moving_mean í…ì„œì™€ moving_variance í…ì„œì— statistics of the moments(ë¯¸ë‹ˆ ë°°ì¹˜ í‰ê· ê³¼ ë¶„ì‚°)ì„ exponential moving average ì‹ì— ë”°ë¼ ì¶•ì í•©ë‹ˆë‹¤. BN ê³„ì‚°ì—ëŠ” ë¯¸ë‹ˆë°°ì¹˜ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.  `is_training`ì´ Falseì¸ ê²½ìš°ì—ëŠ” ê·¸ë™ì•ˆ ì¶•ì í•˜ì˜€ë˜ moving_mean í…ì„œì™€ moving_variance í…ì„œ ê°’ì„ ê°€ì ¸ì™€ BN ê³„ì‚°ì— ì‚¬ìš©í•©ë‹ˆë‹¤. 

Few-shot learning settingì—ì„œ support setê³¼ query setì— ëŒ€í•´ì„œ ë‘˜ ë‹¤ `is_training`ì„ Trueë¡œ ì„¤ì •í•˜ë©´ ì´ëŠ” transductive settingì´ ë©ë‹ˆë‹¤. ì¦‰ queryë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ì„œ support ë¿ë§Œ ì•„ë‹ˆë¼ query ë¶„í¬ì˜ ì •ë³´ê¹Œì§€ ì‚¬ìš©í•˜ê² ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. Few-shot learningì—ì„œëŠ” ëŒ€ë¶€ë¶„ transductive settingì´ non-transductiveì— ë¹„í•´ 3%ì •ë„ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì´ê¸° ë•Œë¬¸ì— ë³¸ì¸ì˜ ì‹¤í—˜ ìƒí™©ì— ì•Œë§ê²Œ ì•„ê·œë¨¼íŠ¸ ê°’ì„ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. 

`tf.contrib.layers.group_norm` ê°™ì€ instance-based normalization ë°©ì‹ì€ ë¯¸ë‹ˆë°°ì¹˜ì— ëŒ€í•œ running statisticsë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— `is_trainable` íŒŒë¼ë¯¸í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

##### ğŸ¤– ML & DL

*2021.05.14*

Moment[^1]ëŠ” ë¬¼ë¦¬í•™ì—ì„œ íŠ¹ì • ë¬¼ë¦¬ëŸ‰ê³¼ distanceì˜ ê³±ì„ í†µí•´ ë¬¼ë¦¬ëŸ‰ì´ ê³µê°„ìƒ ì–´ë–»ê²Œ ìœ„ì¹˜í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ë©° Force, Torque, Angular momentum ë“±ì„ ì˜ˆë¡œ ë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Moment of massì— ëŒ€í•´ì„œ zeroth momentëŠ” total mass, 1st momentëŠ” center of mass, 2nd momentëŠ” moment of inertiaë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

ìˆ˜í•™ì—ì„œëŠ” í•¨ìˆ˜ì˜ íŠ¹ì§•ì„ ë‚˜íƒ€ë‚´ê¸°ìœ„í•´ momentë¼ëŠ” ì›Œë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í•¨ìˆ˜ê°€ í™•ë¥ ë¶„í¬ í˜•íƒœì¸ ê²½ìš° first momentëŠ” í™•ë¥  ë¶„í¬ì˜ ê¸°ëŒ“ê°’ì„ ì˜ë¯¸í•˜ë©°, ì´ë¥¼ moments about zeroë¼ê³ ë„ ë§í•©ë‹ˆë‹¤. ë˜í•œ second central momentë¡œëŠ” variance, third standardized momentëŠ” skewness(ë¹„ëŒ€ì¹­ë„),  fourth standardized momentëŠ” kurtosis(ì²¨ë„, ë¾°ì¡±í•œ ì •ë„) ë“±ì´ ìˆìŠµë‹ˆë‹¤.

##### ğŸ§© ML library

*2021.09.20*

[PyTorch ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze)ë¥¼ ì°¸ê³ í•˜ì—¬ ê°€ì¥ ê¸°ë³¸ì ì¸ torch Tensor ê¸°ëŠ¥ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

- squeeze: ì°¨ì›ì´ 1ì¸ ì°¨ì›ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ë”°ë¡œ ì˜µì…˜ì„ ì£¼ì§€ ì•Šìœ¼ë©´ ì°¨ì›ì´ 1ì¸ ëª¨ë“  ì°¨ì›ì„ ì œê±°í•©ë‹ˆë‹¤.
- unsqueeze: íŠ¹ì • ìœ„ì¹˜ì— 1ì¸ ì°¨ì›ì„ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜í™ë‹ˆë‹¤.
- view: í…ì„œì˜ shapeì„ ë³€ê²½í•´ì£¼ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

##### ğŸ¤– ML & DL

*2021.11.13*

ìœ„í‚¤í”¼ë””ì•„ì˜ Signed Distance Function(SDF)[^4]ì— ëŒ€í•œ ì„¤ëª…ì„ ì½ì—ˆìŠµë‹ˆë‹¤. ë¨¼ì €, SDFëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤.

- If $\Omega$ is a subset of a metric space and $b$ is the boundary of $\Omega$ the signed distance function $f$ is defined by

$$
f(x)=
\begin{cases}
d(x, \partial \Omega) & \text{if } x \in \Omega \\
-d(x, \partial \Omega) & \text{if } x \in \Omega^c
\end{cases}
$$

SDFëŠ” ì–´ë–¤ boundaryê¹Œì§€ì˜ ê±°ë¦¬ë¥¼ í‘œí˜„í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ë§Œì•½ ì–´ë–¤ ì  $x$ê°€ boundary ì•ˆ ìª½ì— ìœ„ì¹˜í•˜ê²Œ ë˜ë©´ function ê°’ì€ ì–‘ìˆ˜ë¥¼ ê°–ê²Œ ë˜ë©°, ì´ ì ì´ boundaryì™€ ì ì  ê°€ê¹ê²Œ ì´ë™í•  ìˆ˜ë¡ function ê°’ì€ 0ì— ê°€ê¹Œì›Œ ì§€ë‹¤ê°€, boundaryì— ìœ„ì¹˜í•˜ëŠ” ê²½ìš°ì—ëŠ” 0ì´ ë©ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ $x$ê°€ boundary ë°”ê¹¥ ìª½ì— ìœ„ì¹˜í•˜ëŠ” ê²½ìš°ì—ëŠ” function ê°’ì´ ìŒìˆ˜ë¥¼ ê°–ìŠµë‹ˆë‹¤.

ìœ„ì—ì„œëŠ” SDF í•¨ìˆ˜ì˜ ì‹ì— ëŒ€í•´ì„œ boundary ì•ˆ ìª½ì¸ ê²½ìš°ì— ì–‘ìˆ˜ë¼ê³  í‘œê¸°í•˜ì˜€ì§€ë§Œ boundary ì•ˆ ìª½ì„ ìŒìˆ˜ë¡œ ë‘ì–´ ë°˜ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ë„ ì¡´ì¬í•©ë‹ˆë‹¤. ì•„ë˜ ì‚¬ì§„ì€ DeepSDF[^5]ë¼ëŠ” ë…¼ë¬¸ì—ì„œ ê°€ì ¸ì˜¨ SDFì˜ ì˜ˆì‹œì´ë©° í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” boundary ì•ˆ ìª½ì„ ìŒìˆ˜ë¡œ ë‘ì—ˆìŠµë‹ˆë‹¤.

![img](../img/21-11-14-2.png)

ê³¼ê±°ì˜ surface ì¶”ì •ì´ë‚˜ 3D reconstruction ê°™ì€ taskì—ì„œëŠ” ì£¼ë¡œ voxel, point, meshë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í–ˆë‹¤ë©´, ìµœê·¼ì—ëŠ” SDF ì‚¬ìš©í•˜ë ¤ëŠ” ì‹œë„ê°€ ëŠ˜ì–´ë‚˜ê³  ìˆëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. íŠ¹íˆ Implicit Neural Representation ì—°êµ¬ì™€ SDFë¥¼ ê²°í•©í•œ ì—°êµ¬ ê²°ê³¼ë“¤ì´ í¥ë¯¸ë¡œì›Œ ë³´ì˜€ìŠµë‹ˆë‹¤.

Implicit Neural Representationì€ ì´ë¯¸ì§€ë‚˜ 3D ë°ì´í„°ë¥¼ pixel, voxel ë‹¨ìœ„ì˜ matrix í˜•íƒœë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, (x, y) ê°’ì„ ë°›ì•˜ì„ ë•Œ (r, g, b) ê°’ì„ ì¶œë ¥í•˜ëŠ” ì–´ë–¤ í•¨ìˆ˜ í•˜ë‚˜ë¡œì¨ í‘œí˜„í•˜ë ¤ëŠ” ì—°êµ¬ì…ë‹ˆë‹¤(í•¨ìˆ˜ 1ê°œëŠ” ë°ì´í„° 1ê°œë¥¼ ì˜ë¯¸í•˜ê³ , ë”°ë¼ì„œ í•™ìŠµ ì…ë ¥ 1ê°œëŠ” í”½ì…€ ê°’ 1ê°œë¡œ ì£¼ì–´ì§€ê²Œ ë  ë“¯ í•©ë‹ˆë‹¤). ë°ì´í„°ë¥¼ ì—°ì†ì ì¸ í•¨ìˆ˜ì˜ í˜•íƒœë¡œ í‘œí˜„í•˜ê¸° ë•Œë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ super resolutionì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì¥ì ì´ ìˆëŠ”ë°, ìµœê·¼ì— ì´ ë°©ì‹ê³¼ SDFë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… outputì„ ë§¤ìš° ë§¤ë„ëŸ½ê²Œ ë§Œë“¤ì–´ë‚´ê³ ì í•˜ëŠ” ì—°êµ¬ê°€ ë§ì´ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤.

##### ğŸ¤– ML & DL

*2021.12.02*

ì§€ê¸ˆê¹Œì§€ëŠ” ì•„ë¬´ ìƒê° ì—†ì´ continuous distributionì—ì„œë„ single pointì— íŠ¹ì • í™•ë¥ ì´ ì¡´ì¬í•œë‹¤ê³  ìƒê°í–ˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ $\mathcal N (0, 1)$ì— ëŒ€í•´ì„œ point $x=1$ì´ ê´€ì¸¡ë  í™•ë¥ ì´ íŠ¹ì • ê°’ìœ¼ë¡œ ì¡´ì¬í•œë‹¤ê³  ì˜ëª» ìƒê°í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤.

[ì´ ê³³](https://www.itl.nist.gov/div898/handbook/eda/section3/eda361.htm)[^6]ì„ ì°¸ê³ í•˜ë‹ˆ continuous probability functionì€ continuous intervalì˜ ë¬´í•œ pointsì— ëŒ€í•´ ì •ì˜ë˜ê¸° ë•Œë¬¸ì— single pointì˜ í™•ë¥ ì€ ì–¸ì œë‚˜ 0ì´ë©°, ë”°ë¼ì„œ continuous probability functionì—ì„œ í™•ë¥ ì€ íŠ¹ì • intervalì— ëŒ€í•´ì„œ ì¸¡ì •í•˜ê³  single pointì— ëŒ€í•´ì„  ì¸¡ì •í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  í•©ë‹ˆë‹¤.

ì–´ì°Œë³´ë©´ ê°„ë‹¨í•œ ê²ƒì´ì—ˆì§€ë§Œ ìì„¸íˆ ìƒê°í•´ë³´ì§€ëŠ” ì•Šì•„ì„œ í—·ê°ˆë ¸ë˜ ë“¯ í•©ë‹ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ, ê·¸ëŸ¬ë©´ ì–´ë–»ê²Œ 0ì´ ëª¨ì—¬ 1ì´ ë˜ëŠ” ê²ƒ ì¸ì§€ê¹Œì§€ ê¶ê¸ˆí•´ì§€ë©´ì„œ ìˆ˜í•™ì„ ë‹¹ì¥ ê·¼ë³¸ë¶€í„° ë‹¤ì‹œ ê³µë¶€í•´ì•¼í•˜ë‚˜ ì‹¶ì—ˆì§€ë§Œ, ì‹œê°„ì€ í•œì •ë˜ì–´ ìˆê³  í•  ì¼ì€ ë§ìœ¼ë‹ˆ ê¸¸ê²Œ ë³´ê³  ì²œì²œíˆ ê³µë¶€í•˜ìëŠ” ê²°ë¡ ìœ¼ë¡œ ëŒì•„ì™”ìŠµë‹ˆë‹¤ ğŸ¥²

##### ğŸ§© ML library

*2021.12.08*

PyTorchì— íŠ¹ì • weightë§Œ freezeí•˜ëŠ” ê¸°ëŠ¥ì´ êµ¬í˜„ë˜ì–´ ìˆëŠ”ì§€ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤.

Layer ë‹¨ìœ„ë¡œ freezing í•˜ëŠ” ê²½ìš°ì—ëŠ” `required_grad=False`ë¥¼ ì‚¬ìš©í•´ì„œ êµ¬í˜„í–ˆì—ˆëŠ”ë°, layer ë‚´ íŠ¹ì • weightë§Œ ê³¨ë¼ì„œ freezeí•˜ëŠ” ê¸°ëŠ¥ì€ ë”°ë¡œ ë³¸ ì ì´ ì—†ëŠ” ê²ƒ ê°™ì•„ ì°¾ì•„ë³´ë‹¤ê°€ [í•´ë‹¹ ë§í¬](https://discuss.pytorch.org/t/how-do-i-freeze-the-specific-weights-in-a-layer/104722/2)ë¥¼ ì½ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì‘ì„±ì ë¶„ì´ ì„¤ëª…í•˜ê¸°ë¡œëŠ” ì•„ë˜ì™€ ê°™ì€ ë‘ ê°€ì§€ ì„ì‹œë°©í¸ì´ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.

- `.step()`ë¥¼ í˜¸ì¶œí•˜ê¸° ì „ì— freeze í•˜ê³ ìí•˜ëŠ” weightì— ëŒ€í•´ì„œ `grad=0` í• ë‹¹. ë‹¤ë§Œ momentum, weight decayë¥¼ ì‚¬ìš©í•˜ëŠ” optimizerì˜ ê²½ìš°ì—” `grad=0`ì´ë”ë¼ë„ `.step()` í˜¸ì¶œ ì‹œ weightì„ ë³€í˜•í•˜ê¸° ë•Œë¬¸ì— ì›í•˜ëŠ”ëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
- Freezeí•˜ê³  ì‹¶ì€ weightì„ ë¯¸ë¦¬ copy í•´ë‘ê³  `.step()` ì„ í˜¸ì¶œí•˜ì—¬ weightì„ ì—…ë°ì´íŠ¸í•œ ë’¤ì—, ë³µì‚¬í–ˆë˜ weightì„ ì—…ë°ì´íŠ¸ëœ weightì— ë®ì–´ì”Œìš°ê¸°

##### ğŸ¤– ML & DL

*2022.01.15*

[ë§í¬](https://omoindrot.github.io/triplet-loss)[^7]ë¥¼ ì°¸ê³ í•˜ì—¬ triplet loss ê´€ë ¨ ìš©ì–´ë¥¼ ìˆ™ì§€í•˜ì˜€ìŠµë‹ˆë‹¤. 

- Easy triplets: $d(a, p) + \text{margin} < d(a, n)$
- Hard triplets: $d(a,n) < d(a, p)$
- Semi-hard triplets: $d(a, p) < d(a, n) < d(a,p) + \text{margin}$

##### ğŸ§© ML library

*2022.02.28*

Random seedë¥¼ ê³ ì •í•  ë•Œ ê°€ì¥ ë¨¼ì € ê³ ë ¤í•˜ë©´ ì¢‹ì„ ê²ƒë“¤ì„ ê¸°ë¡í•˜ì˜€ìŠµë‹ˆë‹¤.

```python
random.seed(args.seed)
np.random.seed(args.seed)
torch.manual_seed(args.seed)
torch.cuda.manual_seed_all(args.seed)
```

##### ğŸ¤– ML & DL

*2022.04.10*

ì—°êµ¬ë¥¼ í•˜ë©°, ëª¨ë¸ í•™ìŠµì˜ ì•ˆì •ì„±ì— ìˆì–´ì„œ residual connectionì´ ìœ ìš©í•˜ë‹¤ëŠ” ê²½í—˜ì ì¸ íŒì„ ì–»ì—ˆìŠµë‹ˆë‹¤. ResNetê³¼ ê°™ì´ ëª¨ë¸ êµ¬ì¡°ì—ì„œ residual connectionì„ í™œìš©í•˜ëŠ” ê²ƒ ë¿ë§Œ ì•„ë‹ˆë¼, ì–´ë–¤ ê°’ì„ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ë°”ê¾¸ê³  ì‹¶ì„ ë•Œ residual connectionì„ ê°€ì§„ êµ¬ì¡°ê°€ ë¹„êµì  ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ GNNì„ í†µí•´ embedding vectorë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ì‹¶ì„ ë•Œ $V_{t+1} = G(V_t)$ì˜ í˜•íƒœë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒ ë³´ë‹¤ $V_{t+1} = V_t + G(V_t)$ì˜ í˜•íƒœë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìœ¼ë©°, í˜„ì¬ ì‹¤í—˜ ì¤‘ì¸ ê²ƒ ì¤‘ì—ì„œëŠ” few-shotìœ¼ë¡œ distributionì˜ meanì„ ì˜ ì¶”ì •í•´ë³´ë ¤ëŠ” ë‚´ìš©ì´ ìˆëŠ”ë°, ì´ ê²½ìš°ì—ë„ $\hat \mu = f_\theta(\text{few-shot})$ ë³´ë‹¤ëŠ” $\hat \mu = \text{mean of few-shot} +  f_\theta(\text{few-shot})$ í˜•íƒœì—ì„œ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤.

ì•„ë¬´ë˜ë„ ì¼ë°˜ì ìœ¼ë¡œ parameterê°€ 0ì— ê°€ê¹Œìš´ ê°€ìš°ì‹œì•ˆìœ¼ë¡œ ì´ˆê¸°í™”ë˜ê¸° ë•Œë¬¸ì—, residual connectionì„ ì‚¬ìš©í•œ ê²½ìš°ì— ì´ˆê¸° lossê°€ ë” ì‘ì•„ì ¸ ë¹„êµì  í•™ìŠµì´ ì•ˆì •ì ì¸ ê²ƒì´ ì•„ë‹ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. (*ì •ë§ë¡œ ê·¸ëŸ° ê²ƒì¸ì§€ ì°¾ì•„ë³´ê³  ë‚´ìš© ì¶”ê°€í•˜ê¸°*)

##### ğŸ¤– ML & DL

*2022.05.16*

Mooreâ€“Penrose inverse(=Pseudo inverse)[^8]ì— ëŒ€í•´ì„œ ì •ë¦¬í•©ë‹ˆë‹¤.

- $A\mathrm  x =\mathrm b$ì˜ í˜•íƒœì˜ linear systemì„ í’€ ë•Œ, $A$ê°€ ì •ë°© í–‰ë ¬ì´ ì•„ë‹ˆë¼ë©´ ì•„ë˜ì˜ ë‘ ê°€ì§€ ìƒí™©ì´ ì¡´ì¬.

1. Underdetemined (n < m): ê°€ë¡œë¡œ ê¸´ A. Infinitely many solution given $\mathrm b$ in general
2. Overdetermined (n > m): ì„¸ë¡œë¡œ ê¸´ A. Zero solution for given $\mathrm b$ in general

- $A$ì— ëŒ€í•´ì„œ singular value decompositionì„ ìˆ˜í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ì „ê°œê°€ ê°€ëŠ¥í•¨

$$
A \mathrm x = b \\
U \Sigma V^\top \mathrm  x =\mathrm b \\
V \Sigma ^{-1} U^\top U \Sigma V^\top \mathrm  x =V \Sigma ^{-1} U^\top \mathrm b \\
\tilde {\mathrm x} = V \Sigma ^{-1} U^\top \mathrm b := A^+ \mathrm  b
$$

- ì—¬ê¸°ì„œ $A^+ = V \Sigma ^+ U^\top $ë¥¼ Aì˜ pseudo inverseë¼ í•¨
- $\Sigma = \text{diag}_{n,m}(\lambda_1, \cdots, \lambda_{\min\{ n, m \}})$ì¼ ë•Œ, $\Sigma^+ = \text{diag}_{m,n}(\lambda_1^+, \cdots, \lambda^+_{\min\{ n, m \}})$ where $\lambda^+= 
  \begin{cases}
      \lambda^{-1},& \lambda \neq 0 \\
      0,              & \lambda = 0
  \end{cases}$

Mooreâ€“Penrose inverseë¥¼ ì‚¬ìš©í•˜ë©´ ì„ í˜•ëŒ€ìˆ˜í•™ì˜ ë§ì€ ë¶€ë¶„ì„ ì‰½ê²Œ ì„œìˆ  ë° ì¦ëª… ê°€ëŠ¥í•¨

1. Underdetemined(í•´ê°€ ì—¬ëŸ¬ ê°œ ì¡´ì¬)ì—ì„œ $A^+ \mathrm b$ëŠ” ìœ í´ë¦¬ë“œ ë…¸ë¦„ $||\tilde {\mathrm x} ||_2$ì„ ìµœì†Œí™”í•˜ëŠ” í•´ì„
2. Overdeterminedì—ì„œ $||A \tilde {\mathrm  x} - \mathrm b||_2 = ||A A^+ \mathrm b - \mathrm b||_2$ëŠ” ìµœì†Œì œê³±ë²•ì˜ ìµœì í•´ì„

##### ğŸ¤– ML & DL

*2022.05.27*

Linear combinationì— ëŒ€í•´ì„œ ê³„ìˆ˜ê°€ ì–‘ìˆ˜ì´ê³  ê³„ìˆ˜ì˜ í•©ì´ 1ì¸ ê²½ìš°, ì´ë¥¼ convex combinationì´ë¼ê³  í•¨

Convex setì˜ ì •ì˜ì™€ ì—°ê´€ì§€ì–´ ë³´ë©´, ì–´ë–¤ ì§‘í•© Cì— ì†í•˜ëŠ” ì„ì˜ì˜ ì ë“¤ì˜ convex combinationì´ Cì— ì†í•˜ë©´ ê·¸ ì§‘í•©ì€ convex setì´ë¼ê³  ë§í•  ìˆ˜ ìˆìœ¼ë©°, ë§ˆì°¬ê°€ì§€ë¡œ convex set Cì— ì†í•˜ëŠ” ì ë“¤ì˜ convex combinationì€ í•­ìƒ Cì— ì†í•¨.

##### ğŸ¤– ML & DL

*2022.05.28*

ë‹¤ì–‘í•œ Data Augmentation ë°©ë²•ë“¤ì— ëŒ€í•´ì„œ [ì´ê³³](https://cse-study.github.io/ai/2022-05/220527-data-augmentation/)ì— ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.

##### ğŸ¤– ML & DL

*2022.06.29*

Upper bound, Lower bound, Supremum, Infimumì— ëŒ€í•œ ìˆ˜í•™ì  ì •ì˜ë¥¼ [ì´ê³³](https://web.math.ucsb.edu/~agboola/teaching/2021/winter/122A/rudin.pdf)ì„ ì°¸ê³ í•˜ì—¬ ì •ë¦¬í•©ë‹ˆë‹¤.

- Upper bound (ìƒê³„): ì–´ë–¤ ì‹¤ìˆ˜ $\beta$ê°€ ìˆì„ ë•Œ, $E$ì˜ ëª¨ë“  ì›ì†Œ $x$ì— ëŒ€í•´ì„œ $x < \beta$ë¥¼ ë§Œì¡±í•  ë•Œ, $\beta$ë¥¼ $E$ì˜ upper boundë¼ê³  í•¨. ì´ ë•Œ $E$ëŠ” ***bounded above***ë¼ê³  í•¨. (Lower boundë„ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì •ì˜ ë¨)
- Supremum, Least upper bound (ìƒí•œ): $\alpha = \sup E$ ì´ë ¤ë©´, $\alpha$ê°€ $E$ì˜ upper boundì´ë©°, $\gamma < \alpha$ì¸ ëª¨ë“  $\gamma$ê°€ $E$ì˜ upper boundê°€ ì•„ë‹ˆì–´ì•¼ í•¨. ì¦‰, **upper bound ì¤‘ leastê°€ supermum**ì„
- Infimum, Greatest lower bound (í•˜í•œ): $\alpha = \inf E$ ì´ë ¤ë©´, $\alpha$ê°€ $E$ì˜ lower boundì´ë©°,  $\beta > \alpha$ì¸ ëª¨ë“  $\beta$ê°€ $E$ì˜ lower boundê°€ ì•„ë‹ˆì–´ì•¼ í•¨. ì¦‰, **lower bound ì¤‘ greatestê°€ infimum**ì„

##### ğŸ¤– ML & DL

*2022.10.06*

10ì›” 6ì¼ì— ì§„í–‰ëœ AI workshop ë‚´ìš©ì„ ê¸°ë¡í•©ë‹ˆë‹¤. ë¨¼ì €, Federated Learningê³¼ ê´€ë ¨ëœ ë‚´ìš©ì…ë‹ˆë‹¤.

1. Federated Learning (FL)
   - Central serverì— clientì˜ dataë¥¼ ì—…ë¡œë“œí•  ìˆ˜ ì—†ëŠ” ìƒí™©ì— ì–´ë–»ê²Œ ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆì„ì§€?
   - Clientì—ì„œ ê°ì ì—…ë°ì´íŠ¸ëœ 'ëª¨ë¸'ì„ ì„œë²„ë¡œ ì˜¬ë¦¬ê³ , í‰ê· ì„ ì·¨í•´ì„œ ë‹¤ì‹œ clientì—ê²Œ ë¿Œë¦¬ëŠ” ë°©ì‹ì´ ì œì¼ ì¼ë°˜ì  (FedAvg)
   - í•˜ì§€ë§Œ ì´ëŸ° ë°©ì‹ì€ non-IID setting(heterogeneous)ì—ì„œ ë§¤ìš° í¬ê²Œ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ê²Œ ë¨: PFL ì—°êµ¬ì˜ ë°°ê²½
2. Personalized Federated Learning (PFL): Client specific weightsì´ ë„ì…ë¨
3. PFL via Meta-learning: PFLì˜ ì»¨ì…‰ê³¼ Meta-learning(MAML)ì˜ ì»¨ì…‰ì´ ë§¤ìš° ìœ ì‚¬í•˜ë‹¤ëŠ” ì ì—ì„œ ê³ ì•ˆë¨

Imitation learning ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤.

1. Reinforcement Learning (RL)
   - Purpose: Find an optimal policy $\pi*$ that miximize $V$
   - Require domain knowledge for real-world application
   - ë“œë¡ ì„ ì˜ˆë¡œ ë“¤ë©´, ì‹¤ì œ ë“œë¡ ì€ ë§¤ìš° ì‰½ê²Œ ë¶€ìˆ´ì§€ë¯€ë¡œ Sim2Real learningì„ ê³ ë ¤í•´ì•¼ í•˜ê³ , ë“œë¡  physicsì— ë§ì€ perturbationì´ ì¡´ì¬í•˜ë¯€ë¡œ Robust learningë„ ê³ ë ¤í•´ì•¼ í•¨
2. Imitation Learning (IL)
   - Behavior cloning (BC), Inverse RL (IRL), IRL + RL ë“±ì˜ ë°©ë²•ì´ ì¡´ì¬
   - BCëŠ” ë§ì€ ì–‘ì˜ ë°ì´í„°ê°€ í•„ìš”í•˜ê³  compounding errorì— ì·¨ì•½í•˜ë¯€ë¡œ, ì´ëŸ° ì ì—ì„œëŠ” IRLì´ ì¥ì ì„ ê°€ì§
3. Generative Adversariel Imitation Learning (GAIL)
   - Real dataë¡œëŠ” expert actionsë¥¼ ì œê³µí•˜ê³ , Fake dataë¡œëŠ” policy actionsë¥¼ ì œê³µí•˜ì—¬ expertì˜ policyë¥¼ í‰ë‚´ë‚´ë„ë¡ í•™ìŠ´
   - Limitation: Real envrionment dangerì™€ environment perturbationì— ëŒ€í•´ì„œëŠ” ì˜ ëª¨ë¸ë§í•˜ì§€ ì•ŠìŒ. ë”°ë¼ì„œ domain-adpative ILì´ í•„ìš”
4. Simulation-based Learning: Domain Adaptive IL
   - Simulation(source) env.ì—ì„œ informationì„ ë½‘ì•„, target envì˜ policyì— ë„ì›€ì„ ì£¼ë„ë¡, information extraction ê³¼ì •ì´ ì¤‘ìš”

##### ğŸ¤– ML & DL

*2022.10.06*

ë ˆë”§ì„ ì½ë‹¤ê°€ "í•™ìŠµì´ ë„ˆë¬´ ì˜¤ë˜ê±¸ë¦¬ëŠ” ê²½ìš°ì—” í•˜ì´í¼íŒŒë¦¬ë¯¸í„° íŠœë‹ì„ ì–´ë–»ê²Œ í•´ì•¼í•˜ëŠ”ê°€?"ì— ëŒ€í•œ ê¸€ì´ ìˆì–´, ê¸€ì— ë‹¬ë¦° ì½”ë©˜íŠ¸ì™€ ê°œì¸ì ì¸ ìƒê°ë“¤ì„ ê¸°ë¡í•©ë‹ˆë‹¤.

- ëª¨ë¸ ìŠ¤ì¼€ì¼ì„ ì¤„ì¸ ìƒíƒœë¡œ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì§„í–‰í•˜ê±°ë‚˜, ë°ì´í„°ì…‹ì„ ì¼ë¶€ë§Œ ì‚¬ìš©í•œ í•™ìŠµì„ í†µí•´ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì§„í–‰
- e.g., ResNet152ë¼ê³  í•œë‹¤ë©´ ResNet18 ê°™ì´ ì‘ì€ ëª¨ë¸ ì‚¬ìš©í•˜ê±°ë‚˜, ImageNetì´ë¼ê³  í•œë‹¤ë©´ 100ê°œ classë§Œ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ìˆ˜í–‰
- ì´ ë°©ë²•ì€ ë‹¹ì—°íˆ sub-optimalì´ê¸´ í•˜ê² ì§€ë§Œ í•™ìŠµì´ ë„ˆë¬´ ì˜¤ë˜ê±¸ë¦¬ëŠ” ê²½ìš°ì— ì¶©ë¶„íˆ í™œìš©í•´ ë³¼ë§Œ í•œ ë°©ë²•ì´ë¼ê³  ìƒê°í–ˆìŒ
- ì‚¬ì‹¤ ì œì¼ ì¢‹ì€ ê²ƒì€ GPU ìì›ì„ ë³‘ë ¬ë¡œ ì¶©ë¶„íˆ í™œìš©í•  ìˆ˜ ìˆê²Œ ì—”ì§€ë‹ˆì–´ë§ì„ ê±°ì¹œ í›„ì— í•™ìŠµí•˜ëŠ” ê²ƒ. ì™œëƒë©´ big modelê³¼ small model ì‚¬ì´ì— í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì— ë”°ë¥¸ ëª¨ë¸ì˜ ë™ì‘ì— ë¶„ëª…íˆ ì°¨ì´ê°€ ì¡´ì¬í•  ê²ƒì´ê¸° ë•Œë¬¸ì—, ì›ë˜ ìŠ¤ì¼€ì¼ëŒ€ë¡œ ì‹¤í—˜í•˜ëŠ”ê²Œ ì œì¼ ì¢‹ìŒ

##### ğŸ¤– ML & DL

*2022.10.14*

ML ë¶„ì•¼ì—ì„œì˜ "Grokking"ì´ë¼ëŠ” ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.

- Overparameterizedëœ ë‰´ëŸ´ë„· ëª¨ë¸ì´, small training datasetì— ëŒ€í•´ì„œ overfit ë˜ì–´ ìˆë‹¤ê°€, ë§¤ìš° ë§ì€ ì‹œê°„(optimization step)ì´ ì§€ë‚œ í›„ì— ì–´ëŠ ì§€ì ì— ê°‘ìê¸° ì¢‹ì€ generalization ì„±ëŠ¥(validation loss ê°ì†Œ)ì„ ë‹¬ì„±í•˜ëŠ” í˜„ìƒ
- OpenAIì˜ ["Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"](https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf) ë…¼ë¬¸ì—ì„œ ëª…ëª…

##### ğŸ¤– ML & DL

*2022.10.21*

- The stability-plasticity dilemma: ìƒˆë¡œìš´ ì§€ì‹ì„ ì–»ê¸° ìœ„í•´ ëª¨ë¸ì˜ ì˜êµ¬ ë³€í˜•ì´ ìš”êµ¬ë˜ë©´ì„œë„, ë™ì‹œì— ê¸°ì¡´ì˜ ì§€ì‹ì„ ìŠì–´ë²„ë¦¬ì§€ë„ ì•Šì•„ì•¼ í•œë‹¤ëŠ” ì 
- Learning in a parallel and distributed system requires plasticity for the integration of new knowledge but also stability in order to prevent the forgetting of previous knowledge.[^10]

##### ğŸ¤– ML & DL

*2022.12.03*

Noisy labelì´ë€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ê°€?

- ë°ì´í„° ì…‹ ë‚´ì— ë°ì´í„°ì˜ labelingì´ ì˜ ëª» ë˜ì–´ìˆëŠ” ê²½ìš°ë¥¼ noisy label í˜¹ì€ labeling noiseë¼ê³  í•¨. Large scale datasetì— ëŒ€í•´ì„œëŠ” labelì„ í™•ì¸í•˜ëŠ” ê³¼ì •ì´ í˜ë“¤ë‹¤ ë³´ë‹ˆê¹Œ(ëˆê³¼ ì‹œê°„ì´ ë§ì´ ì†Œìš”), ì´ëŸ¬í•œ noisy labelì´ ì¶©ë¶„íˆ ì¡´ì¬í•  ìˆ˜ ìˆìŒ
- ì¢…ì¢… semi-supervised learning ë¶„ì•¼ì—ì„œë„ ì‚¬ìš©ë˜ëŠ”ë°, ì´ ë•ŒëŠ” pseudo label ê¸°ë°˜ì˜ self-training modelì´ unlabeled datasetì— ì˜ëª» pseudo labeling í•œ ê²ƒì„ noisy labelì´ë¼ê³  ë¶€ë¥´ëŠ” ë“¯ í•¨

Ad-hocì´ë€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ê°€?

- ì¼ë°˜ì ìœ¼ë¡œëŠ” 'ì˜¤ë¡œì§€ íŠ¹ì • í•˜ë‚˜ì˜ ëª©ì ë§Œì„ ìœ„í•´ ê³ ì•ˆëœ ë°©ë²•' ì •ë„ë¡œ í•´ì„í•´ë³¼ ìˆ˜ ìˆìŒ

##### ğŸ¤– ML & DL

*2023.01.01*

Anomaly detection ê´€ë ¨ ìš©ì–´ì •ë¦¬, ChatGPTë¥¼ í™œìš©í•´ë³´ì•˜ìŒ.

- Target(positive) classê°€ ê°•ì•„ì§€ë¼ê³  ê°€ì •í•  ë•Œ, ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ë°œìƒí•  ìˆ˜ ìˆëŠ” ìƒí™©ë“¤
  1. ê°•ì•„ì§€ì´ì§€ë§Œ, ì´ì „ì— ë³¸ ì  ì—†ëŠ” ìƒˆë¡œìš´ ì¢…ì˜ ê°•ì•„ì§€ë¥¼ ë°œê²¬í•œ ê²½ìš°
  2. ê³ ì–‘ì´ ë°ì´í„° ë“±ê³¼ ê°™ì´ ì•„ì˜ˆ ìƒˆë¡œìš´ í´ë˜ìŠ¤ë¥¼ ë°œê²¬í•œ ê²½ìš°
  3. ê°•ì•„ì§€ ë°ì´í„°ì´ì§€ë§Œ ì†ìƒëœ/ì˜¤ì—¼ëœ ë°ì´í„°ë¥¼ ë°œê²¬í•œ ê²½ìš°
- Novelty detection: Unseen data pointë¥¼ ë°œê²¬í•˜ëŠ” ê²½ìš°ë‚˜, **ìƒˆë¡œìš´ íŠ¸ë Œë“œë‚˜ ê²½í–¥ì„±**ì„ ë°œê²¬í•˜ëŠ” ê²½ìš°ì— ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ìš©ì–´
- Outlier detection: ê¸°ì¡´ ë°ì´í„°ì™€ ë§¤ìš° ì°¨ì´ë‚˜ëŠ” data pointë¥¼ ë°œê²¬í•˜ëŠ” ê²½ìš°ë‚˜, ì œê±°í•´ì•¼ í•  **ì˜¤ì—¼ë˜ê±°ë‚˜ ì†ìƒëœ ë°ì´í„°**ë¥¼ ë°œê²¬í•˜ëŠ” ê²½ìš°ì— ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ìš©ì–´
- Anomaly detection: Novelty detectionê³¼ Outlier detectionì˜ ê²½ìš°ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë„“ì€ ë²”ìœ„ì˜ ìš©ì–´
- ë‹¤ë§Œ ìœ„ì˜ ì„¸ ê°€ì§€ ìš©ì–´ë“¤ì´ ë§¤ìš° ìì£¼ í˜¼ìš©ë˜ë¯€ë¡œ, ë…¼ë¬¸ì´ë‚˜ ìƒí™©ì— ë§ê²Œ ìœ ë™ì ìœ¼ë¡œ ì´í•´í•´ì•¼ í•¨

##### ğŸ¤– ML & DL

*2023.01.11*

Object detectionê³¼ ê´€ë ¨ëœ ìš©ì–´ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤. ë¨¼ì € ë¬¸ì œ ìƒí™©ë“¤ì„ ë‚˜ì—´í•˜ì˜€ìŠµë‹ˆë‹¤.

- Localization: **Single object**, í•´ë‹¹ objectê°€ ì‚¬ì§„ ë‚´ì—ì„œ ì–´ëŠ ìœ„ì¹˜ì— ì¡´ì¬í•˜ëŠ”ì§€ bounding box ì„¤ì •
- Object detection: **Multiple object**, ì—¬ëŸ¬ objectsê°€ ì‚¬ì§„ ë‚´ì—ì„œ ì–´ëŠ ìœ„ì¹˜ì— ì¡´ì¬í•˜ëŠ”ì§€ bounding box ì„¤ì •í•˜ê³  ê°ê°ì˜ class ì •ë³´ê¹Œì§€ ë¶€ì—¬
- Segmentation: **Multiple object**, ì—¬ëŸ¬ objectsê°€ ì‚¬ì§„ ë‚´ì—ì„œ ì–´ëŠ ìœ„ì¹˜ì— ì¡´ì¬í•˜ëŠ”ì§€ë¥¼ 'í”½ì…€ ë‹¨ìœ„ë¡œ' class ì •ë³´ ë¶€ì—¬
- 2-Stage ë°©ì‹: ë¬¼ì²´ê°€ ì¡´ì¬í•  ê²ƒ ê°™ì€ ìœ„ì¹˜ë¥¼ ì œì•ˆí•œ ë‹¤ìŒì—(**Region proposal**, localization), í•´ë‹¹ ìœ„ì¹˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ featureë¥¼ ì¶”ì¶œí•˜ê³  class ë¶€ì—¬
- 1-Stage ë°©ì‹: Localizationê³¼ classificationì„ í•œ ë²ˆì— ìˆ˜í–‰. 2-Stageì— ë¹„í•´ ì„±ëŠ¥ì€ ë‚®ì§€ë§Œ ì†ë„ëŠ” ë¹ ë¦„
- Region proposal ë°©ì‹
  1. Sliding window: Windowë¥¼ ìŠ¬ë¼ì´ë”©í•˜ë©° window ë‚´ì— objectê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
  2. Selective search: ì¸ì ‘í•œ ì˜ì—­ë¼ë¦¬ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•´ í° ì˜ì—­ìœ¼ë¡œ ì°¨ë¡€ëŒ€ë¡œ í†µí•©
- NMS: ì—¬ëŸ¬ bounding boxê°€ ê°™ì€ classë¡œ ê²¹ì³ìˆë‹¤ë©´, í•˜ë‚˜ì˜ classë¡œ í†µí•©í•˜ëŠ” ë°©ë²•
- RoI = Region of Intereset = Region proposal

2-Stage detectorë¥¼ ê°„ë‹¨íˆ ì •ë¦¬í•©ë‹ˆë‹¤.

- R-CNN: Selective searchë¥¼ í†µí•´ 2000ê°œ ì •ë„ì˜ region proposal ì°¾ìŒ. ê°ê°ì˜ crop ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ CNNì— ì…ë ¥í•œ ë’¤ì—, feature vector ì¶”ì¶œ. ë§ˆì§€ë§‰ìœ¼ë¡œëŠ” Regressorë¥¼ í†µí•´ bounding boxë¥¼ ì„¤ì •í•˜ê³ , SVMì„ í†µí•´ classification
- Fast R-CNN: Selective searchë¥¼ í†µí•´ 2000ê°œ ì •ë„ì˜ region proposal ì°¾ìŒ 
- Faster R-CNN: ì´ì „ê¹Œì§€ëŠ” CPU ê¸°ë°˜ì˜ selective search ì˜€ë‹¤ë©´, ë³¸ ì•Œê³ ë¦¬ì¦˜ì€ GPU ê¸°ë°˜ì˜ Region Proposal Network(RPN)ì„ ì œì•ˆí•˜ì—¬ ì†ë„ í–¥ìƒ. ê·¸ ì™¸ì—ëŠ” Fast R-CNNì™€ ë™ì¼

1-Stage detectorë¥¼ ê°„ë‹¨íˆ ì •ë¦¬í•©ë‹ˆë‹¤.

- YOLO: ì´ë¯¸ì§€ë¥¼ NxN ê·¸ë¦¬ë“œë¡œ ë¶„í• í•˜ì—¬ ì˜ˆì¸¡ í…ì„œ(Prediction tensor) ìƒì„±
- SSD: í…Œë‘ë¦¬ ìƒì ì¡°ì •ì„ ìœ„í•´ í”½ì…€ì´ë‚˜ íŠ¹ì§•ë“¤ì„ ì¬ ì¶”ì¶œí•˜ì§€ ì•ŠìŒ

##### ğŸ¤– ML & DL

*2023.01.14*

Bayesian Inferenceì— ëŒ€í•´ ê°„ë‹¨íˆ ì •ë¦¬í•©ë‹ˆë‹¤.

- Bayesian Inference: ì¶”ë¡  ëŒ€ìƒì˜ ì‚¬ì „ í™•ë¥ ê³¼ ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ í†µí•´ í•´ë‹¹ ëŒ€ìƒì˜ ì‚¬í›„ í™•ë¥ ì„ ì¶”ë¡ í•˜ëŠ” ë°©ë²•
- ì¼ë°˜ì ìœ¼ë¡œ ìš°ë¦¬ì˜ ëª©ì ì€ $p(x^* | X)$ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì„. ì¦‰, given data $X$ë¥¼ ê¸°ë°˜ìœ¼ë¡œ test data $x^*$ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ì˜ˆì¸¡ì„ í•  ìˆ˜ ìˆì–´ì•¼ í•¨
- $p(x^* | X) = \int p (x^* | \theta) p(\theta | X) d \theta$ë¡œ ê³„ì‚° í•  ìˆ˜ ìˆìœ¼ë©°, ì—¬ê¸°ì„œ $p(\theta | X)$ëŠ” Bayes ruleì— ì˜í•´ $p(\theta | X) = \frac{p(X|\theta)p(\theta)}{P(X)}$ì„

##### ğŸ¤– ML & DL

*2023.02.22*

CLIPì— ëŒ€í•´ ê°„ë‹¨íˆ ì •ë¦¬í•©ë‹ˆë‹¤

- **Natural language supervision**: ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ ì§ì„ ì´ë£¨ëŠ” ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒ

1. Contastive pre-training: Batchsize ë§Œí¼ì˜ ì´ë¯¸ì§€ì™€ ê·¸ì— í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸(ë¬¸ì¥)ì— ëŒ€í•´ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ê°ê° ë½‘ì•„ë‚´ê³ , ì„œë¡œ ì§ì´ ë§ëŠ” ì„ë² ë”©ê°„ ìœ ì‚¬ë„ê°€ ë†’ì•„ì§€ë„ë¡ ëª¨ë¸ í•™ìŠµ
2. Target datasetì— ëŒ€í•´ class label ì„ë² ë”©ì„ ëª¨ë‘ ë½‘ëŠ”ë°, ì´ ë•Œ í…ìŠ¤íŠ¸ë¡œëŠ” 'a photo of a {class labe}'ë¥¼ ì…ë ¥ìœ¼ë¡œ ì¤Œ (Prompt engineering!)
3. ìµœì¢…ì ìœ¼ë¡œ, í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì˜ ì„ë² ë”©ê³¼ target datasetì˜ 'a photo of a {class labe}' ì„ë² ë”© ì‚¬ì´ì— ìœ ì‚¬ë„ê°€ ì œì¼ ë†’ì€ ê²ƒì„ í™•ì¸í•¨

##### ğŸ¤– ML & DL

*2023.03.24*

- Domain generalization: source domainìœ¼ë¡œ í•™ìŠµí•œ ë’¤ ë°”ë¡œ target domainì— ì¼ë°˜í™”
- Domain adaptation: target domainì—ë„ ì–´ëŠì •ë„ labelì´ ì¡´ì¬í•˜ì—¬ ì¬í•™ìŠµì´ ê°€ëŠ¥
- Style-based generalization: Gram matrix, Maximum Mean Discrepancy(MMD), Mean Var ë“±ì„ styleë¡œ ì—¬ê²¨ì„œ í™œìš©
- ì¼ë°˜ì ìœ¼ë¡œ CNNì€ textureë¥¼ ì˜ ì¡ëŠ” high pass filter(ê³ ì£¼íŒŒ ìœ„ì£¼ë¡œ ì „ë‹¬), TransfomerëŠ” contourë¥¼ ì˜ ì¡ëŠ” low pass filterì˜ íŠ¹ì„±ì„ ë³´ì¸ë‹¤ê³  í•¨. ë”°ë¼ì„œ CNNì— ëŒ€í•´ adversarial attack í•  ë•Œë„ íŠ¹ì • ì´ë¯¸ì§€ì— ë‹¤ë¥¸ texture ì…íˆë©´ ì˜ˆì¸¡ ì„±ëŠ¥ ë–¨ì–´ì§

##### ğŸ¤– ML & DL

*2023.04.03*

Stable diffusionì— ëŒ€í•œ ê°„ë‹¨í•œ ê¸°ë¡

- Text2Imageë¥¼ ìœ„í•´ text encoder(CLIPì˜ text encoder)ì™€ image generator ì‚¬ìš©
- Image generator: Image information creator (UNet + Scheduler)ì™€ image decoder (Autoencoder decoder)ë¡œ êµ¬ì„±ë¨
  - Image information creator: latent space to latent space. Diffusion process ìˆ˜í–‰
  - Image decoder: latent space to image space
- Text conditioning: UNet ë‚´ë¶€ì˜ resnet block ì‚¬ì´ì— attention layerë¥¼ ì¶”ê°€í•˜ê³ , token embeddingì„ ê° attention layerì˜ ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ conditioning

##### ğŸ¤– ML & DL

*2023.04.08*

Random thought of AI tech.

- ìµœê·¼ì— ë‚˜ì˜¨ ë…¼ë¬¸ì¸ Segment Anythingê³¼ PIX2STRUCTë¥¼ ì½ìœ¼ë©° ë“  (ì´ì „ë¶€í„° ìì£¼ í–ˆì§€ë§Œ ë” ê°•í•´ì§„) ìƒê°ì€, 'í•™ìŠµì„ ìœ„í•œ taskë¥¼ ì–´ë–»ê²Œ ì •ì˜í•˜ëŠ”ì§€', ê·¸ë¦¬ê³  'ìˆ˜ë§ì€ ì–‘ì˜ training ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ëª¨ì•„ì•¼í•˜ëŠ”ì§€' ê³ ë¯¼í•˜ëŠ” ê²ƒì´ powerfulí•œ ëª¨ë¸ì„ ë§Œë“œëŠ” ì œì¼ ì¤‘ìš”í•œ ê¸°ë°˜ì´ ë  ê²ƒì´ë¼ëŠ” ê²ƒ
- ê´€ë ¨í•˜ì—¬ Video PreTraining (VPT)ë„ ì´ëŸ° ìƒê°ì„ ê¸°ë°˜ìœ¼ë¡œ ì—°êµ¬ë˜ì—ˆìŒ

##### ğŸ§© ML library

*2023.05.05*

Lightningì—ì„œ Distributed Data Parallel ì‚¬ìš©í•  ë•Œ ì°¸ê³ í•  ì ì— ëŒ€í•´ ê¸°ë¡í•©ë‹ˆë‹¤.

- ì°¸ê³  ë§í¬:  https://github.com/Lightning-AI/lightning/discussions/6501#discussioncomment-553152
- `sync_dist=True` ì˜µì…˜ì„ ì‚¬ìš©í•˜ë©´ ëª¨ë“  processì— ëŒ€í•´ sync ë§ì¶¤. ê¸°ë³¸ ì˜µì…˜ì€ reduced mean
- ë‹¤ë§Œ, torchmetricsê³¼ ê´€ë ¨í•´ì„œëŠ” own sync codeê°€ ìˆê¸° ë•Œë¬¸ì— `self.log(...)`ì˜ `sync_dist`, `sync_dist_op`, `sync_dist_group`, `reduce_fx`, `tbptt_reduce_fx` flagsê°€ metric loggingì—ëŠ” ì „í˜€ ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ
- Metric syncëŠ” `metric.compute()` í•¨ìˆ˜ í˜¸ì¶œì‹œ ë™ì‘í•¨

##### ğŸ¤– ML & DL

*2023.05.05*

Reinforcement Learning from Human Feedback (RLHF)ì— ëŒ€í•´ ê¸°ë¡í•©ë‹ˆë‹¤

- ì˜ìƒ ë§í¬: https://www.youtube.com/watch?v=2MBJOuVq380
- ë…¼ë¬¸ ë§í¬: https://arxiv.org/pdf/2203.02155.pdf
- RLì„ ì´ìš©í•˜ì—¬ human feedbackìœ¼ë¡œë¶€í„° modelì„ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•. ë‹¤ë§Œ 2~3 ë‹¨ê³„ë¥¼ í†µí•´ ì‹¤ì œë¡œ ì™œ í•™ìŠµì´ ë˜ëŠ”ì§€ì— ëŒ€í•´ ì œëŒ€ë¡œ ì´í•´í•˜ì§€ ëª»í•´ì„œ ë‹¤ì‹œ ê³µë¶€í•  í•„ìš” ìˆìŒ.

1. Pretraining a language model (LM)
2. Gathering data and training a reward model
3. Fine-tuning the LM with reinforcement learning

##### ğŸ¤– ML & DL

*2023.05.05*

VQ-VAEì— ëŒ€í•´ ê¸°ë¡í•©ë‹ˆë‹¤.

- AutoEncoder: latent variable $z$ë¥¼ ì˜ ì¶”ì¶œí•˜ê¸° ìœ„í•œ êµ¬ì¡°
- VAE: $z$ encodingì˜ distributionì´ priorë¡œ ì£¼ì–´ì§
- VQ-VAE
  - AutoEncoderì™€ ê°™ì€ êµ¬ì¡°ì´ê¸´ í•˜ë‚˜, $z$ ê¸°ë°˜ìœ¼ë¡œ codebook(Kê°œì˜ embeddings) ë‚´ ê°€ì¥ ê°€ê¹Œìš´ embeddingì„ ê°€ì ¸ì™€ì„œ decoder inputìœ¼ë¡œ ì‚¬ìš©í•¨. codebookì„ ê±°ì³ ê°€ì ¸ì˜¤ê¸° ë•Œë¬¸ì— vector quantizationì„ (codebookì— ëŒ€í•œ ì„¤ëª…ì€ [ì´ ë¸”ë¡œê·¸ í¬ìŠ¤íŒ…](https://zerojsh00.github.io/posts/Vector-Quantization/) ì°¸ê³ )
  - Posteriorì™€ priorê°€ categorical distributionì„
  - í•œê°€ì§€ ì˜ë¬¸: KëŠ” image ìƒ˜í”Œ ìˆ˜ì™€ ê°™ì€ì§€ê°€ ê¶ê¸ˆí•¨
  - Forward pass: ìœ„ì—ì„œ ë§í•œëŒ€ë¡œ codebookì—ì„œ ìœ ì‚¬í•œ embeddingì„ ê°€ì ¸ì™€ì„œ decoderì— feed forward
  - Backward pass: decoderëŠ” ê·¸ëŒ€ë¡œ backward propagation ìˆ˜í–‰í•˜ëŠ”ë°, codebookì—ì„œ embedding ê³ ë¥´ëŠ” ë¶€ë¶„ì€ argminì— ì˜í•´ backprop ë  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, decoderì˜ gradientë¥¼ encoder ëë‹¨ì— ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜´
  - Loss: (encoder-decoderì— ëŒ€í•œ reconstruction error) + (codebook embeddingì´ encoder outputê³¼ ìœ ì‚¬í•´ì§€ë„ë¡ ë•ëŠ” l2 loss) + (encoder outputì´ codebook embeddingê³¼ ìœ ì‚¬í•´ì§€ë„ë¡ ë•ëŠ” l2 loss)

##### ğŸ¤– ML & DL

*2023.05.12*

Metaì—ì„œ 5ì›” 9ì¼ì— ë°œí‘œí•œ ImageBindì— ëŒ€í•´ì„œ ê¸°ë¡í•©ë‹ˆë‹¤.

- 6 mocailities(Image/Video, Text, Heatmap, Depth, Audio, IMU)ë¡œ í•™ìŠµëœ ëª¨ë¸ì´ one modaility specialist modelì˜ ì„±ëŠ¥ì„ ë„˜ê¹€
- íŠ¹íˆ, ì´ ë¿ë§Œ ì•„ë‹ˆë¼ ì—¬ëŸ¬ modality ê¸°ë°˜ìœ¼ë¡œ ë‹¤ë¥¸ modalityë¡œì˜ ì „ì´, ì˜ˆë¥¼ ë“¤ì–´ audio ê¸°ë°˜ìœ¼ë¡œ image ìƒì„± ë“±ì˜ multi-modality ì—°êµ¬ë¡œ í™•ì¥ ê°€ëŠ¥
- Cross-modal retrieval, embedding-space arithmetic, audio-to-image generation ë“± ê°€ëŠ¥
- ìµœê·¼ Metaì˜ open source AI toolë“¤ì˜ ì§‘í•©ì²´ì„. DINO v2, SAM ë“±ì„ í¬í•¨í•˜ê³  ìˆìŒ
- For the four additional modalities (audio, depth, thermal, and IMU readings), ImageBind use naturally paired self-supervised data. ì¦‰, image í˜¹ì€ videoë¥¼ ë‹¤ë¥¸ modailityì™€ pair ì‹œí‚´ìœ¼ë¡œì¨ 6ê°œì˜ modalityë¥¼ ëª¨ë‘ combine í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ImageBindê°€ ë³´ì„

##### ğŸ¤– ML & DL

*2023.05.15*

ViTì™€ CNNì— ëŒ€í•œ ë¹„êµ: [How Do Vision Transformers Work?](https://arxiv.org/abs/2202.06709)

- ViT ì¦‰, Multi-head Self Attention(MSA)ì€ shape(structure) biased = low-pass filter
- ResNet ì¦‰, ConvNetì€ texture biased = high-pass filter

CL ViTì™€ MIM ViTì— ëŒ€í•œ ë¹„êµ: [What Do Self-Supervised Vision Transformers Learn?](https://arxiv.org/abs/2305.00729)

- CL: self-attentions collapse into homogeneity ë°œìƒ / utilizes the low-frequency signals / a crucial role in the later layers
- MIM: utilizes high-frequency signals / focuses on the early layers

##### ğŸ¤– ML & DL

*2023.05.20*

- Hyper-parameter tuning ê³ ë¯¼: shell script ì§œì„œ ë¯¸ë¦¬ ì •í•œ ruleì— ë”°ë¼ ì‹¤í—˜ ì˜µì…˜ ì—¬ëŸ¬ ê°œ ëŒë¦¬ê³ , wandb runsì—ì„œ ì›í•˜ëŠ” optionsë“¤ë§Œ ë„ì›Œì„œ í‘œ í˜•íƒœë¡œ ë³´ëŠ”ê²Œ ì œì¼ í¸ë¦¬í•œë“¯

##### ğŸ¤– ML & DL

*2023.05.20*

ì„œë¹„ìŠ¤ ê°€ëŠ¥í•œ AIëŠ” ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ ê²°ì •ë˜ëŠ”ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì— ëŒ€í•´ ChatGPTê°€ ë‹µë³€í•œ ë‚´ìš©. ì¶©ë¶„íˆ ê³ ë¯¼í•´ë³¼ë§Œí•œ ë‚´ìš©ì¸ ê²ƒ ê°™ì•„ ê¸°ë¡í•¨

1. Define requirements: Clearly identify the specific tasks or problems the AI model needs to address. Determine the desired input-output behavior, performance metrics, scalability, and any other relevant criteria.
2. Training and validation data: The data should cover various scenarios that the AI model will encounter in real-world usage.
3. Model selection: Consider factors like the model's architecture, complexity, size, computational requirements, and availability of resources.
4. Model evaluation: Common metrics include accuracy, precision, recall, F1 score, or domain-specific metrics relevant to the task.
5. Testing and validation: Deploy the AI model in a controlled or limited production environment. Validate its performance against real-world data or simulated scenarios, including edge cases and corner cases.
6. Iterative improvement: Continuously monitor and evaluate the AI model's performance in a live or simulated environment. Collect user feedback and address any issues or limitations through iterative updates, such as fine-tuning, retraining, or architecture modifications.
7. Ethical considerations: Evaluate the AI model's compliance with ethical guidelines, privacy requirements, and legal regulations.
8. Scalability and resource requirements: Assess the AI model's scalability and resource demands, such as computing power, memory, or network bandwidth.
9. Robustness and reliability: Test the AI model's robustness by subjecting it to adversarial attacks, noisy or incomplete data, or other challenging conditions. Assess its reliability by measuring its performance over an extended period, considering factors like model drift or degradation.
10. Cost considerations: Evaluate the total cost of deploying and maintaining the AI model, including infrastructure, licensing, data storage, and ongoing support. Consider the model's value proposition and its impact on productivity, efficiency, or revenue generation.

##### ğŸ¤– ML & DL

DINOì™€ DINO v2ì— ëŒ€í•´ì„œ ê°„ë‹¨íˆ ì •ë¦¬í•©ë‹ˆë‹¤.

- Self-supervised ViTì˜ íŠ¹ì§•: scene layout ê²½ê³„ íŒŒì•… ì˜í•˜ë©°, featureë§Œ ê°€ì§€ê³  k-NN classifier ë§Œë“¤ì–´ë„ ì„±ëŠ¥ ì¢‹ìŒ
- ë‹¤ë§Œ k-NN classifier ì„±ëŠ¥ ìœ„í•´ì„œëŠ”, momentum encoder, multi-crop augmentation, small patchesê°€ ìš”êµ¬ë˜ëŠ” ê²ƒ ë°œê²¬
- DINO: momentum encoder ê¸°ë°˜ BYOL ë°©ì‹ ì°¨ìš©. ì—¬ê¸°ì— loss ì‹ì— ì¡°ê¸ˆ ì°¨ì´ ê°€ì§€ê³ , teacher-student êµ¬ì¡° ë™ì¼í•¨
- DINO v2: Image levelë¡œëŠ” ì„œë¡œ ë‹¤ë¥¸ ì´ë¯¸ì§€ êµ¬ë¶„, patch level ê°™ì€ ì´ë¯¸ì§€ ë‚´ ì„œë¡œ ë‹¤ë¥¸ patch êµ¬ë¶„. ì´ ì™¸ì—ë„ ë§ì€ ì–‘ì˜ â€˜í€„ë¦¬í‹° ì¢‹ì€â€™ ë°ì´í„°ì™€ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ í•™ìŠµ ë°©ë²• ì œì•ˆ

##### ğŸ§© ML library

*2023.08.12*

- Apache Arrow: ì§ë ¬í™”ì™€ ì—­ì§ë ¬í™”ì˜ ì˜¤ë²„í—¤ë“œê°€ ë†’ë‹¤ëŠ” ê²ƒì€ ë°ì´í„° ë‹¤ë£° ë•Œ ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œì ì„. Apach ArrowëŠ” ì§ë ¬í™” ê³¼ì •ì´ ì—†ëŠ” zero-copy readê°€ ê°€ëŠ¥í•œë°, ì¼ë°˜ì ì¸ ë°©ë²•ì¸ ê°ì²´ë¥¼ ê°€ì§€ê³  ì‘ì—…í•˜ëŠ” ë°©ì‹ì´ ì•„ë‹Œ ì§ë ¬í™”ëœ ë°ì´í„° ìì²´ë¥¼ ê°€ì§€ê³  ì‘ì—…í•˜ê¸° ë•Œë¬¸ì— ì´ê²ƒì´ ê°€ëŠ¥
  - Main purpose: Language-independent open standards and libraries to accelerate and simplify in-memory computing
- Huggingface datasets w. arrow: ìœ„ì—ì„œ ì–¸ê¸‰í•œ ê²ƒ ì²˜ëŸ¼ ArrowëŠ” ë§ì€ ì–‘ì˜ ë°ì´í„°ì— ëŒ€í•œ ì²˜ë¦¬ì™€ ì´ë™ì„ ë¹ ë¥´ê²Œ ê°€ëŠ¥í•˜ê²Œ í•¨ (Arrow formatì€ zero-copy read ê°€ëŠ¥í•˜ê¸°ì— ì§ë ¬í™” ì˜¤ë²„í—¤ë“œë¥¼ ì—†ì• ì£¼ê¸° ë•Œë¬¸). ë”°ë¼ Huggingface datasetsì€ arrow í™œìš©í•¨. ë˜í•œ column-orientedì´ê¸° ë•Œë¬¸ì— queryingì´ë‚˜ slicing ë“± ì²˜ë¦¬ ì†ë„ ë¹ ë¦„

### References

[^1]: Wikipedia contributors. (2021, April 12). Moment (mathematics). In Wikipedia, The Free Encyclopedia. Retrieved 12:08, May 24, 2021, from https://en.wikipedia.org/w/index.php?title=Moment_(mathematics)&oldid=1017468752
[^2]: JinWon Lee - PR-317: MLP-Mixer: An all-MLP Architecture for Vision. https://www.youtube.com/watch?v=KQmZlxdnnuY
[^3]: JoonYoung Yi - Slideshare, Dynamically Expandable Network (DEN). https://www.slideshare.net/ssuser62b35f/180808-dynamically-expandable-network

[^4]: Wikipedia contributors. (2021, August 1). Signed distance function. In *Wikipedia, The Free Encyclopedia*. Retrieved 00:41, November 14, 2021, from https://en.wikipedia.org/w/index.php?title=Signed_distance_function&oldid=1036639454
[^5]: Park, Jeong Joon, et al. "Deepsdf: Learning continuous signed distance functions for shape representation." *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2019.
[^6]: 1.3.6.1.What is a Probability Distribution., *NIST/SEMATECH e-Handbook of Statistical Methods*, http://www.itl.nist.gov/div898/handbook/, December 2, 2021.

[^ 7]: Olivier Moindrot. "Triplet Loss and Online Triplet Mining in TensorFlow". https://omoindrot.github.io/triplet-loss, Mar 19, 2018.

[^8]: Wikipedia contributors. (2022, April 27). Mooreâ€“Penrose inverse. In *Wikipedia, The Free Encyclopedia*. Retrieved 06:08, May 16, 2022, from [https://en.wikipedia.org/w/index.php?title=Moore%E2%80%93Penrose_inverse&oldid=1085006448](https://en.wikipedia.org/w/index.php?title=Mooreâ€“Penrose_inverse&oldid=1085006448)
[^9]: https://github.com/onnx/onnx/blob/main/docs/Overview.md
[^10]: Mermillod, Martial, AurÃ©lia Bugaiska, and Patrick Bonin. "The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects." *Frontiers in psychology* 4 (2013): 504.
