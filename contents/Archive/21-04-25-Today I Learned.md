---
title: "Today I Learned"
date: "2021-04-25"
template: "post"
draft: false
path: "/cheatsheet/21-04-25/"
description: "ìƒˆë¡­ê²Œ ì•Œê²Œ ëœ ì§€ì‹ ì¤‘ì—ì„œ í•˜ë‚˜ì˜ í¬ìŠ¤íŒ…ìœ¼ë¡œ ë§Œë“¤ê¸°ì—ëŠ” ë¶€ë‹´ìŠ¤ëŸ¬ìš´ ë‚´ìš©ë“¤ì„ ì´ê³³ì— ëª¨ì•„ë‘¡ë‹ˆë‹¤. ë§¤ì¼ ê³µë¶€í•œ ë‚´ìš©ì„ ê¸°ë¡í•˜ê¸°ë³´ë‹¤ëŠ” ì•„ë¬´ë•Œë‚˜ ì—…ë°ì´íŠ¸ í•  ìƒê°ì…ë‹ˆë‹¤! ë‚˜ì¤‘ì—ëŠ” ì¹´í…Œê³ ë¦¬ í¬ìŠ¤íŒ…ì„ ë‚˜ëˆŒ ìˆ˜ ìˆì„ ì •ë„ë¡œ ë‚´ìš©ì´ ì—„ì²­ ë§ì•„ì¡Œìœ¼ë©´ ì¢‹ê² ë„¤ìš”. ë‚˜ì¤‘ì—ëŠ” ì¹´í…Œê³ ë¦¬ ë³„ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆì„ ì •ë„ë¡œ ë‚´ìš©ì´ ì—„ì²­ ë§ì•„ì¡Œìœ¼ë©´ ì¢‹ê² ë„¤ìš”. (ìµœê·¼ì— ì‘ì„±í•œ ë‚´ìš©ë“¤ì´ í•˜ë‹¨ì— ìœ„ì¹˜í•˜ë„ë¡ ë°°ì—´í•˜ì˜€ìŠµë‹ˆë‹¤)"
category: "Cheat Sheet"
---

ìƒˆë¡­ê²Œ ì•Œê²Œ ëœ ì§€ì‹ ì¤‘ì—ì„œ í•˜ë‚˜ì˜ í¬ìŠ¤íŒ…ìœ¼ë¡œ ë§Œë“¤ê¸°ì—ëŠ” ë¶€ë‹´ìŠ¤ëŸ¬ìš´ ë‚´ìš©ë“¤ì„ ì´ê³³ì— ëª¨ì•„ë‘¡ë‹ˆë‹¤. ë§¤ì¼ ê³µë¶€í•œ ë‚´ìš©ì„ ê¸°ë¡í•˜ê¸°ë³´ë‹¤ëŠ” ì•„ë¬´ë•Œë‚˜ ì—…ë°ì´íŠ¸ í•  ìƒê°ì…ë‹ˆë‹¤! ë‚˜ì¤‘ì—ëŠ” ì¹´í…Œê³ ë¦¬ í¬ìŠ¤íŒ…ì„ ë‚˜ëˆŒ ìˆ˜ ìˆì„ ì •ë„ë¡œ ë‚´ìš©ì´ ì—„ì²­ ë§ì•„ì¡Œìœ¼ë©´ ì¢‹ê² ë„¤ìš” ğŸ¤“

> ìµœê·¼ì— ì‘ì„±í•œ ë‚´ìš©ë“¤ì´ í•˜ë‹¨ì— ìœ„ì¹˜í•˜ë„ë¡ ë°°ì—´í•˜ì˜€ìŠµë‹ˆë‹¤.

##### ğŸ¥§ Python

*2021.04.25*

[íŒŒì´ì¬ ë„íë¨¼íŠ¸](https://docs.python.org/3/reference/simple_stmts.html#future)ì˜ `future` ë¬¸ì— ëŒ€í•œ ì„¤ëª…ì„ ì½ì—ˆìŠµë‹ˆë‹¤. `future` ë¬¸ì€ ë¯¸ë˜ ë²„ì „ íŒŒì´ì¬ì˜ ê¸°ëŠ¥ë“¤ì„ ì‰½ê²Œ ë§ˆì´ê·¸ë ˆì´ì…˜(í•˜ë‚˜ì˜ ìš´ì˜í™˜ê²½ì—ì„œ ë‹¤ë¥¸ ìš´ì˜í™˜ê²½ìœ¼ë¡œ ì˜®ê¸°ëŠ” ê²ƒ)í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤. import ë’¤ì— ë”°ë¼ì˜¤ëŠ” new featureê°€ ë§Œì•½ íŒŒì´ì¬ 3ì˜ ê¸°ëŠ¥ì´ë¼ê³  í•˜ë”ë¼ë„ íŒŒì´ì¬ 2 ë²„ì „ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ë©ë‹ˆë‹¤.

```python
from __future__ import print_function
```

##### ğŸ§© ML library

*2021.04.25* 

[í…ì„œí”Œë¡œìš° ê³µì‹ë¬¸ì„œ](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/map_fn)ì˜ `tf.map_fn` í•¨ìˆ˜ì— ëŒ€í•œ ì„¤ëª…ì„ ì½ì—ˆìŠµë‹ˆë‹¤. dimension 0ì—ì„œ unpackëœ elemsì´ë¼ëŠ” tensor listì˜ ìš”ì†Œë“¤ì„ fnì— mapí•©ë‹ˆë‹¤. 

```python
tf.map_fn(fn, elems, dtype=None, parallel_iterations=None, back_prop=True,
    	  swap_memory=False, infer_shape=True, name=None)
```

MAMLì„ êµ¬í˜„ í•  ë•Œ meta-batchì— ëŒ€í•œ cross entropyë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œ ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ xsì˜ shapeì€ [meta-batch size, nway\*kshot, 84\*84\*3] ì…ë‹ˆë‹¤.

```python
cent, acc = tf.map_fn(lambda inputs: self.get_loss_single(inputs, weights),
					 elems=(xs, ys, xq, yq),
				 	 dtype=(tf.float32, tf.float32),
				 	 parallel_iterations=self.metabatch)
```

##### ğŸ§©  ML library

*2021.04.27*

ëª¨ë¸ ê·¸ë˜í”„ë¥¼ ë¹Œë“œí•˜ëŠ” í•¨ìˆ˜ì—ì„œ for loopë¥¼ ë§ì´ ì‚¬ìš©í•˜ë©´ ì´ê²Œ ê·¸ëŒ€ë¡œ ëª¨ë¸ training ë‹¨ê³„ì—ì„œë„ ë§¤ë²ˆ for loopê°€ ì ìš©ë˜ì–´ ëª¨ë¸ì˜ í•™ìŠµì´ ëŠë ¤ì§€ê² êµ¬ë‚˜ë¼ê³  ìƒê°í–ˆì—ˆëŠ”ë° ê³°ê³°íˆ ìƒê°í•´ë³´ë‹ˆê¹Œ ì•„ë‹ˆë”ë¼êµ¬ìš”. 

ë¹Œë“œí•˜ëŠ” ë‹¨ê³„ì—ì„œëŠ” for loopê°€ ì—¬ëŸ¬ ë²ˆ ëŒë”ë¼ë„, ê·¸ë˜í”„ì˜ ê° ë…¸ë“œë“¤ì´ ì—°ê²°ë˜ê³  ë‚œ ë’¤ì—ëŠ” ë¹Œë“œ ëœ ê·¸ë˜í”„ êµ¬ì¡° ìì²´ê°€ ì¤‘ìš”í•˜ì§€, ë¹Œë“œ ë‹¨ê³„ì—ì„œì˜ for loopëŠ” ê´€ë ¨ì´ ì—†ê²Œ ë©ë‹ˆë‹¤. ê½¤ë‚˜ ì˜¤ë«ë™ì•ˆ ì•„ë¬´ë ‡ì§€ ì•Šê²Œ ì°©ê°í•˜ê³  ìˆì—ˆì–´ì„œ ì´ ê³³ì— ê¸°ë¡í•©ë‹ˆë‹¤. ê·¸ëŸ¼ map\_fnì€ íŠ¹íˆ ì–´ë–¤ ê²½ìš°ì— ë©”ë¦¬íŠ¸ë¥¼ ê°€ì§ˆê¹Œ ê¶ê¸ˆí•˜ê¸´ í•˜ë„¤ìš” ğŸ§

##### ğŸ§©  ML library

*2021.05.02*

TensorFlow 1.15ë¡œ ì½”ë“œë¥¼ ì§œë‹¤ê°€ `softmax_cross_entropy_with_logits`ëŠ” lossì— ëŒ€í•œ 2nd-order ê³„ì‚°ì„ ì§€ì›í•˜ì§€ë§Œ `sparse_softmax_cross_entropy_with_logits`ëŠ” lossì— ëŒ€í•œ 2nd-order ê³„ì‚°ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ”ê±¸ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë‘˜ì˜ ì°¨ì´ëŠ” labelì´ one-hot í˜•íƒœë¡œ ì£¼ì–´ì§€ëƒ ì•„ë‹ˆëƒì˜ ì°¨ì´ë°–ì— ì—†ëŠ”ë° ì´ëŸ° ê²°ê³¼ë¥¼ ë‚˜íƒ€ëƒˆë‹¤ëŠ”ê²Œ ì´ìƒí•´ì„œ ì°¾ì•„ë³´ë‹¤ê°€ tensorflow repositoryì— [ê´€ë ¨ ì´ìŠˆ](https://github.com/tensorflow/tensorflow/issues/5876)ê°€ ì˜¬ë¼ì™”ë˜ ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

ìš”ì•½í•˜ìë©´ ì¼ë¶€ indexing ì‘ì—…ì— ëŒ€í•œ ë„í•¨ìˆ˜ ê³„ì‚°ì´ ì•„ì§ ì œëŒ€ë¡œ êµ¬í˜„ë˜ì§€ ì•Šì•˜ê±°ë‚˜, ëª‡ ê°€ì§€ operationì— ëŒ€í•´ì„œ 2ì°¨ ë¯¸ë¶„ ê³„ì‚°ì´ ê°œë°œìë“¤ë„ ì•„ì§ í•´ê²°í•˜ì§€ ëª»í•œ ì˜¤ë¥˜ë¥¼ ê°€ì§„ë‹¤ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤(êµ¬ì²´ì ì¸ ì›ì¸ì€ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤). 0.2 ë²„ì „ì—ì„œ 1.15 ê¹Œì§€ ê°œë°œì´ ì§„í–‰ë˜ë©´ì„œë„ TensorFlow íŒ€ì´ ì§€ì†ì ìœ¼ë¡œ í•´ê²°í•˜ì§€ ëª»í•˜ê³  ìˆëŠ” ë¬¸ì œì ì´ ìˆë‹¤ëŠ” ê²ƒì´ ì‹ ê¸°í–ˆìŠµë‹ˆë‹¤.

##### ğŸ¤– ML & DL

*2021.05.10*

[PR-317: MLP-Mixer: An all-MLP Architecture for Vision](https://www.youtube.com/watch?v=KQmZlxdnnuY) ì˜ìƒì„ í†µí•´ CNNê³¼ MLPê°€ ë³„ë¡œ ë‹¤ë¥´ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ì•Œì•˜ìŠµë‹ˆë‹¤. ì˜ìƒì—ì„œ ì´ì§„ì›ë‹˜ì€ CNN weightì´ Fully-Conneted weightê³¼ ë‹¤ë¥¸ ì  ë‘ ê°€ì§€ê°€ weight sharingê³¼ locally connectedë¼ê³  ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì‹œê°í™”ëœ ìë£Œë§Œ ë´ë„ ì´ë ‡ê²Œ ê°„ë‹¨í•˜ê²Œ ì´í•´ë˜ëŠ” ë‚´ìš©ì¸ë° ì™œ ì§€ê¸ˆê¹Œì§€ ê¹¨ë‹«ì§€ ëª»í–ˆì„ê¹Œë¼ëŠ” ìƒê°ì´ ë“¤ì—ˆê³ , CNNì— ëª‡ ê°œì˜(ì‚¬ì‹¤ì€ ì—„ì²­ ë§ì€ ì–‘ì´ì§€ë§Œ) weightì„ ì¶”ê°€í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ Fully-Connectedì™€ ì™„ì „íˆ ë™ì¼í•œ êµ¬ì¡°ë¡œ ë§Œë“¤ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì´í•´í–ˆìŠµë‹ˆë‹¤.

##### ğŸ§©  ML library

*2021.05.11*

`tf.contrib.layers.batch_norm` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ë•Œ `is_traning` ì•„ê·œë¨¼íŠ¸ ì„¤ì •ì— ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤. Batch normalizationì„ ì‚¬ìš©í•  ë•Œ í•™ìŠµ ìƒí™©ì¸ì§€ í…ŒìŠ¤íŠ¸ ìƒí™©ì¸ì§€ì— ë”°ë¼ì„œ meanê³¼ varianceë¡œ ì‚¬ìš©í•˜ëŠ” statisticsì˜ ì¶œì²˜ê°€ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— `is_traning`ë¥¼ ì˜ëª» ì„¤ì •í•œë‹¤ë©´ ì •í™•ë„ëŠ” ë†’ê²Œ ë‚˜ì˜¤ë”ë¼ë„ ê·¸ ì‹¤í—˜ì´ ì˜ëª»ëœ ê²°ê³¼ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

`is_training`ì´ Trueì¸ ê²½ìš°ì—ëŠ” moving_mean í…ì„œì™€ moving_variance í…ì„œì— statistics of the moments(ë¯¸ë‹ˆ ë°°ì¹˜ í‰ê· ê³¼ ë¶„ì‚°)ì„ exponential moving average ì‹ì— ë”°ë¼ ì¶•ì í•©ë‹ˆë‹¤. BN ê³„ì‚°ì—ëŠ” ë¯¸ë‹ˆë°°ì¹˜ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.  `is_training`ì´ Falseì¸ ê²½ìš°ì—ëŠ” ê·¸ë™ì•ˆ ì¶•ì í•˜ì˜€ë˜ moving_mean í…ì„œì™€ moving_variance í…ì„œ ê°’ì„ ê°€ì ¸ì™€ BN ê³„ì‚°ì— ì‚¬ìš©í•©ë‹ˆë‹¤. 

Few-shot learning settingì—ì„œ support setê³¼ query setì— ëŒ€í•´ì„œ ë‘˜ ë‹¤ `is_training`ì„ Trueë¡œ ì„¤ì •í•˜ë©´ ì´ëŠ” transductive settingì´ ë©ë‹ˆë‹¤. ì¦‰ queryë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ì„œ support ë¿ë§Œ ì•„ë‹ˆë¼ query ë¶„í¬ì˜ ì •ë³´ê¹Œì§€ ì‚¬ìš©í•˜ê² ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. Few-shot learningì—ì„œëŠ” ëŒ€ë¶€ë¶„ transductive settingì´ non-transductiveì— ë¹„í•´ 3%ì •ë„ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì´ê¸° ë•Œë¬¸ì— ë³¸ì¸ì˜ ì‹¤í—˜ ìƒí™©ì— ì•Œë§ê²Œ ì•„ê·œë¨¼íŠ¸ ê°’ì„ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. 

`tf.contrib.layers.group_norm` ê°™ì€ instance-based normalization ë°©ì‹ì€ ë¯¸ë‹ˆë°°ì¹˜ì— ëŒ€í•œ running statisticsë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— `is_trainable` íŒŒë¼ë¯¸í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

##### ğŸ¤– ML & DL

*2021.05.14*

Moment[^1]ëŠ” ë¬¼ë¦¬í•™ì—ì„œ íŠ¹ì • ë¬¼ë¦¬ëŸ‰ê³¼ distanceì˜ ê³±ì„ í†µí•´ ë¬¼ë¦¬ëŸ‰ì´ ê³µê°„ìƒ ì–´ë–»ê²Œ ìœ„ì¹˜í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ë©° Force, Torque, Angular momentum ë“±ì„ ì˜ˆë¡œ ë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Moment of massì— ëŒ€í•´ì„œ zeroth momentëŠ” total mass, 1st momentëŠ” center of mass, 2nd momentëŠ” moment of inertiaë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

ìˆ˜í•™ì—ì„œëŠ” í•¨ìˆ˜ì˜ íŠ¹ì§•ì„ ë‚˜íƒ€ë‚´ê¸°ìœ„í•´ momentë¼ëŠ” ì›Œë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í•¨ìˆ˜ê°€ í™•ë¥ ë¶„í¬ í˜•íƒœì¸ ê²½ìš° first momentëŠ” í™•ë¥  ë¶„í¬ì˜ ê¸°ëŒ“ê°’ì„ ì˜ë¯¸í•˜ë©°, ì´ë¥¼ moments about zeroë¼ê³ ë„ ë§í•©ë‹ˆë‹¤. ë˜í•œ second central momentë¡œëŠ” variance, third standardized momentëŠ” skewness(ë¹„ëŒ€ì¹­ë„),  fourth standardized momentëŠ” kurtosis(ì²¨ë„, ë¾°ì¡±í•œ ì •ë„) ë“±ì´ ìˆìŠµë‹ˆë‹¤.

##### ğŸ‘¨â€ğŸ’» CS

*2021.05.24*

[API](https://ko.wikipedia.org/wiki/API)(Application Programming Interfaces)[^2]ëŠ” ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡, ìš´ì˜ ì²´ì œë‚˜ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ê°€ ì œê³µí•˜ëŠ” ê¸°ëŠ¥ì„ ì œì–´í•  ìˆ˜ ìˆê²Œ ë§Œë“  ì¸í„°í˜ì´ìŠ¤ë¥¼ ë§í•©ë‹ˆë‹¤. ì™¸ë¶€ì™€ ìƒˆë¡œìš´ ì—°ê²°ë“¤ì„ êµ¬ì¶•í•  í•„ìš” ì—†ì´ ë‚´ë¶€ ê¸°ëŠ¥ë“¤ì´ ì„œë¡œ ì˜ í†µí•©ë˜ì–´ ìˆìœ¼ë©°, APIë¥¼ ì‚¬ìš©í•˜ë©´ í•´ë‹¹ APIì˜ ìì„¸í•œ ì‘ë™ì›ë¦¬ì™€ êµ¬í˜„ë°©ì‹ì€ ì•Œì§€ ëª»í•´ë„, ì œí’ˆ/ì„œë¹„ìŠ¤ê°„ì— ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ì›¹ APIê°€ ëŠ˜ì–´ë‚˜ë©´ì„œ ë©”ì„¸ì§€ ì „ë‹¬ì„ ìœ„í•œ í‘œì¤€ì„ ë§Œë“¤ê³ ì SOAP(Simple Object Access Protocol)ê°€ ê°œë°œë˜ì—ˆê³ , ìµœê·¼ ì›¹ APIë¡œëŠ” [REST](https://ko.wikipedia.org/wiki/REST)ful APIë¼ëŠ” *ì•„í‚¤í…ì³ ìŠ¤íƒ€ì¼*ì´ ë” ë§ì´ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. RESTëŠ” ê·œì •ëœ í”„ë¡œí† ì½œì´ ì•„ë‹ˆë¼ ì•„í‚¤í…ì³ ìŠ¤íƒ€ì¼ì´ê¸° ë•Œë¬¸ì— ì •í•´ì§„ í‘œì¤€ì€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë§Œ Roy Fieldingì˜ ë…¼ë¬¸ì— ì •ì˜ëœ ì•„ë˜ì˜ 6ê°€ì§€ ì›ì¹™ì„ ê¸°ë³¸ìœ¼ë¡œ í•©ë‹ˆë‹¤. (ìì„¸í•œ ì„¤ëª…ì€ ìœ„í‚¤í”¼ë””ì•„ ë¬¸ì„œ[^3] ì°¸ê³ )

- `ì¸í„°í˜ì´ìŠ¤ ì¼ê´€ì„±`, `ë¬´ìƒíƒœ(Stateless)`, `ìºì‹œ ì²˜ë¦¬ ê°€ëŠ¥(Cacheable)`, `ê³„ì¸µí™”(Layered System)`, `Code on demand (optional)`, `í´ë¼ì´ì–¸íŠ¸/ì„œë²„ êµ¬ì¡°`

[URI](https://ko.wikipedia.org/wiki/%ED%86%B5%ED%95%A9_%EC%9E%90%EC%9B%90_%EC%8B%9D%EB%B3%84%EC%9E%90)ëŠ” Uniform Resource Identifier(í†µí•© ìì› ì‹ë³„ì)[^4]ì˜ ì•½ìë¡œ íŠ¹ì • ìì›ì˜ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ì£¼ëŠ” ìœ ì¼í•œ ì£¼ì†Œë¥¼ ë§í•©ë‹ˆë‹¤. RESTful APIëŠ” ì›¹ ìƒì—ì„œ ì‚¬ìš©ë˜ëŠ” ë¦¬ì†ŒìŠ¤ë¥¼ HTTP URIë¡œ í‘œí˜„í•˜ê³ , ë¦¬ì†ŒìŠ¤ì— ëŒ€í•œ ì‘ì—…ë“¤ì„ HTTP Methodë¡œ ì •ì˜í•©ë‹ˆë‹¤.

##### ğŸ¥§ Python

*2021.05.24*

íŒŒì´ì¬ì˜ ê°ì²´ëŠ” ê·¸ ì†ì„±ì´ mutable(ê°’ì´ ë³€í•œë‹¤)ê³¼ immutableë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤. ([ì´ê³³](https://wikidocs.net/32277)ê³¼ [ì´ê³³](https://wikidocs.net/16038)ì„ ì°¸ê³ í•˜ì˜€ìŠµë‹ˆë‹¤.)

- Immutable : ìˆ«ì(number), ë¬¸ìì—´(string), íŠœí”Œ(tuple)
- Mutable : ë¦¬ìŠ¤íŠ¸(list), ë”•ì…”ë„ˆë¦¬(dictionary), NumPyì˜ ë°°ì—´(ndarray)

Immutable íƒ€ì…ì¸ intì— ëŒ€í•´ ì˜ˆë¥¼ ë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤.

```python
x = 1
y = x
y += 3

# results: x = 1, y = 4
```

ë‘ ë²ˆì§¸ ë¼ì¸ê¹Œì§€ëŠ” xì™€ yê°€ 1ì´ë¼ëŠ” ë™ì¼í•œ ***ê°ì²´***ë¥¼ ê°€ë¦¬í‚¤ê³  ìˆìŠµë‹ˆë‹¤. ì„¸ ë²ˆì§¸ì—ì„œ yì˜ ê°’ì„ ë³€ê²½í•˜ëŠ” ìˆœê°„ yëŠ” 4ë¥¼, xëŠ” 1ì„ ê°€ë¦¬í‚¤ê²Œ ë©ë‹ˆë‹¤.

C/C++ê°™ì€ ì–¸ì–´ ê´€ì ì—ì„œ ë³´ë©´ `y=x`ê°€ ì‹¤í–‰í•˜ëŠ” ìˆœê°„ ê°’ì„ ë³µì‚¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆì§€ë§Œ, íŒŒì´ì¬ì€ `y=x`ê°€ í˜¸ì¶œë˜ëŠ” ì‹œì ì—ëŠ” ë™ì¼í•œ ê°ì²´ë¥¼ ê°€ë¦¬í‚¤ë‹¤ê°€ immutable íƒ€ì…ì¸ yë¥¼ ë³€ê²½í–ˆì„ ë•Œ ë³€ê²½ë©ë‹ˆë‹¤.

##### ğŸ¥§ Python

*2021.08.05*

ìµœê·¼ì— ì•Œê²Œëœ ìœ ìš©í•œ Pycharm ë‹¨ì¶•í‚¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.

- ë³€ìˆ˜/í•¨ìˆ˜ê°€ ì‚¬ìš©ëœ ìœ„ì¹˜ ì°¾ê¸°: `Find Usages`, `Alt + F7` (`Option + F7`)
- ë³€ìˆ˜/í•¨ìˆ˜ ì„ ì–¸ë¶€ ì°¾ê¸°: `Ctrl + í´ë¦­` (`Command + í´ë¦­`)

##### ğŸ‘¨â€ğŸ’» CS 

*2021.08.25*

FLOPS[^9] (FLoating point Operations Per Second)ëŠ” '1ì´ˆ ë‹¹ ë¶€ë™ì†Œìˆ˜ì  ì—°ì‚°ëŸ‰'ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì»´í“¨í„°ì˜ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚¼ ë•Œ ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ìŠˆí¼ ì»´í“¨í„°ì˜ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚¼ ê²½ìš°ì—ëŠ” í…Œë¼í”Œë¡­ìŠ¤ TFLOPS(1Ã—1012 í”Œë¡­ìŠ¤)ê°€ ì£¼ë¡œ ì“°ì´ë©° PFLOPSëŠ” í˜íƒ€í”Œë¡­ìŠ¤ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

FLOPSì™€ FLOPsì˜ ì˜ë¯¸ëŠ” ë‹¤ë¦…ë‹ˆë‹¤. FLOPsëŠ” FLoating point Operationsì˜ ì•½ìì¸ë°, ì´ëŠ” 'ë¶€ë™ì†Œìˆ˜ì  ì—°ì‚°ëŸ‰'ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. FLOPs ê°™ì€ ê²½ìš°ì—ëŠ” ë”¥ëŸ¬ë‹ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ëª¨ë¸ì˜ í¬ê¸°, ëª¨ë¸ì˜ ì—°ì‚°ëŸ‰ì„ ë‚˜íƒ€ë‚´ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

##### ğŸ§© ML library

*2021.09.20*

[PyTorch ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze)ë¥¼ ì°¸ê³ í•˜ì—¬ ê°€ì¥ ê¸°ë³¸ì ì¸ torch Tensor ê¸°ëŠ¥ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

- squeeze: ì°¨ì›ì´ 1ì¸ ì°¨ì›ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ë”°ë¡œ ì˜µì…˜ì„ ì£¼ì§€ ì•Šìœ¼ë©´ ì°¨ì›ì´ 1ì¸ ëª¨ë“  ì°¨ì›ì„ ì œê±°í•©ë‹ˆë‹¤.
- unsqueeze: íŠ¹ì • ìœ„ì¹˜ì— 1ì¸ ì°¨ì›ì„ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜í™ë‹ˆë‹¤.
- view: í…ì„œì˜ shapeì„ ë³€ê²½í•´ì£¼ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

##### ğŸ¥§ Python

*2021.09.30*

[Python ê³µì‹ ë¬¸ì„œ](https://docs.python.org/ko/3/tutorial/modules.html)ë¥¼ ì°¸ê³ í•˜ì—¬ ëª¨ë“ˆê³¼ ëª¨ë“ˆì„±ì— ëŒ€í•´ ì •ë¦¬í•©ë‹ˆë‹¤.

í”„ë¡œê·¸ë¨ì˜ ìœ ì§€/ë³´ìˆ˜ë¥¼ ìœ„í•´ ì—¬ëŸ¬ ê°œì˜ íŒŒì¼ë¡œ ë‚˜ëˆ„ê³  ì‹¶ê±°ë‚˜, í•¨ìˆ˜ë¥¼ ì—¬ëŸ¬ í”„ë¡œê·¸ë¨ì— ë³µì‚¬í•˜ì§€ ì•Šê³ ë„ ì‚¬ìš©í•˜ê³  ì‹¶ì€ ê²½ìš°ì—, íŒŒì´ì¬ì€ ì •ì˜ë“¤ì„ íŒŒì¼ì— ë„£ê³  ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. ê·¸ëŸ° íŒŒì¼ì„ ëª¨ë“ˆ[^10]ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. ì¦‰, ë‹¤ë¥¸ íŒŒì´ì¬ í”„ë¡œê·¸ë¨ì—ì„œ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë§Œë“  ë˜ ë‹¤ë¥¸ íŒŒì´ì¬ íŒŒì¼ì„ ëª¨ë“ˆì´ë¼ê³  í•©ë‹ˆë‹¤.

##### ğŸ¤– ML & DL

*2021.11.13*

ìœ„í‚¤í”¼ë””ì•„ì˜ Signed Distance Function(SDF)[^12]ì— ëŒ€í•œ ì„¤ëª…ì„ ì½ì—ˆìŠµë‹ˆë‹¤. ë¨¼ì €, SDFëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤.

- If $\Omega$ is a subset of a metric space and $b$ is the boundary of $\Omega$ the signed distance function $f$ is defined by

$$
f(x)=
\begin{cases}
d(x, \partial \Omega) & \text{if } x \in \Omega \\
-d(x, \partial \Omega) & \text{if } x \in \Omega^c
\end{cases}
$$

SDFëŠ” ì–´ë–¤ boundaryê¹Œì§€ì˜ ê±°ë¦¬ë¥¼ í‘œí˜„í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ë§Œì•½ ì–´ë–¤ ì  $x$ê°€ boundary ì•ˆ ìª½ì— ìœ„ì¹˜í•˜ê²Œ ë˜ë©´ function ê°’ì€ ì–‘ìˆ˜ë¥¼ ê°–ê²Œ ë˜ë©°, ì´ ì ì´ boundaryì™€ ì ì  ê°€ê¹ê²Œ ì´ë™í•  ìˆ˜ë¡ function ê°’ì€ 0ì— ê°€ê¹Œì›Œ ì§€ë‹¤ê°€, boundaryì— ìœ„ì¹˜í•˜ëŠ” ê²½ìš°ì—ëŠ” 0ì´ ë©ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ $x$ê°€ boundary ë°”ê¹¥ ìª½ì— ìœ„ì¹˜í•˜ëŠ” ê²½ìš°ì—ëŠ” function ê°’ì´ ìŒìˆ˜ë¥¼ ê°–ìŠµë‹ˆë‹¤.

ìœ„ì—ì„œëŠ” SDF í•¨ìˆ˜ì˜ ì‹ì— ëŒ€í•´ì„œ boundary ì•ˆ ìª½ì¸ ê²½ìš°ì— ì–‘ìˆ˜ë¼ê³  í‘œê¸°í•˜ì˜€ì§€ë§Œ boundary ì•ˆ ìª½ì„ ìŒìˆ˜ë¡œ ë‘ì–´ ë°˜ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ë„ ì¡´ì¬í•©ë‹ˆë‹¤. ì•„ë˜ ì‚¬ì§„ì€ DeepSDF[^13]ë¼ëŠ” ë…¼ë¬¸ì—ì„œ ê°€ì ¸ì˜¨ SDFì˜ ì˜ˆì‹œì´ë©° í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” boundary ì•ˆ ìª½ì„ ìŒìˆ˜ë¡œ ë‘ì—ˆìŠµë‹ˆë‹¤.

![img](../img/21-11-14-2.png)

ê³¼ê±°ì˜ surface ì¶”ì •ì´ë‚˜ 3D reconstruction ê°™ì€ taskì—ì„œëŠ” ì£¼ë¡œ voxel, point, meshë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í–ˆë‹¤ë©´, ìµœê·¼ì—ëŠ” SDF ì‚¬ìš©í•˜ë ¤ëŠ” ì‹œë„ê°€ ëŠ˜ì–´ë‚˜ê³  ìˆëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. íŠ¹íˆ Implicit Neural Representation ì—°êµ¬ì™€ SDFë¥¼ ê²°í•©í•œ ì—°êµ¬ ê²°ê³¼ë“¤ì´ í¥ë¯¸ë¡œì›Œ ë³´ì˜€ìŠµë‹ˆë‹¤.

Implicit Neural Representationì€ ì´ë¯¸ì§€ë‚˜ 3D ë°ì´í„°ë¥¼ pixel, voxel ë‹¨ìœ„ì˜ matrix í˜•íƒœë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, (x, y) ê°’ì„ ë°›ì•˜ì„ ë•Œ (r, g, b) ê°’ì„ ì¶œë ¥í•˜ëŠ” ì–´ë–¤ í•¨ìˆ˜ í•˜ë‚˜ë¡œì¨ í‘œí˜„í•˜ë ¤ëŠ” ì—°êµ¬ì…ë‹ˆë‹¤(í•¨ìˆ˜ 1ê°œëŠ” ë°ì´í„° 1ê°œë¥¼ ì˜ë¯¸í•˜ê³ , ë”°ë¼ì„œ í•™ìŠµ ì…ë ¥ 1ê°œëŠ” í”½ì…€ ê°’ 1ê°œë¡œ ì£¼ì–´ì§€ê²Œ ë  ë“¯ í•©ë‹ˆë‹¤). ë°ì´í„°ë¥¼ ì—°ì†ì ì¸ í•¨ìˆ˜ì˜ í˜•íƒœë¡œ í‘œí˜„í•˜ê¸° ë•Œë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ super resolutionì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì¥ì ì´ ìˆëŠ”ë°, ìµœê·¼ì— ì´ ë°©ì‹ê³¼ SDFë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… outputì„ ë§¤ìš° ë§¤ë„ëŸ½ê²Œ ë§Œë“¤ì–´ë‚´ê³ ì í•˜ëŠ” ì—°êµ¬ê°€ ë§ì´ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤.

##### ğŸ¤– ML & DL

*2021.12.02*

ì§€ê¸ˆê¹Œì§€ëŠ” ì•„ë¬´ ìƒê° ì—†ì´ continuous distributionì—ì„œë„ single pointì— íŠ¹ì • í™•ë¥ ì´ ì¡´ì¬í•œë‹¤ê³  ìƒê°í–ˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ $\mathcal N (0, 1)$ì— ëŒ€í•´ì„œ point $x=1$ì´ ê´€ì¸¡ë  í™•ë¥ ì´ íŠ¹ì • ê°’ìœ¼ë¡œ ì¡´ì¬í•œë‹¤ê³  ì˜ëª» ìƒê°í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤.

[ì´ ê³³](https://www.itl.nist.gov/div898/handbook/eda/section3/eda361.htm)[^14]ì„ ì°¸ê³ í•˜ë‹ˆ continuous probability functionì€ continuous intervalì˜ ë¬´í•œ pointsì— ëŒ€í•´ ì •ì˜ë˜ê¸° ë•Œë¬¸ì— single pointì˜ í™•ë¥ ì€ ì–¸ì œë‚˜ 0ì´ë©°, ë”°ë¼ì„œ continuous probability functionì—ì„œ í™•ë¥ ì€ íŠ¹ì • intervalì— ëŒ€í•´ì„œ ì¸¡ì •í•˜ê³  single pointì— ëŒ€í•´ì„  ì¸¡ì •í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  í•©ë‹ˆë‹¤.

ì–´ì°Œë³´ë©´ ê°„ë‹¨í•œ ê²ƒì´ì—ˆì§€ë§Œ ìì„¸íˆ ìƒê°í•´ë³´ì§€ëŠ” ì•Šì•„ì„œ í—·ê°ˆë ¸ë˜ ë“¯ í•©ë‹ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ, ê·¸ëŸ¬ë©´ ì–´ë–»ê²Œ 0ì´ ëª¨ì—¬ 1ì´ ë˜ëŠ” ê²ƒ ì¸ì§€ê¹Œì§€ ê¶ê¸ˆí•´ì§€ë©´ì„œ ìˆ˜í•™ì„ ë‹¹ì¥ ê·¼ë³¸ë¶€í„° ë‹¤ì‹œ ê³µë¶€í•´ì•¼í•˜ë‚˜ ì‹¶ì—ˆì§€ë§Œ, ì‹œê°„ì€ í•œì •ë˜ì–´ ìˆê³  í•  ì¼ì€ ë§ìœ¼ë‹ˆ ê¸¸ê²Œ ë³´ê³  ì²œì²œíˆ ê³µë¶€í•˜ìëŠ” ê²°ë¡ ìœ¼ë¡œ ëŒì•„ì™”ìŠµë‹ˆë‹¤ ğŸ¥²

##### ğŸ§© ML library

*2021.12.08*

PyTorchì— íŠ¹ì • weightë§Œ freezeí•˜ëŠ” ê¸°ëŠ¥ì´ êµ¬í˜„ë˜ì–´ ìˆëŠ”ì§€ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤.

Layer ë‹¨ìœ„ë¡œ freezing í•˜ëŠ” ê²½ìš°ì—ëŠ” `required_grad=False`ë¥¼ ì‚¬ìš©í•´ì„œ êµ¬í˜„í–ˆì—ˆëŠ”ë°, layer ë‚´ íŠ¹ì • weightë§Œ ê³¨ë¼ì„œ freezeí•˜ëŠ” ê¸°ëŠ¥ì€ ë”°ë¡œ ë³¸ ì ì´ ì—†ëŠ” ê²ƒ ê°™ì•„ ì°¾ì•„ë³´ë‹¤ê°€ [í•´ë‹¹ ë§í¬](https://discuss.pytorch.org/t/how-do-i-freeze-the-specific-weights-in-a-layer/104722/2)ë¥¼ ì½ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì‘ì„±ì ë¶„ì´ ì„¤ëª…í•˜ê¸°ë¡œëŠ” ì•„ë˜ì™€ ê°™ì€ ë‘ ê°€ì§€ ì„ì‹œë°©í¸ì´ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.

- `.step()`ë¥¼ í˜¸ì¶œí•˜ê¸° ì „ì— freeze í•˜ê³ ìí•˜ëŠ” weightì— ëŒ€í•´ì„œ `grad=0` í• ë‹¹. ë‹¤ë§Œ momentum, weight decayë¥¼ ì‚¬ìš©í•˜ëŠ” optimizerì˜ ê²½ìš°ì—” `grad=0`ì´ë”ë¼ë„ `.step()` í˜¸ì¶œ ì‹œ weightì„ ë³€í˜•í•˜ê¸° ë•Œë¬¸ì— ì›í•˜ëŠ”ëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
- Freezeí•˜ê³  ì‹¶ì€ weightì„ ë¯¸ë¦¬ copy í•´ë‘ê³  `.step()` ì„ í˜¸ì¶œí•˜ì—¬ weightì„ ì—…ë°ì´íŠ¸í•œ ë’¤ì—, ë³µì‚¬í–ˆë˜ weightì„ ì—…ë°ì´íŠ¸ëœ weightì— ë®ì–´ì”Œìš°ê¸°

##### ğŸ¤– ML & DL

*2022.01.15*

[ë§í¬](https://omoindrot.github.io/triplet-loss)[^15]ë¥¼ ì°¸ê³ í•˜ì—¬ triplet loss ê´€ë ¨ ìš©ì–´ë¥¼ ìˆ™ì§€í•˜ì˜€ìŠµë‹ˆë‹¤. 

- Easy triplets: $d(a, p) + \text{margin} < d(a, n)$
- Hard triplets: $d(a,n) < d(a, p)$
- Semi-hard triplets: $d(a, p) < d(a, n) < d(a,p) + \text{margin}$

##### ğŸ§© ML library

*2022.02.28*

Random seedë¥¼ ê³ ì •í•  ë•Œ ê°€ì¥ ë¨¼ì € ê³ ë ¤í•˜ë©´ ì¢‹ì„ ê²ƒë“¤ì„ ê¸°ë¡í•˜ì˜€ìŠµë‹ˆë‹¤.

```python
random.seed(args.seed)
np.random.seed(args.seed)
torch.manual_seed(args.seed)
torch.cuda.manual_seed_all(args.seed)
```

##### ğŸ‘¨â€ğŸ’» CS 

*2022.03.30*

ì¹œêµ¬([@jiun0](https://github.com/jiun0), [@bwmelon97](https://github.com/bwmelon97))ë“¤ì„ í†µí•´ ì•Œê²Œ ëœ ë„¤ì´ë° ìŠ¤íƒ€ì¼ì— ëŒ€í•´ì„œ ê°„ë‹¨íˆ ê¸°ë¡í•©ë‹ˆë‹¤.

- ë„¤ì´ë° ìŠ¤íƒ€ì¼ ì¢…ë¥˜: `lowerCamelCase`, `UpperCamelCase (PascalCase)`, `snake_case`, `Train_Case`, `spinal_case`, `UPPER_SNAKE_CASE`, ...
- ìë°”ìŠ¤í¬ë¦½íŠ¸ëŠ” ì£¼ë¡œ: ë³€ìˆ˜, í•¨ìˆ˜, ë©”ì„œë“œëŠ” lowerCamelCase / í´ë˜ìŠ¤ëª…ì€ PascalCase / ìƒìˆ˜ëª…ì€ UPPER_SNAKE_CASE
- íŒŒì´ì¬(PEP8)ì€ ì£¼ë¡œ: ë³€ìˆ˜, í•¨ìˆ˜ëŠ” snake_case / í´ë˜ìŠ¤ëŠ” CamelCase

##### ğŸ¤– ML & DL

*2022.04.10*

ì—°êµ¬ë¥¼ í•˜ë©°, ëª¨ë¸ í•™ìŠµì˜ ì•ˆì •ì„±ì— ìˆì–´ì„œ residual connectionì´ ìœ ìš©í•˜ë‹¤ëŠ” ê²½í—˜ì ì¸ íŒì„ ì–»ì—ˆìŠµë‹ˆë‹¤. ResNetê³¼ ê°™ì´ ëª¨ë¸ êµ¬ì¡°ì—ì„œ residual connectionì„ í™œìš©í•˜ëŠ” ê²ƒ ë¿ë§Œ ì•„ë‹ˆë¼, ì–´ë–¤ ê°’ì„ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ë°”ê¾¸ê³  ì‹¶ì„ ë•Œ residual connectionì„ ê°€ì§„ êµ¬ì¡°ê°€ ë¹„êµì  ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ GNNì„ í†µí•´ embedding vectorë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ì‹¶ì„ ë•Œ $V_{t+1} = G(V_t)$ì˜ í˜•íƒœë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒ ë³´ë‹¤ $V_{t+1} = V_t + G(V_t)$ì˜ í˜•íƒœë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìœ¼ë©°, í˜„ì¬ ì‹¤í—˜ ì¤‘ì¸ ê²ƒ ì¤‘ì—ì„œëŠ” few-shotìœ¼ë¡œ distributionì˜ meanì„ ì˜ ì¶”ì •í•´ë³´ë ¤ëŠ” ë‚´ìš©ì´ ìˆëŠ”ë°, ì´ ê²½ìš°ì—ë„ $\hat \mu = f_\theta(\text{few-shot})$ ë³´ë‹¤ëŠ” $\hat \mu = \text{mean of few-shot} +  f_\theta(\text{few-shot})$ í˜•íƒœì—ì„œ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤.

ì•„ë¬´ë˜ë„ ì¼ë°˜ì ìœ¼ë¡œ parameterê°€ 0ì— ê°€ê¹Œìš´ ê°€ìš°ì‹œì•ˆìœ¼ë¡œ ì´ˆê¸°í™”ë˜ê¸° ë•Œë¬¸ì—, residual connectionì„ ì‚¬ìš©í•œ ê²½ìš°ì— ì´ˆê¸° lossê°€ ë” ì‘ì•„ì ¸ ë¹„êµì  í•™ìŠµì´ ì•ˆì •ì ì¸ ê²ƒì´ ì•„ë‹ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. (*ì •ë§ë¡œ ê·¸ëŸ° ê²ƒì¸ì§€ ì°¾ì•„ë³´ê³  ë‚´ìš© ì¶”ê°€í•˜ê¸°*)

##### ğŸ‘¨â€ğŸ’» CS 

*2022.05.11*

- ì¸í„°í”„ë¦¬í„°[^18] ì–¸ì–´: Pythonê³¼ ê°™ì´, í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ ì†ŒìŠ¤ ì½”ë“œë¥¼ ë°”ë¡œ ì‹¤í–‰. ë¹Œë“œ ì‹œê°„ì´ ì—†ì§€ë§Œ, runtimeì—ì„œëŠ” ì»´íŒŒì¼ ì–¸ì–´ì— ë¹„í•´ ì†ë„ê°€ ëŠë¦¼
- ì»´íŒŒì¼ ì–¸ì–´[^19]: C/C++ê³¼ ê°™ì´, íŠ¹ì • í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ ì“°ì—¬ ìˆëŠ” ë¬¸ì„œë¥¼ ë‹¤ë¥¸ í”„ë¡œê·¸ë˜ë° ì–¸ì–´(í˜¹ì€ ê¸°ê³„ì–´)ë¡œ ë²ˆì—­í•˜ì—¬ ì‹¤í–‰. ë¹Œë“œ ì‹œê°„ì´ ì†Œìš”ë˜ì§€ë§Œ, runtimeì—ì„œ ë¹ ë¥´ê²Œ ì‹¤í–‰ ê°€ëŠ¥. ì›ë˜ì˜ ë¬¸ì„œë¥¼ ì†ŒìŠ¤ ì½”ë“œ(í˜¹ì€ ì›ì‹œ ì½”ë“œ)ë¼ê³  ë¶€ë¥´ê³ , ì¶œë ¥ëœ ë¬¸ì„œë¥¼ ëª©ì  ì½”ë“œë¼ê³  ë¶€ë¦„. ëª©ì  ì½”ë“œëŠ” ì£¼ë¡œ í•˜ë“œì›¨ì–´ê°€ ì²˜ë¦¬í•˜ê¸°ì— ìš©ì´í•œ í˜•íƒœë¡œ ì¶œë ¥ë˜ì§€ë§Œ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ì„œ íŒŒì¼ì´ë‚˜ ê·¸ë¦¼ íŒŒì¼ ë“±ìœ¼ë¡œ ì˜®ê¸°ëŠ” ê²½ìš°ë„ ìˆìŒ
- í˜„ëŒ€ì— ë“¤ì–´ ë§ì€ ì¸í„°í”„ë¦¬í„°ê°€ JIT(just-in-time) ì»´íŒŒì¼ ë“±ì˜ ê¸°ìˆ ë¡œ ì‹¤ì‹œê°„ ì»´íŒŒì¼ì„ ìˆ˜í–‰í•˜ë¯€ë¡œ, ì»´íŒŒì¼ëŸ¬ì™€ ì¸í„°í”„ë¦¬í„° ì‚¬ì´ì˜ ê¸°ìˆ ì  êµ¬ë¶„ì€ ì‚¬ë¼ì ¸ ê°€ëŠ” ì¶”ì„¸. Javaê°€ JIT ì»´íŒŒì¼ì„ ì§€ì›í•˜ê¸° ë•Œë¬¸ì— ì»´íŒŒì¼ ì–¸ì–´ì¸ ë™ì‹œì— ì¸í„°í”„ë¦¬í„° ì–¸ì–´ë¼ê³  í•  ìˆ˜ ìˆìŒ.

##### ğŸ¤– ML & DL

*2022.05.16*

Mooreâ€“Penrose inverse(=Pseudo inverse)[^20]ì— ëŒ€í•´ì„œ ì •ë¦¬í•©ë‹ˆë‹¤.

- $A\mathrm  x =\mathrm b$ì˜ í˜•íƒœì˜ linear systemì„ í’€ ë•Œ, $A$ê°€ ì •ë°© í–‰ë ¬ì´ ì•„ë‹ˆë¼ë©´ ì•„ë˜ì˜ ë‘ ê°€ì§€ ìƒí™©ì´ ì¡´ì¬.

1. Underdetemined (n < m): ê°€ë¡œë¡œ ê¸´ A. Infinitely many solution given $\mathrm b$ in general
2. Overdetermined (n > m): ì„¸ë¡œë¡œ ê¸´ A. Zero solution for given $\mathrm b$ in general

- $A$ì— ëŒ€í•´ì„œ singular value decompositionì„ ìˆ˜í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ì „ê°œê°€ ê°€ëŠ¥í•¨

$$
A \mathrm x = b \\
U \Sigma V^\top \mathrm  x =\mathrm b \\
V \Sigma ^{-1} U^\top U \Sigma V^\top \mathrm  x =V \Sigma ^{-1} U^\top \mathrm b \\
\tilde {\mathrm x} = V \Sigma ^{-1} U^\top \mathrm b := A^+ \mathrm  b
$$

- ì—¬ê¸°ì„œ $A^+ = V \Sigma ^+ U^\top $ë¥¼ Aì˜ pseudo inverseë¼ í•¨
- $\Sigma = \text{diag}_{n,m}(\lambda_1, \cdots, \lambda_{\min\{ n, m \}})$ì¼ ë•Œ, $\Sigma^+ = \text{diag}_{m,n}(\lambda_1^+, \cdots, \lambda^+_{\min\{ n, m \}})$ where $\lambda^+= 
  \begin{cases}
      \lambda^{-1},& \lambda \neq 0 \\
      0,              & \lambda = 0
  \end{cases}$

Mooreâ€“Penrose inverseë¥¼ ì‚¬ìš©í•˜ë©´ ì„ í˜•ëŒ€ìˆ˜í•™ì˜ ë§ì€ ë¶€ë¶„ì„ ì‰½ê²Œ ì„œìˆ  ë° ì¦ëª… ê°€ëŠ¥í•¨

1. Underdetemined(í•´ê°€ ì—¬ëŸ¬ ê°œ ì¡´ì¬)ì—ì„œ $A^+ \mathrm b$ëŠ” ìœ í´ë¦¬ë“œ ë…¸ë¦„ $||\tilde {\mathrm x} ||_2$ì„ ìµœì†Œí™”í•˜ëŠ” í•´ì„
2. Overdeterminedì—ì„œ $||A \tilde {\mathrm  x} - \mathrm b||_2 = ||A A^+ \mathrm b - \mathrm b||_2$ëŠ” ìµœì†Œì œê³±ë²•ì˜ ìµœì í•´ì„

##### ğŸ¤– ML & DL

*2022.05.27*

Linear combinationì— ëŒ€í•´ì„œ ê³„ìˆ˜ê°€ ì–‘ìˆ˜ì´ê³  ê³„ìˆ˜ì˜ í•©ì´ 1ì¸ ê²½ìš°, ì´ë¥¼ convex combinationì´ë¼ê³  í•¨

Convex setì˜ ì •ì˜ì™€ ì—°ê´€ì§€ì–´ ë³´ë©´, ì–´ë–¤ ì§‘í•© Cì— ì†í•˜ëŠ” ì„ì˜ì˜ ì ë“¤ì˜ convex combinationì´ Cì— ì†í•˜ë©´ ê·¸ ì§‘í•©ì€ convex setì´ë¼ê³  ë§í•  ìˆ˜ ìˆìœ¼ë©°, ë§ˆì°¬ê°€ì§€ë¡œ convex set Cì— ì†í•˜ëŠ” ì ë“¤ì˜ convex combinationì€ í•­ìƒ Cì— ì†í•¨.

##### ğŸ¤– ML & DL

*2022.05.28*

ë‹¤ì–‘í•œ Data Augmentation ë°©ë²•ë“¤ì— ëŒ€í•´ì„œ [ì´ê³³](https://cse-study.github.io/ai/2022-05/220527-data-augmentation/)ì— ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.

##### ğŸ¤– ML & DL

*2022.06.29*

Upper bound, Lower bound, Supremum, Infimumì— ëŒ€í•œ ìˆ˜í•™ì  ì •ì˜ë¥¼ [ì´ê³³](https://web.math.ucsb.edu/~agboola/teaching/2021/winter/122A/rudin.pdf)ì„ ì°¸ê³ í•˜ì—¬ ì •ë¦¬í•©ë‹ˆë‹¤.

- Upper bound (ìƒê³„): ì–´ë–¤ ì‹¤ìˆ˜ $\beta$ê°€ ìˆì„ ë•Œ, $E$ì˜ ëª¨ë“  ì›ì†Œ $x$ì— ëŒ€í•´ì„œ $x < \beta$ë¥¼ ë§Œì¡±í•  ë•Œ, $\beta$ë¥¼ $E$ì˜ upper boundë¼ê³  í•¨. ì´ ë•Œ $E$ëŠ” ***bounded above***ë¼ê³  í•¨. (Lower boundë„ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì •ì˜ ë¨)
- Supremum, Least upper bound (ìƒí•œ): $\alpha = \sup E$ ì´ë ¤ë©´, $\alpha$ê°€ $E$ì˜ upper boundì´ë©°, $\gamma < \alpha$ì¸ ëª¨ë“  $\gamma$ê°€ $E$ì˜ upper boundê°€ ì•„ë‹ˆì–´ì•¼ í•¨. ì¦‰, **upper bound ì¤‘ leastê°€ supermum**ì„
- Infimum, Greatest lower bound (í•˜í•œ): $\alpha = \inf E$ ì´ë ¤ë©´, $\alpha$ê°€ $E$ì˜ lower boundì´ë©°,  $\beta > \alpha$ì¸ ëª¨ë“  $\beta$ê°€ $E$ì˜ lower boundê°€ ì•„ë‹ˆì–´ì•¼ í•¨. ì¦‰, **lower bound ì¤‘ greatestê°€ infimum**ì„

##### ğŸ‘¨â€ğŸ’» CS 

*2022.08.26*

'ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ ë””ìì¸ íŒ¨í„´ (ì‹œë¶€ì´ ìœ ìš°ìŠ¤ì¼€ ì§€ìŒ)' ì±…ì— ë“±ì¥í•˜ëŠ” ê¸°ìˆ  ìŠ¤íƒì— ëŒ€í•´ ê°„ë‹¨íˆ ì •ë¦¬í•©ë‹ˆë‹¤.

- ONNX[^21]: Intermediate Representation(IR)ì˜ í•œ ì¢…ë¥˜. ML modelì— ëŒ€í•´ì„œ static graphsì˜ í˜•íƒœì¸ CNTK, Caffe2, Theano, TensorFlowì™€ dynamic graphsì˜ í˜•íƒœì¸ PyTorch, Chainer ë“± ë‹¤ì–‘í•œ í”„ë ˆì„ì›Œí¬ë“¤ì´ ì¡´ì¬í•˜ê³ , ì´ë“¤ì€ ê°ê° ê·¸ë“¤ë§Œì˜ íŠ¹ì¥ì ì´ ì¡´ì¬í•¨. ë”°ë¼ì„œ ê°œë°œ ë‹¨ê³„ì— ë”°ë¼ ì í•©í•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ìš”êµ¬ë˜ëŠ”ë°, ì´ ë•Œ ONNXë¥¼ í™œìš©í•˜ë©´ ì„œë¡œ ë‹¤ë¥¸ í™˜ê²½ì—ì„œ ë§Œë“¤ì–´ì§„ ëª¨ë¸ì„ í•˜ë‚˜ì˜ ê³µí†µëœ í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
- Redis: Database ì¤‘ í•˜ë‚˜ì´ë©°, Cassandra DB, DynamoDBì™€ ê°™ì´ **key value DB engine** ì¤‘ í•˜ë‚˜ì„. in-memory ë°ì´í„° êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§¤ìš° ë¹ ë¥¸ ì†ë„ë¥¼ ê°€ì§€ë©°, ìºì‹±ì„ ìœ„í•´ ìì£¼ ì‚¬ìš©ë¨
- gRPC(Remote Procedure Calls): êµ¬ê¸€ì´ ìµœì´ˆë¡œ ê°œë°œí•œ ì˜¤í”ˆ ì†ŒìŠ¤ ì›ê²© í”„ë¡œì‹œì € í˜¸ì¶œ (RPC) ì‹œìŠ¤í…œ. ì—¬ê¸°ì„œ RPC[^22]ëŠ” ë³„ë„ì˜ ì›ê²© ì œì–´ë¥¼ ìœ„í•œ ì½”ë”© ì—†ì´ ë‹¤ë¥¸ ì£¼ì†Œ ê³µê°„ì—ì„œ í•¨ìˆ˜ë‚˜ í”„ë¡œì‹œì €ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œí•˜ëŠ” í”„ë¡œì„¸ìŠ¤ ê°„ í†µì‹  ê¸°ìˆ ì„ ë§í•¨. 

##### ğŸ¤– ML & DL

*2022.10.06*

10ì›” 6ì¼ì— ì§„í–‰ëœ AI workshop ë‚´ìš©ì„ ê¸°ë¡í•©ë‹ˆë‹¤. ë¨¼ì €, Federated Learningê³¼ ê´€ë ¨ëœ ë‚´ìš©ì…ë‹ˆë‹¤.

1. Federated Learning (FL)
   - Central serverì— clientì˜ dataë¥¼ ì—…ë¡œë“œí•  ìˆ˜ ì—†ëŠ” ìƒí™©ì— ì–´ë–»ê²Œ ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆì„ì§€?
   - Clientì—ì„œ ê°ì ì—…ë°ì´íŠ¸ëœ 'ëª¨ë¸'ì„ ì„œë²„ë¡œ ì˜¬ë¦¬ê³ , í‰ê· ì„ ì·¨í•´ì„œ ë‹¤ì‹œ clientì—ê²Œ ë¿Œë¦¬ëŠ” ë°©ì‹ì´ ì œì¼ ì¼ë°˜ì  (FedAvg)
   - í•˜ì§€ë§Œ ì´ëŸ° ë°©ì‹ì€ non-IID setting(heterogeneous)ì—ì„œ ë§¤ìš° í¬ê²Œ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ê²Œ ë¨: PFL ì—°êµ¬ì˜ ë°°ê²½
2. Personalized Federated Learning (PFL): Client specific weightsì´ ë„ì…ë¨
3. PFL via Meta-learning: PFLì˜ ì»¨ì…‰ê³¼ Meta-learning(MAML)ì˜ ì»¨ì…‰ì´ ë§¤ìš° ìœ ì‚¬í•˜ë‹¤ëŠ” ì ì—ì„œ ê³ ì•ˆë¨

Imitation learning ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤.

1. Reinforcement Learning (RL)
   - Purpose: Find an optimal policy $\pi*$ that miximize $V$
   - Require domain knowledge for real-world application
   - ë“œë¡ ì„ ì˜ˆë¡œ ë“¤ë©´, ì‹¤ì œ ë“œë¡ ì€ ë§¤ìš° ì‰½ê²Œ ë¶€ìˆ´ì§€ë¯€ë¡œ Sim2Real learningì„ ê³ ë ¤í•´ì•¼ í•˜ê³ , ë“œë¡  physicsì— ë§ì€ perturbationì´ ì¡´ì¬í•˜ë¯€ë¡œ Robust learningë„ ê³ ë ¤í•´ì•¼ í•¨
2. Imitation Learning (IL)
   - Behavior cloning (BC), Inverse RL (IRL), IRL + RL ë“±ì˜ ë°©ë²•ì´ ì¡´ì¬
   - BCëŠ” ë§ì€ ì–‘ì˜ ë°ì´í„°ê°€ í•„ìš”í•˜ê³  compounding errorì— ì·¨ì•½í•˜ë¯€ë¡œ, ì´ëŸ° ì ì—ì„œëŠ” IRLì´ ì¥ì ì„ ê°€ì§
3. Generative Adversariel Imitation Learning (GAIL)
   - Real dataë¡œëŠ” expert actionsë¥¼ ì œê³µí•˜ê³ , Fake dataë¡œëŠ” policy actionsë¥¼ ì œê³µí•˜ì—¬ expertì˜ policyë¥¼ í‰ë‚´ë‚´ë„ë¡ í•™ìŠ´
   - Limitation: Real envrionment dangerì™€ environment perturbationì— ëŒ€í•´ì„œëŠ” ì˜ ëª¨ë¸ë§í•˜ì§€ ì•ŠìŒ. ë”°ë¼ì„œ domain-adpative ILì´ í•„ìš”
4. Simulation-based Learning: Domain Adaptive IL
   - Simulation(source) env.ì—ì„œ informationì„ ë½‘ì•„, target envì˜ policyì— ë„ì›€ì„ ì£¼ë„ë¡, information extraction ê³¼ì •ì´ ì¤‘ìš”

##### ğŸ¤– ML & DL

*2022.10.06*

ë ˆë”§ì„ ì½ë‹¤ê°€ "í•™ìŠµì´ ë„ˆë¬´ ì˜¤ë˜ê±¸ë¦¬ëŠ” ê²½ìš°ì—” í•˜ì´í¼íŒŒë¦¬ë¯¸í„° íŠœë‹ì„ ì–´ë–»ê²Œ í•´ì•¼í•˜ëŠ”ê°€?"ì— ëŒ€í•œ ê¸€ì´ ìˆì–´, ê¸€ì— ë‹¬ë¦° ì½”ë©˜íŠ¸ì™€ ê°œì¸ì ì¸ ìƒê°ë“¤ì„ ê¸°ë¡í•©ë‹ˆë‹¤.

- ëª¨ë¸ ìŠ¤ì¼€ì¼ì„ ì¤„ì¸ ìƒíƒœë¡œ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì§„í–‰í•˜ê±°ë‚˜, ë°ì´í„°ì…‹ì„ ì¼ë¶€ë§Œ ì‚¬ìš©í•œ í•™ìŠµì„ í†µí•´ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì§„í–‰
- e.g., ResNet152ë¼ê³  í•œë‹¤ë©´ ResNet18 ê°™ì´ ì‘ì€ ëª¨ë¸ ì‚¬ìš©í•˜ê±°ë‚˜, ImageNetì´ë¼ê³  í•œë‹¤ë©´ 100ê°œ classë§Œ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ìˆ˜í–‰
- ì´ ë°©ë²•ì€ ë‹¹ì—°íˆ sub-optimalì´ê¸´ í•˜ê² ì§€ë§Œ í•™ìŠµì´ ë„ˆë¬´ ì˜¤ë˜ê±¸ë¦¬ëŠ” ê²½ìš°ì— ì¶©ë¶„íˆ í™œìš©í•´ ë³¼ë§Œ í•œ ë°©ë²•ì´ë¼ê³  ìƒê°í–ˆìŒ
- ì‚¬ì‹¤ ì œì¼ ì¢‹ì€ ê²ƒì€ GPU ìì›ì„ ë³‘ë ¬ë¡œ ì¶©ë¶„íˆ í™œìš©í•  ìˆ˜ ìˆê²Œ ì—”ì§€ë‹ˆì–´ë§ì„ ê±°ì¹œ í›„ì— í•™ìŠµí•˜ëŠ” ê²ƒ. ì™œëƒë©´ big modelê³¼ small model ì‚¬ì´ì— í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì— ë”°ë¥¸ ëª¨ë¸ì˜ ë™ì‘ì— ë¶„ëª…íˆ ì°¨ì´ê°€ ì¡´ì¬í•  ê²ƒì´ê¸° ë•Œë¬¸ì—, ì›ë˜ ìŠ¤ì¼€ì¼ëŒ€ë¡œ ì‹¤í—˜í•˜ëŠ”ê²Œ ì œì¼ ì¢‹ìŒ

##### ğŸ¤– ML & DL

*2022.10.14*

ML ë¶„ì•¼ì—ì„œì˜ "Grokking"ì´ë¼ëŠ” ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.

- Overparameterizedëœ ë‰´ëŸ´ë„· ëª¨ë¸ì´, small training datasetì— ëŒ€í•´ì„œ overfit ë˜ì–´ ìˆë‹¤ê°€, ë§¤ìš° ë§ì€ ì‹œê°„(optimization step)ì´ ì§€ë‚œ í›„ì— ì–´ëŠ ì§€ì ì— ê°‘ìê¸° ì¢‹ì€ generalization ì„±ëŠ¥(validation loss ê°ì†Œ)ì„ ë‹¬ì„±í•˜ëŠ” í˜„ìƒ
- OpenAIì˜ ["Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"](https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf) ë…¼ë¬¸ì—ì„œ ëª…ëª…

##### ğŸ¤– ML & DL

*2022.10.21*

- The stability-plasticity dilemma: ìƒˆë¡œìš´ ì§€ì‹ì„ ì–»ê¸° ìœ„í•´ ëª¨ë¸ì˜ ì˜êµ¬ ë³€í˜•ì´ ìš”êµ¬ë˜ë©´ì„œë„, ë™ì‹œì— ê¸°ì¡´ì˜ ì§€ì‹ì„ ìŠì–´ë²„ë¦¬ì§€ë„ ì•Šì•„ì•¼ í•œë‹¤ëŠ” ì 
- Learning in a parallel and distributed system requires plasticity for the integration of new knowledge but also stability in order to prevent the forgetting of previous knowledge.[^23]

### References

[^1]: Wikipedia contributors. (2021, April 12). Moment (mathematics). In Wikipedia, The Free Encyclopedia. Retrieved 12:08, May 24, 2021, from https://en.wikipedia.org/w/index.php?title=Moment_(mathematics)&oldid=1017468752
[^2]: API. (2021ë…„ 3ì›” 2ì¼). ìœ„í‚¤ë°±ê³¼, . 04:58, 2021ë…„ 5ì›” 24ì¼ì— í™•ì¸ https://ko.wikipedia.org/w/index.php?title=API&oldid=28891731 
[^3]: REST. (2021ë…„ 4ì›” 28ì¼). ìœ„í‚¤ë°±ê³¼, . 04:57, 2021ë…„ 5ì›” 24ì¼ì— í™•ì¸ https://ko.wikipedia.org/w/index.php?title=REST&oldid=29220143
[^4]: í†µí•© ìì› ì‹ë³„ì. (2021ë…„ 3ì›” 14ì¼). ìœ„í‚¤ë°±ê³¼, . 05:02, 2021ë…„ 5ì›” 24ì¼ì— í™•ì¸ https://ko.wikipedia.org/w/index.php?title=%ED%86%B5%ED%95%A9%EC%9E%90%EC%9B%90%EC%8B%9D%EB%B3%84%EC%9E%90&oldid=28963926
[^ 5]: mutable vs immutable. (2019ë…„ 5ì›” 24ì¼). ê³µí•™ìë¥¼ ìœ„í•œ Python, WikiDocs. 2021ë…„ 5ì›” 24ì¼ì— í™•ì¸ https://wikidocs.net/32277
[^ 6]: ì–•ì€ ë³µì‚¬(shallow copy)ì™€ ê¹Šì€ ë³µì‚¬(deep copy). (2018ë…„ 3ì›” 13ì¼). íŒŒì´ì¬ - ê¸°ë³¸ì„ ê°ˆê³  ë‹¦ì!, WikiDocs. 2021ë…„ 5ì›” 24ì¼ì— í™•ì¸ https://wikidocs.net/16038
[^7]: JinWon Lee - PR-317: MLP-Mixer: An all-MLP Architecture for Vision. https://www.youtube.com/watch?v=KQmZlxdnnuY
[^8]: JoonYoung Yi - Slideshare, Dynamically Expandable Network (DEN). https://www.slideshare.net/ssuser62b35f/180808-dynamically-expandable-network

[^9]: í”Œë¡­ìŠ¤. (2021ë…„ 2ì›” 3ì¼). *ìœ„í‚¤ë°±ê³¼,* . 13:21, 2021ë…„ 8ì›” 25ì¼ì— í™•ì¸ [https://ko.wikipedia.org/w/index.php?title=%ED%94%8C%EB%A1%AD%EC%8A%A4&oldid=28682165](https://ko.wikipedia.org/w/index.php?title=í”Œë¡­ìŠ¤&oldid=28682165)
[^10]: 6. ëª¨ë“ˆ. (2021ë…„ 9ì›” 30ì¼). Python 3.9.7 ë¬¸ì„œ, https://docs.python.org/ko/3/tutorial/modules.html
[^11]: ëª¨ë“ˆì„± (í”„ë¡œê·¸ë˜ë°). (2019ë…„ 4ì›” 16ì¼). ìœ„í‚¤ë°±ê³¼, . 15:08, 2021ë…„ 9ì›” 30ì¼ì— í™•ì¸ https://ko.wikipedia.org/w/index.php?title=%EB%AA%A8%EB%93%88%EC%84%B1_(%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D)&oldid=24041546
[^12]: Wikipedia contributors. (2021, August 1). Signed distance function. In *Wikipedia, The Free Encyclopedia*. Retrieved 00:41, November 14, 2021, from https://en.wikipedia.org/w/index.php?title=Signed_distance_function&oldid=1036639454
[^13]: Park, Jeong Joon, et al. "Deepsdf: Learning continuous signed distance functions for shape representation." *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2019.
[^14]: 1.3.6.1.What is a Probability Distribution., *NIST/SEMATECH e-Handbook of Statistical Methods*, http://www.itl.nist.gov/div898/handbook/, December 2, 2021.

[^ 15]: Olivier Moindrot. "Triplet Loss and Online Triplet Mining in TensorFlow". https://omoindrot.github.io/triplet-loss, Mar 19, 2018.

[^ 16]: Wikipedia contributors. (2021, December 22). Web query. In *Wikipedia, The Free Encyclopedia*. Retrieved 06:04, February 6, 2022, from https://en.wikipedia.org/w/index.php?title=Web_query&oldid=1061542579
[^ 17]: Christina Kopecky. "What is a database query? SQL and NoSQL queries explained". https://www.educative.io/blog/what-is-database-query-sql-nosql#what-is, Aug 31, 2020.
[^18]: ì¸í„°í”„ë¦¬í„°. (2022ë…„ 3ì›” 3ì¼). *ìœ„í‚¤ë°±ê³¼,* . 14:47, 2022ë…„ 5ì›” 10ì¼ì— í™•ì¸ [https://ko.wikipedia.org/w/index.php?title=%EC%9D%B8%ED%84%B0%ED%94%84%EB%A6%AC%ED%84%B0&oldid=32006110](https://ko.wikipedia.org/w/index.php?title=ì¸í„°í”„ë¦¬í„°&oldid=32006110) ì—ì„œ ì°¾ì•„ë³¼ ìˆ˜ ìˆìŒ.
[^19]: ì»´íŒŒì¼ëŸ¬. (2022ë…„ 3ì›” 15ì¼). *ìœ„í‚¤ë°±ê³¼,* . 15:23, 2022ë…„ 5ì›” 10ì¼ì— í™•ì¸ [https://ko.wikipedia.org/w/index.php?title=%EC%BB%B4%ED%8C%8C%EC%9D%BC%EB%9F%AC&oldid=32228964](https://ko.wikipedia.org/w/index.php?title=ì»´íŒŒì¼ëŸ¬&oldid=32228964) ì—ì„œ ì°¾ì•„ë³¼ ìˆ˜ ìˆìŒ.

[^20]: Wikipedia contributors. (2022, April 27). Mooreâ€“Penrose inverse. In *Wikipedia, The Free Encyclopedia*. Retrieved 06:08, May 16, 2022, from [https://en.wikipedia.org/w/index.php?title=Moore%E2%80%93Penrose_inverse&oldid=1085006448](https://en.wikipedia.org/w/index.php?title=Mooreâ€“Penrose_inverse&oldid=1085006448)
[^21]: https://github.com/onnx/onnx/blob/main/docs/Overview.md
[^22]: ì›ê²© í”„ë¡œì‹œì € í˜¸ì¶œ. (2022ë…„ 2ì›” 26ì¼). *ìœ„í‚¤ë°±ê³¼,* . 11:52, 2022ë…„ 8ì›” 26ì¼ì— í™•ì¸ [https://ko.wikipedia.org/w/index.php?title=%EC%9B%90%EA%B2%A9_%ED%94%84%EB%A1%9C%EC%8B%9C%EC%A0%80_%ED%98%B8%EC%B6%9C&oldid=31906127](https://ko.wikipedia.org/w/index.php?title=ì›ê²©_í”„ë¡œì‹œì €_í˜¸ì¶œ&oldid=31906127) ì—ì„œ ì°¾ì•„ë³¼ ìˆ˜ ìˆìŒ.
[^23]: Mermillod, Martial, AurÃ©lia Bugaiska, and Patrick Bonin. "The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects." *Frontiers in psychology* 4 (2013): 504.
