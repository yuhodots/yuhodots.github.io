---
title: "Variants of Vision Transformer"
date: "2023-05-27"
template: "post"
draft: false
path: "/deeplearning/23-05-27/"
description: "ViT ê´€ë ¨ ì•Œê³ ë¦¬ì¦˜ë“¤ì„ ëª¨ì•„ ì •ë¦¬í•©ë‹ˆë‹¤. ì•Œê³ ë¦¬ì¦˜ì€ ì‹œê°„ ìˆœìœ¼ë¡œ ë‚˜ì—´í•˜ì˜€ìŠµë‹ˆë‹¤. í¬ìŠ¤íŒ…ì˜ ì œëª©ì€ Variants of Vision Transformerì´ë‚˜, êµ¬ì¡°ì  ì°¨ì´ë¥¼ ê°–ëŠ” ë…¼ë¬¸ ì™¸ì—ë„ ë™ì¼í•œ êµ¬ì¡°ì—ì„œ í•™ìŠµ ë°©ë²•ë§Œ ì°¨ì´ë¥¼ ê°–ëŠ” ë…¼ë¬¸ë“¤ë„ ê¸°ë¡í•˜ì˜€ìŠµë‹ˆë‹¤."
category: "Deep Learning"
thumbnail: "deeplearning"
---

> ViT ê´€ë ¨ ì•Œê³ ë¦¬ì¦˜ë“¤ì„ ëª¨ì•„ ì •ë¦¬í•©ë‹ˆë‹¤. ì•Œê³ ë¦¬ì¦˜ì€ ì‹œê°„ ìˆœìœ¼ë¡œ ë‚˜ì—´í•˜ì˜€ìŠµë‹ˆë‹¤. í¬ìŠ¤íŒ…ì˜ ì œëª©ì€ Variants of Vision Transformerì´ë‚˜, êµ¬ì¡°ì  ì°¨ì´ë¥¼ ê°–ëŠ” ë…¼ë¬¸ ì™¸ì—ë„ ë™ì¼í•œ êµ¬ì¡°ì—ì„œ í•™ìŠµ ë°©ë²•ë§Œ ì°¨ì´ë¥¼ ê°–ëŠ” ë…¼ë¬¸ë“¤ë„ ê¸°ë¡í•˜ì˜€ìŠµë‹ˆë‹¤. 

### Vision Transformer

Alexey Dosovitskiy, et al. â€œAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.â€ (Oct 2020 / ICLR2021)

![img](../img/ViT.png)

<center><p><i>Taken from Alexey Dosovitskiy, et al.</i></p></center>

1. ì „ì²´ ì´ë¯¸ì§€ë¥¼ 16x16 sizeì˜ patchë¡œ ì ˆë‹¨. ì˜ˆë¥¼ ë“¤ì–´ 48 x 48 ì´ë¯¸ì§€ë¼ë©´ 9ê°œì˜ patchë¡œ ë‚˜ë‰¨: `ViT-Base/16` modelì—ì„œ 16ì´ ì˜ë¯¸í•˜ëŠ” ê²ƒì´ patch sizeì„
2. ê°ê°ì˜ patchë¥¼ í‰íƒ„í™”(16x16x3 = 768) í•œ ë’¤ì—, ì›Œë“œ ì„ë² ë”©ì„ ë§Œë“œëŠ” ê²ƒì²˜ëŸ¼ linear projectionì„ í†µí•´ embedding vectorì˜ í˜•íƒœë¡œ ë³€í™˜
3. í•´ë‹¹ embedding vectorì—, BERT ì²˜ëŸ¼ CLS í† í°ê³¼ positional embeddingì„ ì¶”ê°€í•¨. ì´ ë‘˜ì€ learnable parameter ì„
   - CLS í† í°ì€ patch embeddingê³¼ ë™ì¼í•œ ì‚¬ì´ì¦ˆì´ë©°, 9ê°œì˜ patch embeddingë“¤ê³¼ concat í•¨
   - position embeddingì€ ì´ 9 + 1 = 10ê°œë¡œ, patch embeddingì— í•©(+)í•´ì£¼ëŠ” í˜•íƒœë¡œ êµ¬ì„±
4. ê° embedding vectorë¥¼ í•˜ë‚˜ì˜ ì›Œë“œ í† í°ì²˜ëŸ¼ì—¬ê²¨, transformerì˜ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
5. BERT ì²˜ëŸ¼, CLS í† í°ì˜ ìµœì¢… ì¶œë ¥ì´ í•´ë‹¹ ì´ë¯¸ì§€ì˜ output classë¼ê³  ê°€ì •. ë”°ë¼ì„œ transformer ì¶œë ¥ ë‹¨ì—ì„œ CLSì˜ embeddingì´ ì–´ë–»ê²Œ ë³€í–ˆëŠ”ì§€ í™•ì¸í•˜ì—¬ inference ìˆ˜í–‰

### DeiT

Hugo Touvron, et al., "Training data-efficient image transformers & distillation through attention." (Dec 2020, ICML 2021)

![img](../img/Deit.png)

- ViTëŠ” CNNì— ë¹„í•´ inductive biasê°€ ì ì€ êµ¬ì¡°ë¥¼ ê°€ì§€ì§€ë§Œ ê·¸ì— ë”°ë¼ ë§¤ìš° ë§ì€ ì–‘ì˜ í•™ìŠµ ë°ì´í„°ë¥¼ ìš”êµ¬í•¨
- ë”°ë¼ì„œ DeiT ë…¼ë¬¸ì—ì„œëŠ” ViT êµ¬ì¡°ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³ , transformerì— ì í•©í•œ teacher-student ë°©ì‹ì¸ distillation tokenì„ ì¶”ê°€í•˜ì—¬ ë¹ ë¥¸ ìˆ˜ë ´ ê°€ëŠ¥í•˜ê²Œ í–ˆìŒ
- Distillation token: ViTì—ì„œ CLS tokenê³¼ ë”ë¶ˆì–´ distillation tokenì´ë¼ëŠ” ê²ƒì„ ì¶”ê°€í•˜ê³ , teacher modelì˜ outputì„ distillation tokenì˜ targetìœ¼ë¡œ ì‚¬ìš©
  - CNN ê¸°ë°˜ modelì„ teacherë¡œ ì‚¬ìš©í–ˆì„ ë•Œ ì„±ëŠ¥ ì¢‹ìŒ


### Swin Transformer

Ze Liu, et al. "Swin transformer: Hierarchical vision transformer using shifted windows." (March 2021, ICCV 2021)

![img](../img/SwinTransformer.png)

<center><p><i>Taken from Ze Liu, et al. </i></p></center>

- ê¸°ì¡´ ViTëŠ” ì´ë¯¸ì§€ë¥¼ ìœ„í•œ íŠ¹ì„±ì´ ViTì— ì œëŒ€ë¡œ ì ìš©ë˜ì§€ ì•ŠìŒ. ì¦‰, í•´ìƒë„(resolution)ê³¼ ë¬¼ì²´ì˜ í¬ê¸°(scale)ê°€ ì´ë¯¸ì§€ì— ë”°ë¼ ë‹¬ë¼ì§€ë¯€ë¡œ ì´ë¥¼ ê³ ë ¤í•œ ëª¨ë¸ë§ í•„ìš”í•¨
- ë”°ë¼ì„œ local window & patch mergingì´ë¼ëŠ” inductive biasë¥¼ ViTì— ì ìš©
  - ê¸°ì¡´ ViTëŠ” ëª¨ë“  layerê°€ 16x16x3ì˜ íŒ¨ì¹˜(ì˜ ì„ë² ë”©)ë¥¼ ë™ì¼í•˜ê²Œ ì…ë ¥ìœ¼ë¡œ ë°›ì•˜ë‹¤ë©´, Swin TransformerëŠ” ì²˜ìŒì—ëŠ” 4x4x3 íŒ¨ì¹˜ë¶€í„° ì‹œì‘í•´ì„œ, layerë¥¼ ê±°ì¹  ë•Œ ë§ˆë‹¤ windowì˜ ì‚¬ì´ì¦ˆê°€ ì ì  ì»¤ì§€ëŠ” í˜•íƒœ
  - Swin Transformer block: window-based multi-head self-attention(W-MSA)ì™€ shifted window multi-head self-attention(SW-MSA) ì¡´ì¬
  - Swin Transformer blockì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ efficient batch computation, relative position bias ë“±ì˜ ë°©ë²•ë“¤ ì‚¬ìš©í•˜ëŠ”ë° ìì„¸í•œ ë‚´ìš© ë…¼ë¬¸ ì°¸ê³ 
- Layer ê³„ì¸µì— ë”°ë¼ ë‹¤ë¥¸ í•´ìƒë„ì˜ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— detection, segmentation taskì— í™œìš©ë„ ë†’ìŒ. ê´€ë ¨í•˜ì—¬ muti-resolution modelì¸ FPNê³¼ ë¹„êµí•´ë³´ë©´ ì¢‹ì„ë“¯

### DINO

Mathilde Caron, et al. â€œEmerging properties in self-supervised vision transformers.â€ (Apr 2021, ICCV 2021)

![img](../img/DINO.png)

<center><p><i>Taken from Mathilde Caron, et al. </i></p></center>

- Self-supervised ViTì— ëŒ€í•´ íŠ¹ì§• ë°œê²¬
  1. Self-supervised ViT featureë“¤ì€ ê·¸ ìì²´ë¡œ scene layout and object boundaries ê°™ì€ ì •ë³´ë¥¼ ì§€ë‹ˆê³  ìˆìŒ
  2. Finetuning, linear classifier, data augmentation ì—†ì´ë„ ì¢‹ì€ kNN classifier ì„±ëŠ¥ ë³´ì„
- ì•„ë˜ì˜ ë°©ë²• í™œìš©í•´ì„œ self-supervised ViT ë§Œë“¦
  1. ViTë¥¼ [BYOL](https://yuhodots.github.io/deeplearning/21-04-04/)ì˜ mannerë¡œ í•™ìŠµ. ì¦‰, momentum update í™œìš©í•¨
  2. BYOLê³¼ ë‹¬ë¦¬ normalized embeddingì˜ L2 distanceë¥¼ ì‚¬ìš©í•˜ì§€ëŠ” ì•Šê³ , $ğ‘_2\logğ‘_1$ í˜•íƒœì˜ cross-entropy loss ì‚¬ìš©
  3. Momentum teacher outputì˜ centering, sharpening ë§Œìœ¼ë¡œ collapse ë°©ì–´

### Masked AutoEncoder (MAE)

Kaiming He, et al. "Masked autoencoders are scalable vision learners." (Nov 2021, CVPR 2022)

![img](../img/MAE.png)

<center><p><i>Taken from Kaiming He, et al. </i></p></center>

- Masked image modeling (MIM) task
- ê¸°ì¡´ì˜ ViT í•™ìŠµ ë°©ì‹ë³´ë‹¤ëŠ”, ë§ˆì¹˜ BERT ì²˜ëŸ¼ masked patchë¥¼ ë³µì›í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ SSL pre-training ìˆ˜í–‰ í›„ì— downstream task(e.g., classification)ì„ í‘¸ëŠ” ê²ƒì´ ë” ì¢‹ìŒ. ë¬¼ë¡  CNNìœ¼ë¡œë„ ê°€ëŠ¥í•˜ì§€ë§Œ ViTì¼ ë•Œ ë” ì¢‹ìŒ
  - ì•½ 75%ì •ë„ë¥¼ maskingí•˜ê³ , inputì— masked patchëŠ” ë„£ì–´ì£¼ì§€ ì•Šì•„ í•™ìŠµ ë¹ ë¦„
  - Masked patchì— ëŒ€í•´ì„œë§Œ reconstruction loss ë¶€ì—¬
- ìœ ì‚¬í•œ ì—°êµ¬ë¡œëŠ” BEiTê°€ ìˆìŒ. BEiTëŠ” dVAE ê¸°ë°˜ì˜ image tokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ masked tokenì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, BERTì™€ ê±°ì˜ ìœ ì‚¬í•¨

### Masked Siamese Networks (MSN)

Mahmoud Assran, et al. "Masked siamese networks for label-efficient learning." (Apr 2022, ECCV 2022)

![img](../img/MSN.png)

<center><p><i>Taken from Mahmoud Assran, et al. </i></p></center>

- Siamese Network: "[Siamese Neural Networks for One-shot Image Recognition](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)"
  - ë™ì¼í•œ ì´ë¯¸ì§€ì— ëŒ€í•œ ë‹¤ë¥¸ viewê°€, ìœ ì‚¬í•œ representationì„ ê°€ì§€ë„ë¡ í•™ìŠµ
  - ë‘ ì…ë ¥ ì´ë¯¸ì§€ì— ëŒ€í•´ ë™ì¼í•œ ëª¨ë¸ì„ ì‚¬ìš© (shared weights)
  - ë‹¤ë§Œ collapse ë°œìƒí•  ìˆ˜ ìˆì–´ ìµœê·¼ ì—°êµ¬ë“¤ tripletì´ë‚˜ contrastive loss í™œìš©
- MAE patch reconstructionì˜ ë¬¸ì œì 
  - MAEì˜ reconstruction lossëŠ” ë‹¨ìˆœ classification taskë¥¼ í‘¸ëŠ”ë°ì—ë„ ë„ˆë¬´ ë””í…Œì¼í•œ low-level image modeling ìš”êµ¬
  - ì´ëŸ¬í•œ íŠ¹ì§•ì´ low-shot fine-tuningì—ì„œ over-fittingì„ ìœ ë°œ
- Masked Siamese Networks
  - Masekd anchor viewì™€ unmasked target viewê°€ ìœ ì‚¬í•œ output í™•ë¥  ë¶„í¬ ê°€ì§€ë„ë¡ í•™ìŠµ
  - PrototypeëŠ” learnable parameterì¸ë°, cluster assignment(for í™•ë¥  ë¶„í¬ ì¶œë ¥)ë¥¼ ìœ„í•´ ì‚¬ìš©. Class ê°œìˆ˜ ëª¨ë¥¸ë‹¤ê³  ê°€ì •í•˜ê¸° ë•Œë¬¸ì— prototype ê°œìˆ˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°
