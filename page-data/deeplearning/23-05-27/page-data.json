{"componentChunkName":"component---src-templates-post-js","path":"/deeplearning/23-05-27/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p>ViT ê´€ë ¨ ì•Œê³ ë¦¬ì¦˜ë“¤ì„ ëª¨ì•„ ì •ë¦¬í•©ë‹ˆë‹¤. ì•Œê³ ë¦¬ì¦˜ì€ ì‹œê°„ ìˆœìœ¼ë¡œ ë‚˜ì—´í•˜ì˜€ìŠµë‹ˆë‹¤. í¬ìŠ¤íŒ…ì˜ ì œëª©ì€ Variants of Vision Transformerì´ë‚˜, êµ¬ì¡°ì  ì°¨ì´ë¥¼ ê°–ëŠ” ë…¼ë¬¸ ì™¸ì—ë„ ë™ì¼í•œ êµ¬ì¡°ì—ì„œ í•™ìŠµ ë°©ë²•ë§Œ ì°¨ì´ë¥¼ ê°–ëŠ” ë…¼ë¬¸ë“¤ë„ ê¸°ë¡í•˜ì˜€ìŠµë‹ˆë‹¤. </p>\n</blockquote>\n<h3 id=\"vision-transformer\" style=\"position:relative;\"><a href=\"#vision-transformer\" aria-label=\"vision transformer permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Vision Transformer</h3>\n<p>Alexey Dosovitskiy, et al. â€œAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.â€ (Oct 2020 / ICLR2021)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f64510be344af58225dec98a359991f1/b0bb3/ViT.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 40%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABLklEQVQoz4VRTU+DQBDt/z960nj3pImapj0oStIP00KhlA9NZcuyFBLAtAmNCqblCUtrUtLWl0zezs682ZmdBrbI85xbCV8TsPBekX0lSJMY1W2+l1PX7LhxKEifLhHoIqjUAu1fo45DhXfYK4gysTh/BhZ+khBRFMH3XM5BEPxZGFaxko92WH+Jj16IXeaBui4mug7TNDkTQsA8xnm9Xh8fuUSapvDnHpbLBXw2hyYrMDUD1sTg/GZaMMYT6KoGMn3fav8pqBZCmzjIsgwxY/DGI8zkAWxZhqMosCUJZKQgtu1qWac7/MbVbQd3osV9MmO4aQ3QFDS8dHt4Fvt4ELp4bN+j1xGx2WxOj7xarXBxfoZmu8X9KP7AUHegTkM4DsWMUmiGjqHc539bX8ovjZJcLjssPsoAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/f64510be344af58225dec98a359991f1/15813/ViT.webp 190w,\n/static/f64510be344af58225dec98a359991f1/1cdb2/ViT.webp 380w,\n/static/f64510be344af58225dec98a359991f1/9046c/ViT.webp 760w,\n/static/f64510be344af58225dec98a359991f1/c89f9/ViT.webp 1140w,\n/static/f64510be344af58225dec98a359991f1/7afe4/ViT.webp 1520w,\n/static/f64510be344af58225dec98a359991f1/64607/ViT.webp 2344w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/f64510be344af58225dec98a359991f1/a2d4f/ViT.png 190w,\n/static/f64510be344af58225dec98a359991f1/3f520/ViT.png 380w,\n/static/f64510be344af58225dec98a359991f1/3c051/ViT.png 760w,\n/static/f64510be344af58225dec98a359991f1/b5cea/ViT.png 1140w,\n/static/f64510be344af58225dec98a359991f1/891d5/ViT.png 1520w,\n/static/f64510be344af58225dec98a359991f1/b0bb3/ViT.png 2344w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/f64510be344af58225dec98a359991f1/3c051/ViT.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<center><p><i>Taken from Alexey Dosovitskiy, et al.</i></p></center>\n<ol>\n<li>ì „ì²´ ì´ë¯¸ì§€ë¥¼ 16x16 sizeì˜ patchë¡œ ì ˆë‹¨. ì˜ˆë¥¼ ë“¤ì–´ 48 x 48 ì´ë¯¸ì§€ë¼ë©´ 9ê°œì˜ patchë¡œ ë‚˜ë‰¨: <code class=\"language-text\">ViT-Base/16</code> modelì—ì„œ 16ì´ ì˜ë¯¸í•˜ëŠ” ê²ƒì´ patch sizeì„</li>\n<li>ê°ê°ì˜ patchë¥¼ í‰íƒ„í™”(16x16x3 = 768) í•œ ë’¤ì—, ì›Œë“œ ì„ë² ë”©ì„ ë§Œë“œëŠ” ê²ƒì²˜ëŸ¼ linear projectionì„ í†µí•´ embedding vectorì˜ í˜•íƒœë¡œ ë³€í™˜</li>\n<li>\n<p>í•´ë‹¹ embedding vectorì—, BERT ì²˜ëŸ¼ CLS í† í°ê³¼ positional embeddingì„ ì¶”ê°€í•¨. ì´ ë‘˜ì€ learnable parameter ì„</p>\n<ul>\n<li>CLS í† í°ì€ patch embeddingê³¼ ë™ì¼í•œ ì‚¬ì´ì¦ˆì´ë©°, 9ê°œì˜ patch embeddingë“¤ê³¼ concat í•¨</li>\n<li>position embeddingì€ ì´ 9 + 1 = 10ê°œë¡œ, patch embeddingì— í•©(+)í•´ì£¼ëŠ” í˜•íƒœë¡œ êµ¬ì„±</li>\n</ul>\n</li>\n<li>ê° embedding vectorë¥¼ í•˜ë‚˜ì˜ ì›Œë“œ í† í°ì²˜ëŸ¼ì—¬ê²¨, transformerì˜ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬</li>\n<li>BERT ì²˜ëŸ¼, CLS í† í°ì˜ ìµœì¢… ì¶œë ¥ì´ í•´ë‹¹ ì´ë¯¸ì§€ì˜ output classë¼ê³  ê°€ì •. ë”°ë¼ì„œ transformer ì¶œë ¥ ë‹¨ì—ì„œ CLSì˜ embeddingì´ ì–´ë–»ê²Œ ë³€í–ˆëŠ”ì§€ í™•ì¸í•˜ì—¬ inference ìˆ˜í–‰</li>\n</ol>\n<h3 id=\"deit\" style=\"position:relative;\"><a href=\"#deit\" aria-label=\"deit permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DeiT</h3>\n<p>Hugo Touvron, et al., \"Training data-efficient image transformers &#x26; distillation through attention.\" (Dec 2020, ICML 2021)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/bc73995310b6fe3781e674cd2acaa8e4/1bba3/Deit.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 48.94736842105264%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA00lEQVQoz52SUa/CIAyF/f//0BejwkYoZXA3r4ls7FhQE703PgySpodT8qVQdviz1nWteV4WEIcPL+cMYxmL1N7997X7BrzdEk49/6sdtRNg3g6c5xlhGDCOI2KIiDFiEh3CUGubgeV614vFdTKYoq5RdPFeZzYCFzhWIDqC3OkRop1TrcAM7zswnyUrCf3UXXuHBWDtAVrtofUeJLp4zR3+XH7BIUAbA90beBlM8ZqAZZLWkrwZo+v6GuSc7F3blMvnJaLaacnes4ASlFJIKX0F3gGy1RACMYMXCQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/bc73995310b6fe3781e674cd2acaa8e4/15813/Deit.webp 190w,\n/static/bc73995310b6fe3781e674cd2acaa8e4/1cdb2/Deit.webp 380w,\n/static/bc73995310b6fe3781e674cd2acaa8e4/9046c/Deit.webp 760w,\n/static/bc73995310b6fe3781e674cd2acaa8e4/c89f9/Deit.webp 1140w,\n/static/bc73995310b6fe3781e674cd2acaa8e4/b68e6/Deit.webp 1433w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/bc73995310b6fe3781e674cd2acaa8e4/a2d4f/Deit.png 190w,\n/static/bc73995310b6fe3781e674cd2acaa8e4/3f520/Deit.png 380w,\n/static/bc73995310b6fe3781e674cd2acaa8e4/3c051/Deit.png 760w,\n/static/bc73995310b6fe3781e674cd2acaa8e4/b5cea/Deit.png 1140w,\n/static/bc73995310b6fe3781e674cd2acaa8e4/1bba3/Deit.png 1433w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/bc73995310b6fe3781e674cd2acaa8e4/3c051/Deit.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<ul>\n<li>ViTëŠ” CNNì— ë¹„í•´ inductive biasê°€ ì ì€ êµ¬ì¡°ë¥¼ ê°€ì§€ì§€ë§Œ ê·¸ì— ë”°ë¼ ë§¤ìš° ë§ì€ ì–‘ì˜ í•™ìŠµ ë°ì´í„°ë¥¼ ìš”êµ¬í•¨</li>\n<li>ë”°ë¼ì„œ DeiT ë…¼ë¬¸ì—ì„œëŠ” ViT êµ¬ì¡°ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³ , transformerì— ì í•©í•œ teacher-student ë°©ì‹ì¸ distillation tokenì„ ì¶”ê°€í•˜ì—¬ ë¹ ë¥¸ ìˆ˜ë ´ ê°€ëŠ¥í•˜ê²Œ í–ˆìŒ</li>\n<li>\n<p>Distillation token: ViTì—ì„œ CLS tokenê³¼ ë”ë¶ˆì–´ distillation tokenì´ë¼ëŠ” ê²ƒì„ ì¶”ê°€í•˜ê³ , teacher modelì˜ outputì„ distillation tokenì˜ targetìœ¼ë¡œ ì‚¬ìš©</p>\n<ul>\n<li>CNN ê¸°ë°˜ modelì„ teacherë¡œ ì‚¬ìš©í–ˆì„ ë•Œ ì„±ëŠ¥ ì¢‹ìŒ</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"swin-transformer\" style=\"position:relative;\"><a href=\"#swin-transformer\" aria-label=\"swin transformer permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Swin Transformer</h3>\n<p>Ze Liu, et al. \"Swin transformer: Hierarchical vision transformer using shifted windows.\" (March 2021, ICCV 2021)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/77599310dfe08c588bac0ce6e3bcb560/11f80/SwinTransformer.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 48.42105263157895%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB0ElEQVQoz2WT246bMBCG9/1fpxetqu5Nq666oa1EE3KABEjAIeEQTuEYzL8zzu6m0VpYHo093/zjMQ+gMcoRZXlGkRfwfR993yMnu65q2hzpG1GVBc55DjkMqMsSeZpioHO8ryZzaH1g4yhsGLoGsXXQNC2K0xFzfQLPWaPvelwuPUz9L7amhXAfYP1nAnexgLDWOIUhyiy7BwY7C7PJD/g2AfoL0sjHcjqhBBsFlEOP2a9HPD9+JtA/rLXvePr6CUvtiSrJUVXVDSiHCxa/f0L79gWb2VQBs1iQ4md47lXhOA7wbALNNYSBDbGZwjImOLhLdT3Xgl+BbVNj7znwSc0piZXz0rfI0gRd29wOS6As6J6LUimqzpWysyxVMXcl/z+klG93TPa1IeyTcngNkojiDMcoVY3b+aFS+QGYUtfiOKasBZIkIYXpu4/videQGpBTA4yNjaXjEjSCZszRtu0NWNc1XNeFYRgwTROHw0HZ/Hxs28ZqtVIg9gkhSF2MHQH3Ww9JFGMzX90Du65TEA50HEcFWZalfJyIfUEQqGS8x+otOuuSPyaFM10HMz6UzFlY7dvkYC6zaRrVBF558uMXewERCASU1Nm59FOU78AXcI/zlamco9gAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/77599310dfe08c588bac0ce6e3bcb560/15813/SwinTransformer.webp 190w,\n/static/77599310dfe08c588bac0ce6e3bcb560/1cdb2/SwinTransformer.webp 380w,\n/static/77599310dfe08c588bac0ce6e3bcb560/9046c/SwinTransformer.webp 760w,\n/static/77599310dfe08c588bac0ce6e3bcb560/c89f9/SwinTransformer.webp 1140w,\n/static/77599310dfe08c588bac0ce6e3bcb560/7afe4/SwinTransformer.webp 1520w,\n/static/77599310dfe08c588bac0ce6e3bcb560/37d62/SwinTransformer.webp 2498w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/77599310dfe08c588bac0ce6e3bcb560/a2d4f/SwinTransformer.png 190w,\n/static/77599310dfe08c588bac0ce6e3bcb560/3f520/SwinTransformer.png 380w,\n/static/77599310dfe08c588bac0ce6e3bcb560/3c051/SwinTransformer.png 760w,\n/static/77599310dfe08c588bac0ce6e3bcb560/b5cea/SwinTransformer.png 1140w,\n/static/77599310dfe08c588bac0ce6e3bcb560/891d5/SwinTransformer.png 1520w,\n/static/77599310dfe08c588bac0ce6e3bcb560/11f80/SwinTransformer.png 2498w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/77599310dfe08c588bac0ce6e3bcb560/3c051/SwinTransformer.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<center><p><i>Taken from Ze Liu, et al. </i></p></center>\n<ul>\n<li>ê¸°ì¡´ ViTëŠ” ì´ë¯¸ì§€ë¥¼ ìœ„í•œ íŠ¹ì„±ì´ ViTì— ì œëŒ€ë¡œ ì ìš©ë˜ì§€ ì•ŠìŒ. ì¦‰, í•´ìƒë„(resolution)ê³¼ ë¬¼ì²´ì˜ í¬ê¸°(scale)ê°€ ì´ë¯¸ì§€ì— ë”°ë¼ ë‹¬ë¼ì§€ë¯€ë¡œ ì´ë¥¼ ê³ ë ¤í•œ ëª¨ë¸ë§ í•„ìš”í•¨</li>\n<li>\n<p>ë”°ë¼ì„œ local window &#x26; patch mergingì´ë¼ëŠ” inductive biasë¥¼ ViTì— ì ìš©</p>\n<ul>\n<li>ê¸°ì¡´ ViTëŠ” ëª¨ë“  layerê°€ 16x16x3ì˜ íŒ¨ì¹˜(ì˜ ì„ë² ë”©)ë¥¼ ë™ì¼í•˜ê²Œ ì…ë ¥ìœ¼ë¡œ ë°›ì•˜ë‹¤ë©´, Swin TransformerëŠ” ì²˜ìŒì—ëŠ” 4x4x3 íŒ¨ì¹˜ë¶€í„° ì‹œì‘í•´ì„œ, layerë¥¼ ê±°ì¹  ë•Œ ë§ˆë‹¤ windowì˜ ì‚¬ì´ì¦ˆê°€ ì ì  ì»¤ì§€ëŠ” í˜•íƒœ</li>\n<li>Swin Transformer block: window-based multi-head self-attention(W-MSA)ì™€ shifted window multi-head self-attention(SW-MSA) ì¡´ì¬</li>\n<li>Swin Transformer blockì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ efficient batch computation, relative position bias ë“±ì˜ ë°©ë²•ë“¤ ì‚¬ìš©í•˜ëŠ”ë° ìì„¸í•œ ë‚´ìš© ë…¼ë¬¸ ì°¸ê³ </li>\n</ul>\n</li>\n<li>Layer ê³„ì¸µì— ë”°ë¼ ë‹¤ë¥¸ í•´ìƒë„ì˜ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— detection, segmentation taskì— í™œìš©ë„ ë†’ìŒ. ê´€ë ¨í•˜ì—¬ muti-resolution modelì¸ FPNê³¼ ë¹„êµí•´ë³´ë©´ ì¢‹ì„ë“¯</li>\n</ul>\n<h3 id=\"dino\" style=\"position:relative;\"><a href=\"#dino\" aria-label=\"dino permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DINO</h3>\n<p>Mathilde Caron, et al. â€œEmerging properties in self-supervised vision transformers.â€ (Apr 2021, ICCV 2021)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/5496c/DINO.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 46.8421052631579%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABAUlEQVQoz5WSW2vCQBCF/f+/R2NRsM/SNwuBQIIpmuZ+J/fL6c7gljRNqx5YdpgdPs7M7AoTjePIh9Q0DeI45tjzPDiOw3Gapvwm6+dazROyqG1bhGHIcZZliKKIYwInSfLLwCJQPtZ1Ddu2YZ7P31DpTtd1mKaJsiwXXS4C6SZXBJXtkcilpmkwDOM5YF1V8MXcPNdFLCDkuBK56+UCVVXxfjohujl/COiKOb0oCrabDV4PB5RFgTzP8XY8Yr/bQVmv8SHaJg3DcB/Y9z1v2LIsFAImRc4/RS4IAnRiaXcdzqGB73Or1LI89IXaP2D/LgW3uOs6/iahcDVd0I/aib4Afi68b7A6ycsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/15813/DINO.webp 190w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/1cdb2/DINO.webp 380w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/9046c/DINO.webp 760w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/c89f9/DINO.webp 1140w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/7afe4/DINO.webp 1520w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/04951/DINO.webp 2452w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/a2d4f/DINO.png 190w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/3f520/DINO.png 380w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/3c051/DINO.png 760w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/b5cea/DINO.png 1140w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/891d5/DINO.png 1520w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/5496c/DINO.png 2452w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/3c051/DINO.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<center><p><i>Taken from Mathilde Caron, et al. </i></p></center>\n<ul>\n<li>\n<p>Self-supervised ViTì— ëŒ€í•´ íŠ¹ì§• ë°œê²¬</p>\n<ol>\n<li>Self-supervised ViT featureë“¤ì€ ê·¸ ìì²´ë¡œ scene layout and object boundaries ê°™ì€ ì •ë³´ë¥¼ ì§€ë‹ˆê³  ìˆìŒ</li>\n<li>Finetuning, linear classifier, data augmentation ì—†ì´ë„ ì¢‹ì€ kNN classifier ì„±ëŠ¥ ë³´ì„</li>\n</ol>\n</li>\n<li>\n<p>ì•„ë˜ì˜ ë°©ë²• í™œìš©í•´ì„œ self-supervised ViT ë§Œë“¦</p>\n<ol>\n<li>ViTë¥¼ <a href=\"https://yuhodots.github.io/deeplearning/21-04-04/\">BYOL</a>ì˜ mannerë¡œ í•™ìŠµ. ì¦‰, momentum update í™œìš©í•¨</li>\n<li>BYOLê³¼ ë‹¬ë¦¬ normalized embeddingì˜ L2 distanceë¥¼ ì‚¬ìš©í•˜ì§€ëŠ” ì•Šê³ , <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>ğ‘</mi><mn>2</mn></msub><mi>log</mi><mo>â¡</mo><msub><mi>ğ‘</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">ğ‘_2\\logğ‘_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">â€‹</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">â€‹</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> í˜•íƒœì˜ cross-entropy loss ì‚¬ìš©</li>\n<li>Momentum teacher outputì˜ centering, sharpening ë§Œìœ¼ë¡œ collapse ë°©ì–´</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"masked-autoencoder-mae\" style=\"position:relative;\"><a href=\"#masked-autoencoder-mae\" aria-label=\"masked autoencoder mae permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Masked AutoEncoder (MAE)</h3>\n<p>Kaiming He, et al. \"Masked autoencoders are scalable vision learners.\" (Nov 2021, CVPR 2022)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/66580287771f37112f8c93bd3d42ee45/16529/MAE.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 37.368421052631575%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABWklEQVQoz2VRTUsCURT1L7XtL7Sthf2Mdm3aNRtzGUWFSRAVGQyIpKAIpWGaypik4pBGJJOOT2d0/Gg+Tu/NqDl24c47b969h3Pv8ViWBZYs2Gkaho0HwyEGmmZj9qfdJVDHE1g9gul7HazD1HVXL0sPZjF/mEf9+QHFCI+2OkSlp0CSZQymP0CfEoo1V+1yr8dWZZr2pdkQwQePUMim0chlkA3d4KVSRUkm+Ja7kEgPY6kF5a2Mi+AZAj4OYtUhZxy2QvYxZmMm+FvsejdwfehHOszjLnCKx1wepY6ML5rx1BPyiThahRx825s4WF+DEAk7hJRjQThXKL4W4d/fQ4yOKkTDSJ6foFCrU0KqTlHxSVUqH01olTKOOQ473i1kkgk34eoedN1Rm4reI3Z1iRYdU6BEfW2EiUlrRhr0tuTULhni2uGfU+YCd6gJhBAbT+gEqy7PFvfP5V8AKQWH12SEhwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/66580287771f37112f8c93bd3d42ee45/15813/MAE.webp 190w,\n/static/66580287771f37112f8c93bd3d42ee45/1cdb2/MAE.webp 380w,\n/static/66580287771f37112f8c93bd3d42ee45/9046c/MAE.webp 760w,\n/static/66580287771f37112f8c93bd3d42ee45/c89f9/MAE.webp 1140w,\n/static/66580287771f37112f8c93bd3d42ee45/7afe4/MAE.webp 1520w,\n/static/66580287771f37112f8c93bd3d42ee45/7da0b/MAE.webp 2506w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/66580287771f37112f8c93bd3d42ee45/a2d4f/MAE.png 190w,\n/static/66580287771f37112f8c93bd3d42ee45/3f520/MAE.png 380w,\n/static/66580287771f37112f8c93bd3d42ee45/3c051/MAE.png 760w,\n/static/66580287771f37112f8c93bd3d42ee45/b5cea/MAE.png 1140w,\n/static/66580287771f37112f8c93bd3d42ee45/891d5/MAE.png 1520w,\n/static/66580287771f37112f8c93bd3d42ee45/16529/MAE.png 2506w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/66580287771f37112f8c93bd3d42ee45/3c051/MAE.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<center><p><i>Taken from Kaiming He, et al. </i></p></center>\n<ul>\n<li>Masked image modeling (MIM) task</li>\n<li>\n<p>ê¸°ì¡´ì˜ ViT í•™ìŠµ ë°©ì‹ë³´ë‹¤ëŠ”, ë§ˆì¹˜ BERT ì²˜ëŸ¼ masked patchë¥¼ ë³µì›í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ SSL pre-training ìˆ˜í–‰ í›„ì— downstream task(e.g., classification)ì„ í‘¸ëŠ” ê²ƒì´ ë” ì¢‹ìŒ. ë¬¼ë¡  CNNìœ¼ë¡œë„ ê°€ëŠ¥í•˜ì§€ë§Œ ViTì¼ ë•Œ ë” ì¢‹ìŒ</p>\n<ul>\n<li>ì•½ 75%ì •ë„ë¥¼ maskingí•˜ê³ , inputì— masked patchëŠ” ë„£ì–´ì£¼ì§€ ì•Šì•„ í•™ìŠµ ë¹ ë¦„</li>\n<li>Masked patchì— ëŒ€í•´ì„œë§Œ reconstruction loss ë¶€ì—¬</li>\n</ul>\n</li>\n<li>ìœ ì‚¬í•œ ì—°êµ¬ë¡œëŠ” BEiTê°€ ìˆìŒ. BEiTëŠ” dVAE ê¸°ë°˜ì˜ image tokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ masked tokenì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, BERTì™€ ê±°ì˜ ìœ ì‚¬í•¨</li>\n</ul>\n<h3 id=\"masked-siamese-networks-msn\" style=\"position:relative;\"><a href=\"#masked-siamese-networks-msn\" aria-label=\"masked siamese networks msn permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Masked Siamese Networks (MSN)</h3>\n<p>Mahmoud Assran, et al. \"Masked siamese networks for label-efficient learning.\" (Apr 2022, ECCV 2022)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/6d18191c92138eac275f2ef70f363711/1cb21/MSN.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 39.473684210526315%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABt0lEQVQoz22SSWtTURTH8/X8BFpw7dIibvQLdOHCqUo3ihunYhaCihasjU0xphVLUypY0iZmaN/ryxvy5vnneTemuvAPB+7h3Ps7060hKstSWaU4zUnSUqwgjBKiOGWuoig4m1g4rqf8IIzwg1DuRRKbva8pmFwULFmW0u1+o73zln7/B6fakNH4mKkX4Pohnh9hTz3le0EkwBjTcTEsh3lhNf5RWRbcvLHI40dLbDRe09xssNdpMdZNDntjxpqJbthoYiNtwtGvE5afrPJ9/+efDgSYJiHbrTovX91i/+ALL1brbDU/op+dsrvbwfN84iRTVVUjsBxPVVf59tTn8HigzhVMVahpIy5dvMDy/eu8/7DC/5TnubQbqLNhTQXqCjQ4j0dxLElns67ZjsXitSvcvnOVVvsN7iDA6OrE04jJ0YTICtXFUBZQqarOk3lWy6iUZhmJwDJJqoDzLFkek4UpO8++svZ8jUF7wOenn9jb7BBEMb2hLtv1cWUEofim7UjLLutb2wxP9L9LmX2Z+VagUd/g8sIC/U6Pe0t3WXnwUIVM21VtVe1nWa4eV9B3600M0z4H/gY8iVFW+UCzNQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/6d18191c92138eac275f2ef70f363711/15813/MSN.webp 190w,\n/static/6d18191c92138eac275f2ef70f363711/1cdb2/MSN.webp 380w,\n/static/6d18191c92138eac275f2ef70f363711/9046c/MSN.webp 760w,\n/static/6d18191c92138eac275f2ef70f363711/c89f9/MSN.webp 1140w,\n/static/6d18191c92138eac275f2ef70f363711/7afe4/MSN.webp 1520w,\n/static/6d18191c92138eac275f2ef70f363711/36d21/MSN.webp 2586w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/6d18191c92138eac275f2ef70f363711/a2d4f/MSN.png 190w,\n/static/6d18191c92138eac275f2ef70f363711/3f520/MSN.png 380w,\n/static/6d18191c92138eac275f2ef70f363711/3c051/MSN.png 760w,\n/static/6d18191c92138eac275f2ef70f363711/b5cea/MSN.png 1140w,\n/static/6d18191c92138eac275f2ef70f363711/891d5/MSN.png 1520w,\n/static/6d18191c92138eac275f2ef70f363711/1cb21/MSN.png 2586w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/6d18191c92138eac275f2ef70f363711/3c051/MSN.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<center><p><i>Taken from Mahmoud Assran, et al. </i></p></center>\n<ul>\n<li>\n<p>Siamese Network: \"<a href=\"https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf\">Siamese Neural Networks for One-shot Image Recognition</a>\"</p>\n<ul>\n<li>ë™ì¼í•œ ì´ë¯¸ì§€ì— ëŒ€í•œ ë‹¤ë¥¸ viewê°€, ìœ ì‚¬í•œ representationì„ ê°€ì§€ë„ë¡ í•™ìŠµ</li>\n<li>ë‘ ì…ë ¥ ì´ë¯¸ì§€ì— ëŒ€í•´ ë™ì¼í•œ ëª¨ë¸ì„ ì‚¬ìš© (shared weights)</li>\n<li>ë‹¤ë§Œ collapse ë°œìƒí•  ìˆ˜ ìˆì–´ ìµœê·¼ ì—°êµ¬ë“¤ tripletì´ë‚˜ contrastive loss í™œìš©</li>\n</ul>\n</li>\n<li>\n<p>MAE patch reconstructionì˜ ë¬¸ì œì </p>\n<ul>\n<li>MAEì˜ reconstruction lossëŠ” ë‹¨ìˆœ classification taskë¥¼ í‘¸ëŠ”ë°ì—ë„ ë„ˆë¬´ ë””í…Œì¼í•œ low-level image modeling ìš”êµ¬</li>\n<li>ì´ëŸ¬í•œ íŠ¹ì§•ì´ low-shot fine-tuningì—ì„œ over-fittingì„ ìœ ë°œ</li>\n</ul>\n</li>\n<li>\n<p>Masked Siamese Networks</p>\n<ul>\n<li>Masekd anchor viewì™€ unmasked target viewê°€ ìœ ì‚¬í•œ output í™•ë¥  ë¶„í¬ ê°€ì§€ë„ë¡ í•™ìŠµ</li>\n<li>PrototypeëŠ” learnable parameterì¸ë°, cluster assignment(for í™•ë¥  ë¶„í¬ ì¶œë ¥)ë¥¼ ìœ„í•´ ì‚¬ìš©. Class ê°œìˆ˜ ëª¨ë¥¸ë‹¤ê³  ê°€ì •í•˜ê¸° ë•Œë¬¸ì— prototype ê°œìˆ˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°</li>\n</ul>\n</li>\n</ul>","tableOfContents":"<ul>\n<li><a href=\"/MachineLearning/23-05-27-Variants%20of%20Vision%20Transformer/#vision-transformer\">Vision Transformer</a></li>\n<li><a href=\"/MachineLearning/23-05-27-Variants%20of%20Vision%20Transformer/#deit\">DeiT</a></li>\n<li><a href=\"/MachineLearning/23-05-27-Variants%20of%20Vision%20Transformer/#swin-transformer\">Swin Transformer</a></li>\n<li><a href=\"/MachineLearning/23-05-27-Variants%20of%20Vision%20Transformer/#dino\">DINO</a></li>\n<li><a href=\"/MachineLearning/23-05-27-Variants%20of%20Vision%20Transformer/#masked-autoencoder-mae\">Masked AutoEncoder (MAE)</a></li>\n<li><a href=\"/MachineLearning/23-05-27-Variants%20of%20Vision%20Transformer/#masked-siamese-networks-msn\">Masked Siamese Networks (MSN)</a></li>\n</ul>","frontmatter":{"path":"/deeplearning/23-05-27/","title":"Variants of Vision Transformer","category":"Deep Learning","date":"2023-05-27"}}},"pageContext":{}},"staticQueryHashes":["2390655019","256249292","63159454"]}