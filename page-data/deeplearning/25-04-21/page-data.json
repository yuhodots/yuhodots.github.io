{"componentChunkName":"component---src-templates-post-js","path":"/deeplearning/25-04-21/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p>Retrieval-Augmented Generation 방법론들에 대해 정리합니다. <strong><u>Gemin 2.5 Pro Deep Research를 사용한 결과를 필요한 부분만 정리</u></strong>한 내용입니다. 한 번 쭉 훑으며 내용을 수정하긴 하였지만, Deep Research의 결과물임을 감안하고 선별적으로 읽어주시기 바랍니다.</p>\n</blockquote>\n<h3 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h3>\n<h5 id=\"limitation-of-llm-models\" style=\"position:relative;\"><a href=\"#limitation-of-llm-models\" aria-label=\"limitation of llm models permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Limitation of LLM Models</h5>\n<p>최근 몇 년간 대규모 언어 모델(Large Language Models, LLM)은 자연어 이해(NLU) 및 텍스트 생성 분야에서 괄목할 만한 발전을 이루며 인공지능 기술의 새로운 지평을 열었다. LLM은 방대한 텍스트 데이터 학습을 통해 인간과 유사한 수준의 언어 구사 능력을 보여주며, 질의응답, 요약, 번역, 창작 등 다양한 분야에서 활용 가능성을 입증했다.</p>\n<p>하지만 이러한 강력한 성능에도 불구하고 LLM은 본질적인 한계를 지니고 있다. 가장 큰 문제점 중 하나는 '환각(hallucination)' 현상으로, 모델이 학습 데이터에 근거하지 않거나 사실과 다른 내용을 그럴듯하게 생성하는 경향이다. 이는 LLM이 내부 파라미터에 저장된 정적 지식에 의존하기 때문에 발생하는 문제로, 학습 데이터의 시점 이후 최신 정보를 반영하지 못하거나(Knowledge cutoff 문제) 특정 도메인 지식이 부족할 때 더욱 두드러진다. 또한, LLM의 응답 생성 과정은 투명성이 부족하여 결과의 신뢰성을 검증하기 어렵다는 문제도 제기된다.</p>\n<p>이러한 LLM의 한계를 극복하기 위한 방안으로 검색 증강 생성(Retrieval-Augmented Generation, RAG) 프레임워크가 등장했다. RAG는 LLM이 응답을 생성하기 전에 외부 지식 소스(예: <em>문서 데이터베이스, 웹 페이지</em> 등)를 참조하도록 함으로써, LLM의 응답을 보다 정확하고 신뢰할 수 있는 최신 정보에 기반하도록 만든다. 이는 LLM의 응답을 검증 가능한 정보에 '근거(grounding)'시키는 효과를 가져와 환각 현상을 줄이고 사실적 정확성을 높인다. 또한, 모델 전체를 재학습할 필요 없이 특정 도메인 지식이나 조직 내부 데이터를 활용할 수 있게 하여 비용 효율적인 솔루션을 제공한다. </p>\n<h5 id=\"limitation-of-naive-rag\" style=\"position:relative;\"><a href=\"#limitation-of-naive-rag\" aria-label=\"limitation of naive rag permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Limitation of Naive RAG</h5>\n<p>기본적인 RAG(종종 \"Naive RAG\"로 지칭됨)는 LLM의 성능을 크게 향상시켰지만, 여전히 몇 가지 중요한 한계점을 안고 있다. 가장 큰 문제는 검색(retrieval) 단계의 품질에 크게 의존한다는 점이다. 검색된 정보가 부정확하거나 관련성이 낮으면, 오히려 LLM의 응답 품질을 저하시킬 수 있다.</p>\n<p>Naive RAG는 검색된 문서를 <strong>무비판적으로 활용</strong>하는 경향이 있으며, 검색 결과의 <strong>정밀도(precision)나 재현율(recall)이 낮아</strong> 관련 없는 정보가 포함되거나 중요한 정보가 누락될 수 있다. 또한, 검색된 문서 내에서도 응답 생성에 불필요하거나 노이즈가 되는 정보가 많아 이를 효과적으로 필터링하지 못하는 문제도 있다. LLM이 제공된 컨텍스트를 무시하거나, 컨텍스트가 있음에도 불구하고 환각을 일으키는 경우도 발생한다.</p>\n<p>이러한 Naive RAG의 취약점을 해결하고 보다 견고하고 신뢰성 높은 응답 생성을 위해, 검색 결과를 평가하고 교정하거나 생성 과정 자체를 성찰하고 제어하는 고급 RAG 방법론들이 제안되었다. 중점적으로 다룰 <code class=\"language-text\">Corrective-RAG (CRAG)</code>, <code class=\"language-text\">Self-RAG</code>, <code class=\"language-text\">Self-Corrective RAG (SCRAG)</code>는 이러한 진화의 대표적인 예시로, 각각 평가, 교정, 자기 성찰의 메커니즘을 도입하여 RAG 시스템의 신뢰성과 제어 가능성을 높이는 것을 목표로 한다.</p>\n<h3 id=\"retrieval-augmented-generation\" style=\"position:relative;\"><a href=\"#retrieval-augmented-generation\" aria-label=\"retrieval augmented generation permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Retrieval-Augmented Generation</h3>\n<p>가장 기본적인 형태의 RAG, 즉 Naive RAG 워크플로우는 일반적으로 다음 세 단계로 구성된다: 인덱싱(Indexing), 검색(Retrieval), 생성(Generation)</p>\n<h5 id=\"indexing\" style=\"position:relative;\"><a href=\"#indexing\" aria-label=\"indexing permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Indexing</h5>\n<p>이 단계에서는 LLM이 참조할 외부 데이터를 처리하여 검색 가능한 형태로 만든다. 외부 데이터는 API, 데이터베이스, 문서 저장소 등 다양한 소스에서 올 수 있으며, 파일, DB 레코드, 장문 텍스트 등 여러 형식을 가질 수 있다.</p>\n<ol>\n<li><strong>데이터 청크 분할 (Chunking):</strong> 원본 문서를 LLM의 컨텍스트 창 크기나 임베딩 모델의 처리 능력에 맞춰 더 작은 단위(청크)로 분할한다. 청크 크기가 너무 크면 관련 없는 정보가 포함될 수 있고, 너무 작으면 의미 있는 정보를 충분히 담지 못할 수 있으므로 적절한 분할 전략(예: 고정 크기, 문장/단락 단위 분할)이 중요하다.</li>\n<li><strong>임베딩 생성 (Embedding Generation):</strong> 분할된 각 청크를 임베딩 언어 모델(embedding language model)을 사용하여 고차원 벡터(vector)로 변환한다. 이 벡터는 청크의 의미론적 내용을 수치적으로 표현한다.</li>\n<li><strong>벡터 저장소 저장 (Storing in Vector Database):</strong> 생성된 벡터 임베딩을 해당 청크의 원본 텍스트와 함께 벡터 데이터베이스(vector database) 또는 벡터 저장소(vector store)에 저장하고 인덱싱한다. 이를 통해 유사도 기반의 빠른 검색이 가능해진다.</li>\n</ol>\n<h5 id=\"retrieval\" style=\"position:relative;\"><a href=\"#retrieval\" aria-label=\"retrieval permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Retrieval</h5>\n<p>사용자의 질의(query)가 입력되면, 이를 처리하여 관련성이 높은 정보(문서 청크)를 인덱싱된 지식 베이스에서 검색한다.</p>\n<ol>\n<li><strong>질의 임베딩 (Query Embedding):</strong> 사용자 질의를 인덱싱 단계에서 사용한 것과 동일한 임베딩 모델을 사용하여 벡터로 변환한다.</li>\n<li><strong>유사도 검색 (Similarity Search):</strong> 질의 벡터와 벡터 저장소 내의 문서 청크 벡터들 간의 유사도(예: 코사인 유사도)를 계산한다.</li>\n<li><strong>Top-K 청크 선택 (Top-K Chunk Selection):</strong> 계산된 유사도를 바탕으로 질의와 가장 관련성이 높은 상위 K개의 문서 청크를 검색 결과로 반환한다.</li>\n</ol>\n<h5 id=\"generation\" style=\"position:relative;\"><a href=\"#generation\" aria-label=\"generation permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Generation</h5>\n<p>검색된 관련 정보를 LLM에게 제공하여 최종 응답을 생성하는 단계이다.</p>\n<ol>\n<li><strong>프롬프트 증강 (Prompt Augmentation):</strong> 검색된 K개의 관련 문서 청크(컨텍스트)를 사용자의 원본 질의와 결합하여 LLM에게 전달할 증강된 프롬프트(augmented prompt)를 구성한다. 효과적인 정보 전달을 위해 프롬프트 엔지니어링 기법이 사용될 수 있다.</li>\n<li><strong>응답 생성 (Response Generation):</strong> LLM은 증강된 프롬프트를 입력받아, 내부의 파라미터 지식과 제공된 외부 컨텍스트 정보를 종합적으로 활용하여 사용자 질의에 대한 최종 응답을 생성한다.</li>\n</ol>\n<p>RAG의 핵심은 추론 시점에 외부 데이터를 동적으로 검색하여 LLM에 제공한다는 점이다. 이는 모델 파라미터를 정적으로 업데이트하는 미세조정이나 재학습과는 달리, LLM 자체를 변경하지 않고도 최신 정보나 특정 도메인 지식을 활용할 수 있게 해준다. 또한, 전체 RAG 파이프라인의 효과는 <strong>초기 인덱싱 및 검색 단계의 품질(청크 분할 전략, 임베딩 모델 성능, 유사도 검색 정확성 등)에 크게 좌우</strong>된다. 이 단계에서의 미흡함은 후속 생성 단계의 품질 저하로 이어질 수 있으며, 이는 고급 RAG 기법들이 개선하고자 하는 주요 지점이다.</p>\n<h3 id=\"corrective-rag-crag\" style=\"position:relative;\"><a href=\"#corrective-rag-crag\" aria-label=\"corrective rag crag permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Corrective-RAG (CRAG)</h3>\n<p>표준적인 RAG 시스템은 검색된 문서의 품질에 크게 의존한다. 만약 검색 단계에서 관련성이 낮거나 부정확한 문서가 반환될 경우, 이는 최종 생성 결과의 품질 저하로 직결될 수 있다.Corrective-RAG (CRAG)는 이러한 문제점을 해결하기 위해 제안된 프레임워크로, <strong>검색 결과의 신뢰성을 평가하고 필요한 경우 교정 조치를 수행</strong>함으로써 RAG 시스템의 생성 견고성(robustness)을 향상시키는 것을 목표로 한다.</p>\n<p>CRAG의 핵심 아이디어는 부정확한 검색 결과가 LLM의 생성 과정에 미치는 악영향을 최소화하기 위해, 생성 단계 이전에 검색된 정보를 능동적으로 평가하고 '<strong>자가 교정(self-correct)</strong>'하는 메커니즘을 도입하는 것이다.</p>\n<h5 id=\"lightweight-retrieval-evaluator\" style=\"position:relative;\"><a href=\"#lightweight-retrieval-evaluator\" aria-label=\"lightweight retrieval evaluator permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Lightweight Retrieval Evaluator</h5>\n<ul>\n<li>목적: 주어진 질의에 대해 초기에 검색된 문서 집합의 전반적인 품질, 즉 관련성 및 신뢰성을 평가한다.</li>\n<li>구현: CRAG는 이 평가를 위해 별도의 경량 모델을 사용한다. 예를 들어, 사전 훈련된 T5-large 모델(0.77B 파라미터)을 기존 데이터셋(예: PopQA)을 사용하여 미세조정하여, 질의-문서 쌍의 관련성을 판단하도록 학습시킨다. 이 평가기는 주요 생성 LLM과는 분리되어 작동한다.</li>\n<li>이점: 상대적으로 작은 모델을 사용함으로써 평가 과정의 계산 효율성을 높이고, 전체 시스템에 과도한 오버헤드를 추가하지 않는다. 이는 CRAG의 핵심 혁신 중 하나로, 평가 및 교정 로직을 외부의 전용 경량 평가기로 분리하여, 주 생성 LLM에 의존하는 방식과 차별화된다. 이 외부화는 평가 구성 요소의 전문적인 최적화를 가능하게 한다.</li>\n</ul>\n<h5 id=\"confidence-scoring-and-action-trigger\" style=\"position:relative;\"><a href=\"#confidence-scoring-and-action-trigger\" aria-label=\"confidence scoring and action trigger permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Confidence Scoring and Action Trigger</h5>\n<ul>\n<li>Retrieval Evaluator는 각 검색된 문서에 대해 관련성 점수를 예측한다. 이 점수들을 종합하여 검색 결과 전체의 신뢰도(confidence degree)를 정량화한다.</li>\n<li>\n<p>미리 정의된 상한 및 하한 임계값(threshold)을 기준으로 신뢰도를 평가하여 다음 세 가지 조치 중 하나를 트리거한다:</p>\n<ul>\n<li><strong>Correct</strong>: 검색된 문서 중 하나 이상이 상한 임계값을 초과하는 높은 관련성 점수를 가질 경우. 이 경우, 검색 결과가 신뢰할 만하다고 판단하고 <strong>지식 정제(Knowledge Refinement) 단계로 진행</strong>한다.</li>\n<li><strong>Incorrect</strong>: 모든 검색된 문서의 관련성 점수가 하한 임계값 미만일 경우. 이는 검색 결과가 전반적으로 부적절하다고 판단하며, <strong>웹 검색(Web Search)을 트리거</strong>한다.</li>\n<li><strong>Ambiguous</strong>: 점수들이 임계값 사이에 분포하거나 평가기가 불확실성을 나타낼 경우. 이 경우, 초기 검색 결과와 웹 검색 모두에서 정보를 활용하기 위해 <strong>지식 정제와 웹 검색을 모두 수행</strong>한다. 이 'Ambiguous' 상태는 불확실성에 대한 실용적인 접근 방식을 나타내며, 초기 검색이 불확실할 경우 정적 소스에서 최상의 정보를 수집하는 동시에 동적 웹에서 더 좋거나 보충적인 정보를 찾는 전략을 의미한다.</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"corrective-actions\" style=\"position:relative;\"><a href=\"#corrective-actions\" aria-label=\"corrective actions permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Corrective Actions</h5>\n<ul>\n<li>\n<p><strong>Knowledge Refinement - Decompose-then-Recompose</strong>: 'Correct' 또는 'Ambiguous' 조치가 트리거될 때 적용된다.</p>\n<ol>\n<li><strong>분해 (Decompose)</strong>: 검색된 문서를 더 작은 의미 단위(\"knowledge strips\")로 분할한다. 짧은 문서는 그대로 스트립으로 간주하고, 긴 문서는 몇 개의 문장으로 구성된 세그먼트로 나눈다.</li>\n<li><strong>필터링 (Filter)</strong>: 미세조정된 검색 평가기를 다시 사용하여 각 지식 스트립의 관련성 점수를 계산하고, 미리 정의된 임계값 이하의 점수를 가진 관련 없는 스트립을 제거한다.</li>\n<li><strong>재구성 (Recompose)</strong>: 필터링 후 남은 관련성 높은 지식 스트립들을 원래 순서대로 연결하여 정제된 \"내부 지식(internal knowledge)\"을 구성한다.</li>\n</ol>\n</li>\n<li>\n<p><strong>Web Search Augmentation</strong>: 'Incorrect' 또는 'Ambiguous' 조치가 트리거될 때 적용된다.</p>\n<ol>\n<li><strong>질의 재작성 (Query Rewriting):</strong> 원본 질의를 웹 검색에 적합한 키워드 집합으로 변환한다 (예: ChatGPT 사용).</li>\n<li><strong>웹 검색 API 활용 (Web Search API):</strong> 재작성된 질의를 사용하여 공개 웹 검색 API(예: Google Search API, Tavily)를 호출하고 관련 URL 목록을 얻는다. 편향 및 신뢰성 문제를 완화하기 위해 Wikipedia와 같은 권위 있는 사이트를 선호한다.</li>\n<li><strong>콘텐츠 추출 및 정제 (Content Extraction and Refinement):</strong> 검색된 웹 페이지에서 콘텐츠를 추출하고, 위에서 설명한 지식 정제 방법(decompose-then-recompose)을 동일하게 적용하여 관련성 높은 \"외부 지식(external knowledge)\"을 추출한다.</li>\n</ol>\n</li>\n</ul>\n<p>교정 조치를 통해 얻어진 최종 지식(정제된 내부 지식, 외부 지식, 또는 둘의 조합)을 원본 질의와 함께 생성 LLM에 입력하여 최종 응답을 생성한다.</p>\n<h3 id=\"self-rag\" style=\"position:relative;\"><a href=\"#self-rag\" aria-label=\"self rag permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Self-RAG</h3>\n<p>표준 RAG가 검색된 정보를 무차별적으로 활용하는 한계를 극복하고, LLM의 생성 품질과 사실적 정확성을 향상시키면서도 모델 본연의 다재다능함(versatility)을 해치지 않는 것을 목표로 한다.</p>\n<p>Self-RAG의 핵심 아이디어는 <strong>LLM 자체가 생성 과정에서 능동적으로 검색 필요성을 판단</strong>하고(적응형 검색), <strong>검색된 정보와 자신의 생성 결과를 비판적으로 평가(자기 성찰)하도록 학습</strong>시키는 것이다. 이를 통해 추론 단계에서 생성 과정을 보다 세밀하게 제어하고 다양한 작업 요구사항에 맞춰 모델의 행동을 조정할 수 있게 한다.</p>\n<p>Self-RAG는 '<strong>리플렉션 토큰(reflection tokens)'</strong>이라는 특수 토큰을 사용하여 LLM의 검색 및 생성 과정을 제어하고 평가한다. 이 토큰들은 확장된 어휘(vocabulary)를 가진 주 생성 LLM에 의해 일반 텍스트와 함께 생성된다.</p>\n<h5 id=\"reflection-tokens\" style=\"position:relative;\"><a href=\"#reflection-tokens\" aria-label=\"reflection tokens permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reflection Tokens</h5>\n<ul>\n<li><code class=\"language-text\">Retrieve</code>: 현재 생성 맥락에서 <strong>외부 정보 검색이 필요한지 여부</strong>를 나타낸다 (값: Yes, No, Continue). 이는 적응형 검색(adaptive retrieval)을 가능하게 한다.</li>\n<li><code class=\"language-text\">IsRel</code>: <strong>검색된 각 구절(passage)이 입력 질의 및 이전 생성 내용과 관련이 있는지 평가</strong>한다 (값: Relevant, Irrelevant)</li>\n<li><code class=\"language-text\">IsSup</code>: 생성된 출력 세그먼트가 <strong>해당 구절에 의해 사실적으로 뒷받침되는지 평가</strong>한다 (값: Fully Supported, Partially Supported, No Support/Contradictory)</li>\n<li><code class=\"language-text\">IsUse</code>: 생성된 세그먼트의 <strong>전반적인 품질과 유용성을 평가</strong>한다 (예: 1-5점 척도)</li>\n</ul>\n<h5 id=\"adaptive-retrieval\" style=\"position:relative;\"><a href=\"#adaptive-retrieval\" aria-label=\"adaptive retrieval permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Adaptive Retrieval</h5>\n<ul>\n<li>LLM은 외부 지식이 필요할 수 있는 생성 지점에서 <code class=\"language-text\">Retrieve</code> 토큰을 예측한다. <code class=\"language-text\">Retrieve</code> 토큰이 'Yes'로 예측되거나 해당 토큰의 예측 확률이 특정 임계값을 넘을 경우에만 검색이 트리거된다.</li>\n<li>이를 통해 모델의 파라미터 지식만으로 충분하거나 사실적 근거가 중요하지 않은 작업(예: 창의적 글쓰기)에서는 불필요한 검색을 생략하여 효율성을 높인다. 이는 Naive RAG의 '무차별적 검색' 문제를 직접적으로 해결한다.</li>\n</ul>\n<h5 id=\"generation-and-self-critique\" style=\"position:relative;\"><a href=\"#generation-and-self-critique\" aria-label=\"generation and self critique permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Generation and Self-Critique</h5>\n<ul>\n<li>검색이 발생하면, Self-RAG는 여러 검색된 구절을 병렬적으로 처리한다. 각 구절에 대해, 해당 구절을 컨텍스트로 사용하여 후보 출력 세그먼트를 생성하고, 동시에 관련된 비판 토큰(<code class=\"language-text\">IsUse</code>)도 함께 생성한다.</li>\n<li><strong>세그먼트 수준 빔 검색 (Segment-level Beam Search):</strong> 생성된 여러 후보 세그먼트 중에서 최적의 경로를 선택하기 위해 빔 검색을 사용한다. 각 세그먼트의 점수는 단순히 생성 확률뿐만 아니라, 비판 토큰들의 점수(바람직한 비판 토큰 값의 정규화된 확률의 가중 합)를 함께 고려하여 계산된다. 추론 시점에 이 가중치를 조절하거나 비판 토큰 값에 기반한 하드 제약 조건을 적용하여, 재학습 없이도 모델의 행동(예: 사실성 우선 vs. 유창성 우선)을 제어할 수 있다.</li>\n</ul>\n<p>Self-RAG의 핵심은 평가 및 제어 로직을 주 생성 LLM 자체 내부에 통합하고, 리플렉션 토큰을 내장된 제어 메커니즘으로 사용하는 것이다. 이는 CRAG의 외부 평가기와 근본적으로 다르며, 생성과 평가를 동일한 모델이 수행하므로 자기 비판에 더 깊은 문맥적 이해를 가능하게 할 수 있다.</p>\n<h5 id=\"학습-과정\" style=\"position:relative;\"><a href=\"#%ED%95%99%EC%8A%B5-%EA%B3%BC%EC%A0%95\" aria-label=\"학습 과정 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>학습 과정</h5>\n<p>Self-RAG 학습은 별도의 '비평가(Critic)' 모델과 주 '생성기(Generator)' 모델을 포함한다.</p>\n<ol>\n<li><strong>비평가 모델 학습:</strong> 먼저, 강력한 LLM(예: GPT-4)을 사용하여 다양한 입력-출력 쌍에 대한 리플렉션 토큰을 생성하는 방식으로 지도 학습 데이터를 구축한다. 이 데이터를 사용하여 사전 훈련된 LM 기반의 비평가 모델(C)을 학습시킨다.</li>\n<li><strong>생성기 모델 학습:</strong> 학습된 비평가 모델(C)과 검색기(Retriever, R)를 사용하여 기존 학습 데이터(입력-출력 쌍)를 증강한다. 각 출력 세그먼트에 대해 비평가가 검색 필요성, 관련성, 지원 여부 등을 예측하고 해당 리플렉션 토큰을 데이터에 추가한다. 주 생성기 모델(M)은 이 증강된 데이터 코퍼스를 사용하여 학습된다. 학습 목표는 다음 토큰(일반 텍스트 토큰 또는 리플렉션 토큰)을 예측하는 표준적인 방식이다. 생성기의 어휘는 리플렉션 토큰을 포함하도록 확장된다. 이 과정을 통해 생성기는 추론 시 별도의 비평가 모델 없이 스스로 리플렉션 토큰을 생성할 수 있게 된다.</li>\n</ol>\n<h3 id=\"self-corrective-rag-scrag\" style=\"position:relative;\"><a href=\"#self-corrective-rag-scrag\" aria-label=\"self corrective rag scrag permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Self-Corrective RAG (SCRAG)</h3>\n<p>SCRAG의 핵심적인 특징은 Self-RAG와 유사한 <strong>자기 성찰(self-reflection)</strong> 메커니즘과 CRAG와 유사한 <strong>교정적 검색(corrective retrieval)</strong> 메커니즘을 결합한다는 점이다. 즉, LLM이 스스로 검색 정보의 가치를 평가하고 생성 내용을 검토하는 능력과 함께, 검색 결과의 품질을 외부적으로 평가하고 필요한 경우 추가 검색이나 정제와 같은 교정 조치를 수행하는 능력을 통합하고자 한다.</p>\n<h5 id=\"적응형-검색-및-자기-성찰\" style=\"position:relative;\"><a href=\"#%EC%A0%81%EC%9D%91%ED%98%95-%EA%B2%80%EC%83%89-%EB%B0%8F-%EC%9E%90%EA%B8%B0-%EC%84%B1%EC%B0%B0\" aria-label=\"적응형 검색 및 자기 성찰 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>적응형 검색 및 자기 성찰</h5>\n<ul>\n<li>Self-RAG와 유사하게, 모델이 검색된 구절의 <strong>가치(value)</strong>를 평가하고 검색 시점을 결정하기 위해 <strong>리플렉션 토큰</strong>을 사용할 수 있는 적응형 검색 메커니즘을 채택할 수 있다.</li>\n<li>모델은 생성된 콘텐츠를 실시간으로 평가하기 위해 리플렉션/비판 토큰을 사용하여 자기 평가(품질, 관련성 등)를 수행한다.</li>\n</ul>\n<h5 id=\"교정적-검색-요소\" style=\"position:relative;\"><a href=\"#%EA%B5%90%EC%A0%95%EC%A0%81-%EA%B2%80%EC%83%89-%EC%9A%94%EC%86%8C\" aria-label=\"교정적 검색 요소 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>교정적 검색 요소</h5>\n<ul>\n<li>CRAG와 유사하게, 초기에 검색된 문서의 <strong>품질/정확성(quality/accuracy)</strong>을 평가하는 절차를 포함한다 (잠재적으로 별도의 평가기 사용 가능).</li>\n<li>만약 검색된 데이터가 불충분하거나 오류가 있는 것으로 판단되면, 웹 검색과 같은 추가적인, 더 철저한 검색을 통해 더 신뢰할 수 있는 소스를 찾는 <strong>교정 조치</strong>를 트리거한다.</li>\n</ul>\n<h5 id=\"refinement\" style=\"position:relative;\"><a href=\"#refinement\" aria-label=\"refinement permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Refinement</h5>\n<ul>\n<li>CRAG의 \"decompose-then-recompose\" 절차와 유사한 방법을 사용하여, 초기 검색이나 웹 검색을 통해 얻은 데이터를 필터링하고, 관련 있는 부분에 집중하며 노이즈를 제거하여 정제한다.</li>\n</ul>\n<h3 id=\"comparative-analysis\" style=\"position:relative;\"><a href=\"#comparative-analysis\" aria-label=\"comparative analysis permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Comparative Analysis</h3>\n<table>\n<thead>\n<tr>\n<th>특징</th>\n<th>Corrective-RAG (CRAG)</th>\n<th>Self-RAG</th>\n<th>Self-Corrective RAG (SCRAG)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>목표</td>\n<td>검색 오류에 대한 생성 견고성 향상</td>\n<td>생성 품질/사실성 향상 및 추론 시 제어 가능성 확보</td>\n<td>자기 성찰과 교정 결합을 통한 최대 신뢰성 확보</td>\n</tr>\n<tr>\n<td>핵심</td>\n<td>경량 평가기를 통한 검색 결과 평가 및 교정</td>\n<td>LLM 자체의 리플렉션 토큰을 통한 적응형 검색 및 자기 비판</td>\n<td>자기 성찰과 교정적 검색 메커니즘의 통합</td>\n</tr>\n<tr>\n<td>평가 시점</td>\n<td>주로 생성 이전</td>\n<td>생성 도중 (세그먼트 단위)</td>\n<td>생성 이전 및 생성 도중 가능성</td>\n</tr>\n<tr>\n<td>평가 주체</td>\n<td>별도 경량 평가기 (예: T5)</td>\n<td>주 생성 LLM (리플렉션 토큰 생성)</td>\n<td>LLM (자기 성찰) + 잠재적 외부 평가기 (교정 트리거)</td>\n</tr>\n<tr>\n<td>교정 트리거</td>\n<td>평가기 신뢰도 점수 및 임계값</td>\n<td>비판 토큰 점수 기반 빔 검색 경로 선택</td>\n<td>품질 평가 결과 (교정) + 비판 토큰 (정제/선택)</td>\n</tr>\n<tr>\n<td>교정 메커니즘</td>\n<td>지식 정제 (Decompose-recompose), 웹 검색</td>\n<td>최적 생성 경로 선택</td>\n<td>Decompose-recompose, 웹 검색, 자기 비판 기반 정제/선택</td>\n</tr>\n<tr>\n<td>특수 토큰 사용</td>\n<td>아니오</td>\n<td>예 (리플렉션 토큰)</td>\n<td>예 (리플렉션/비판 토큰)</td>\n</tr>\n<tr>\n<td>학습 복잡성</td>\n<td>평가기 미세조정 필요</td>\n<td>비평가 모델 학습 및 생성기 증강 데이터 학습 필요</td>\n<td>잠재적으로 가장 높음 (두 메커니즘 통합)</td>\n</tr>\n<tr>\n<td>추론 복잡성/지연</td>\n<td>평가기 실행, 웹 검색 등으로 오버헤드 발생</td>\n<td>병렬 처리, 비판 토큰 생성/평가로 지연 가능성</td>\n<td>잠재적으로 가장 높음 (두 메커니즘 결합)</td>\n</tr>\n<tr>\n<td>장점</td>\n<td>견고성, 적응성, 경량 평가, 웹 검색 활용</td>\n<td>적응형 검색, 제어 가능성, 자기 비판, 인용 정확도</td>\n<td>잠재적 최대 정확성/신뢰성, 다층적 검증</td>\n</tr>\n<tr>\n<td>단점</td>\n<td>평가기 의존성, 미세조정 필요, 오버헤드</td>\n<td>미지원 출력 가능성, 학습/추론 복잡성</td>\n<td>높은 복잡성 및 잠재적 지연 시간 (명시적 언급 없음)</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"use-cases\" style=\"position:relative;\"><a href=\"#use-cases\" aria-label=\"use cases permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Use Cases</h3>\n<h5 id=\"corrective-rag-crag-1\" style=\"position:relative;\"><a href=\"#corrective-rag-crag-1\" aria-label=\"corrective rag crag 1 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Corrective-RAG (CRAG)</h5>\n<p>CRAG는 검색 단계의 신뢰성 문제에 초점을 맞추므로 다음과 같은 시나리오에 특히 유용하다.</p>\n<ul>\n<li>초기 검색 품질이 불안정하거나 낮은 경우: CRAG는 검색 결과 평가 및 교정 메커니즘을 통해 이러한 문제를 완화하도록 설계되었다.</li>\n<li>관련 없는 문서에 대한 높은 견고성이 요구될 때: 검색된 문서 중 관련 없는 내용이 많을 것으로 예상되는 경우, CRAG의 평가 및 정제 기능이 효과적이다.</li>\n<li>정적 지식 베이스가 불충분하여 웹 검색이 필요할 때: 기본 코퍼스에서 답을 찾지 못하거나 최신 정보가 필요할 때, CRAG의 조건부 웹 검색 기능이 중요한 역할을 한다. 동적 정보 접근 필요성은 CRAG 또는 SCRAG를 선택하는 주요 차별점이 될 수 있다.</li>\n<li>모듈식 평가 구성 요소를 선호하는 경우: 평가기를 별도로 관리하고 업데이트하는 것이 용이한 환경에 적합하다 (경량 평가기).</li>\n<li>예시: 외부 정보 검증이 필요한 팩트체킹 시스템, 정적 코퍼스가 빠르게 구식이 되는 분야(뉴스, 기술 동향 등)의 질의응답 시스템, 기존 RAG 파이프라인에 견고성을 추가하고자 할 때.</li>\n</ul>\n<h5 id=\"self-rag에-적합한-시나리오\" style=\"position:relative;\"><a href=\"#self-rag%EC%97%90-%EC%A0%81%ED%95%A9%ED%95%9C-%EC%8B%9C%EB%82%98%EB%A6%AC%EC%98%A4\" aria-label=\"self rag에 적합한 시나리오 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Self-RAG에 적합한 시나리오</h5>\n<p>Self-RAG는 생성 과정 자체의 제어와 성찰에 강점을 가지므로 다음과 같은 경우에 적합하다.</p>\n<ul>\n<li>추론 시 생성 프로세스의 세밀한 제어가 필요할 때: 리플렉션 토큰 가중치 조정을 통해 사실성, 관련성, 간결성 등 다양한 측면을 동적으로 제어해야 하는 애플리케이션.</li>\n<li>검색 필요 여부를 동적으로 판단하는 것이 중요할 때: 모든 질의에 검색이 필요하지 않은 경우, Self-RAG의 적응형 검색 기능이 효율성을 높인다.</li>\n<li>높은 인용 정확도와 생성 근거의 추적성이 요구될 때: 각 생성 세그먼트에 대한 자체 평가(지원 여부 등)를 제공하여 신뢰도를 높이고 검증을 용이하게 한다.</li>\n<li>세그먼트별 사실적 근거 유지가 중요한 장문 생성 작업: 긴 보고서나 답변 생성 시 각 부분의 정확성을 유지하는 데 도움이 된다.</li>\n<li>예시: 인용 정보가 포함된 상세 보고서 생성, 반복적인 개선이 필요한 복잡한 질의응답, 사실성과 대화 흐름 간의 균형 조절이 필요한 챗봇.</li>\n</ul>\n<h5 id=\"self-corrective-rag-scrag-1\" style=\"position:relative;\"><a href=\"#self-corrective-rag-scrag-1\" aria-label=\"self corrective rag scrag 1 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Self-Corrective RAG (SCRAG)</h5>\n<p>SCRAG는 CRAG와 Self-RAG의 장점을 결합하여 최대의 신뢰성을 목표로 하므로 다음과 같은 시나리오에 적합할 수 있다.</p>\n<ul>\n<li>최고 수준의 신뢰성과 사실적 정확성이 요구되는 고위험 애플리케이션: 여러 단계의 검증 및 교정 메커니즘이 중요한 경우. 의료, 금융 분야의 중요한 의사 결정 지원 시스템 등이 해당될 수 있다.</li>\n<li>검색 품질 문제와 생성 제어 필요성이 모두 존재하는 복잡한 환경: 두 가지 문제를 동시에 해결해야 할 때.</li>\n<li>잠재적인 지연 시간 및 복잡성 증가를 감수할 수 있는 경우: 최고 품질의 출력을 위해 추가적인 리소스 투자가 가능한 상황.</li>\n<li>예시: 중요한 의사 결정 지원 시스템, 높은 정확성이 요구되는 고급 교육 도구 , 복잡한 연구 지원 시스템.</li>\n</ul>\n<h3 id=\"research-and-development-direction\" style=\"position:relative;\"><a href=\"#research-and-development-direction\" aria-label=\"research and development direction permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Research and Development Direction</h3>\n<p>RAG 연구는 크게 세 가지 패러다임으로 발전해왔다</p>\n<ul>\n<li>Naive RAG: 기본적인 검색-증강-생성 파이프라인. 구현이 간단하지만 검색 및 생성 품질에 한계가 있다.</li>\n<li>Advanced RAG: Naive RAG의 한계를 개선하기 위해 검색 전후 처리(pre/post-processing) 단계를 추가하거나 파이프라인을 최적화하는 접근 방식. 재순위화(reranking) , 질의 변환(query transformation) , 피드백 루프, 검색기와 생성기 미세조정 등이 포함된다. <strong>CRAG와 Self-RA</strong>G도 이 범주에 속한다고 볼 수 있다.</li>\n<li>Modular RAG: RAG 파이프라인을 검색, 생성, 증강 등 기능적 모듈로 분해하고, 이를 유연하게 조합하거나 교체할 수 있도록 설계하는 방식. 특정 작업에 맞춰 모듈을 최적화하거나 새로운 모듈(예: 메모리, 플래닝)을 추가하여 시스템을 확장할 수 있다.</li>\n</ul>\n<p>최근 RAG 연구는 다음과 같은 방향으로 활발히 진행되고 있다.</p>\n<ul>\n<li>Agentic RAG: RAG 파이프라인 내에 자율적인 AI 에이전트(agent)를 도입하여 전체 프로세스를 보다 지능적으로 관리하고 실행하는 패러다임이다. 에이전트는 <strong>성찰(Reflection)</strong>(자체 평가 및 오류 수정), <strong>계획(Planning)</strong>(작업 흐름 설계), <strong>도구 사용(Tool Use)</strong>(외부 API, 데이터베이스 연동), <strong>다중 에이전트 협업(Multi-Agent Collaboration)</strong>과 같은 패턴을 활용하여, 복잡한 질의를 해결하고, 동적으로 검색 전략을 선택하며, 여러 단계에 걸친 추론을 수행하고, 외부 도구를 활용하여 정보를 수집 및 처리한다.</li>\n<li>Graph RAG: 기존의 비정형 텍스트 데이터뿐만 아니라 그래프 구조의 지식(Knowledge Graph)을 RAG 시스템에 통합하여 활용하는 연구이다. 그래프는 개체 간의 관계 정보를 명시적으로 표현하므로, 보다 정교한 추론과 의미론적 검색을 가능하게 할 수 있다.</li>\n<li>Multimodal RAG: RAG의 적용 범위를 텍스트를 넘어 이미지, 비디오, 오디오 등 다양한 모달리티(modality)로 확장하는 연구이다.</li>\n<li>검색 및 인덱싱 최적화: RAG 성능의 근간이 되는 검색 품질을 향상시키기 위한 연구가 지속되고 있다. 여기에는 더 나은 청크 분할 전략 개발, 임베딩 모델 성능 개선, 문장-창 검색(sentence-window retrieval) 및 자동 병합 검색(auto-merging retrieval)과 같은 고급 검색 기법, 질의와 문서 간의 의미론적 정렬 강화(예: HyDE, 합성 질의 생성), 검색 결과 재순위화(re-ranking) 알고리즘 개선 등이 포함된다.</li>\n<li>생성 및 증강 최적화: 검색된 정보를 LLM이 더 효과적으로 활용하도록 돕는 기법 연구도 활발하다. 프롬프트 엔지니어링 기법(예: StepBack-prompt), 검색기와 생성기 간의 정렬을 위한 미세조정, LLM의 제한된 컨텍스트 창을 효율적으로 관리하는 방법 등이 연구되고 있다.</li>\n</ul>\n<h3 id=\"summary\" style=\"position:relative;\"><a href=\"#summary\" aria-label=\"summary permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Summary</h3>\n<p>대규모 언어 모델(LLM)의 한계를 보완하기 위해 등장한 검색 증강 생성(RAG) 기술, 특히 그 중에서도 기본적인 Naive RAG의 문제점을 해결하기 위해 제안된 고급 방법론인 Corrective-RAG (CRAG), Self-RAG, Self-Corrective RAG (SCRAG)에 대해 심층적으로 분석하고 비교했다.</p>\n<ul>\n<li><strong>Corrective-RAG (CRAG):</strong> 생성 단계 이전에 별도의 경량 평가기를 사용하여 검색된 문서의 품질을 평가하고, 그 결과에 따라 지식 정제(decompose-then-recompose)나 웹 검색과 같은 교정 조치를 수행하여 입력 컨텍스트의 품질을 높이는 데 중점을 둔다. 핵심은 <strong>생성 전 교정</strong>을 통한 견고성 확보다.</li>\n<li><strong>Self-RAG:</strong> 주 생성 LLM 자체가 리플렉션 토큰을 사용하여 검색 필요성을 동적으로 판단하고(적응형 검색), 생성 과정에서 각 세그먼트의 관련성, 사실적 지원 여부, 유용성을 스스로 평가(자기 비판)하여 최적의 응답 경로를 선택한다. 핵심은 <strong>생성 중 자기 성찰</strong>을 통한 제어 가능성 및 적응성 확보다.</li>\n<li><strong>Self-Corrective RAG (SCRAG):</strong> CRAG의 교정적 검색 메커니즘과 Self-RAG의 자기 성찰 메커니즘을 통합하려는 시도이다. 검색 품질 평가 및 교정, 그리고 생성 중 자기 평가를 모두 수행하여 최대한의 신뢰성을 확보하는 것을 목표로 한다. 핵심은 <strong>교정과 자기 성찰의 결합</strong>이다.</li>\n</ul>","tableOfContents":"<ul>\n<li>\n<p><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#introduction\">Introduction</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#limitation-of-llm-models\">Limitation of LLM Models</a></li>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#limitation-of-naive-rag\">Limitation of Naive RAG</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#retrieval-augmented-generation\">Retrieval-Augmented Generation</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#indexing\">Indexing</a></li>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#retrieval\">Retrieval</a></li>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#generation\">Generation</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#corrective-rag-crag\">Corrective-RAG (CRAG)</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#lightweight-retrieval-evaluator\">Lightweight Retrieval Evaluator</a></li>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#confidence-scoring-and-action-trigger\">Confidence Scoring and Action Trigger</a></li>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#corrective-actions\">Corrective Actions</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#self-rag\">Self-RAG</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#reflection-tokens\">Reflection Tokens</a></li>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#adaptive-retrieval\">Adaptive Retrieval</a></li>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#generation-and-self-critique\">Generation and Self-Critique</a></li>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#%ED%95%99%EC%8A%B5-%EA%B3%BC%EC%A0%95\">학습 과정</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#self-corrective-rag-scrag\">Self-Corrective RAG (SCRAG)</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#%EC%A0%81%EC%9D%91%ED%98%95-%EA%B2%80%EC%83%89-%EB%B0%8F-%EC%9E%90%EA%B8%B0-%EC%84%B1%EC%B0%B0\">적응형 검색 및 자기 성찰</a></li>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#%EA%B5%90%EC%A0%95%EC%A0%81-%EA%B2%80%EC%83%89-%EC%9A%94%EC%86%8C\">교정적 검색 요소</a></li>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#refinement\">Refinement</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#comparative-analysis\">Comparative Analysis</a></li>\n<li>\n<p><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#use-cases\">Use Cases</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#corrective-rag-crag-1\">Corrective-RAG (CRAG)</a></li>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#self-rag%EC%97%90-%EC%A0%81%ED%95%A9%ED%95%9C-%EC%8B%9C%EB%82%98%EB%A6%AC%EC%98%A4\">Self-RAG에 적합한 시나리오</a></li>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#self-corrective-rag-scrag-1\">Self-Corrective RAG (SCRAG)</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#research-and-development-direction\">Research and Development Direction</a></li>\n<li><a href=\"/MachineLearning/25-04-21-Retrieval-Augmented%20Generation/#summary\">Summary</a></li>\n</ul>","frontmatter":{"path":"/deeplearning/25-04-21/","title":"Retrieval-Augmented Generation","category":"Deep Learning","date":"2025-04-21"}}},"pageContext":{}},"staticQueryHashes":["2390655019","256249292","63159454"]}